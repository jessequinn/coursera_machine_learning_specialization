[Musik] Nun haben wir die Grundlagen von Entscheidungsbäumen kennengelernt. Entscheidungsbäume sind eine erstaunliche Art von ... ... Klassifizierern, die eine Reihe verschiedener Typen von Daten verwendet werden können. Entscheidungsbäume sind jedoch hochanfällig für Overfitting. Lasst uns in diesem Modul ein wenig erforschen, wie wir Overfitting im ... ... Kontext von Entscheidungsbäumen verhindern können. Zur Erinnerung, wir werden weiterhin das System zur Evaluation der Kreditanträge ... ... als Beispiel verwenden, wo Kreditanträge eingehen und ... ... wir in der Lage sein werden vorherzusagen, ob es ein sicherer oder unsicherer Kredit ist. Das ist die Entscheidung, die wir versuchen zu machen. Von diesem Kreditantrag werden wir den Entscheidungsbaum lernen. Das erlaubt uns den Baum entlang nach unten zu traversieren und ... ... eine Vorhersage darüber zu treffen, ob ein bestimmter Kredit riskant oder sicher ist. Die Eingaben werden die x i sein und ... ... die Ausgaben werden die y i sein, die wir auf Basis der Daten vorhersagen. Lasst uns zunächst ein paar Minuten Overfitting wiederholen und dann erforschen, ... ... wie es in Entscheidungsbäumen in Erscheinung tritt, was sehr heftig sein wird. Wir erinnern uns: Overfitting ist das Symptom des Unterschieds zwischen Fehler auf den Trainingsdaten, ... ... der mit komplexeren Modellen abnimmt, und dem wahren ... ... Fehler, der bei zu hoher steigender Modellkomplexität ansteigt. Genauer gesagt tritt Overfitting dann auf, wenn wir ein Modell w Dach haben, ... ... das einen Trainingsfehler hat, der geringer ist, als der wahre Fehler. Aber es gab ein anderes Modell oder andere Modell-Parameter, w Stern, ... ... das vielleicht einen höheren Trainingsfehler hat, aber einen niedrigeren wahren Fehler. Das ist das Overfitting-Problem. Dann wählen wir auf irgendeine Art und Weise ein weniger komplexes Modell, um diese Art von Overfitting zu vermeiden. Wir sahen diesen Effekt bei logistischer Regression ziemlich deutlich, als wir ... ... den Grad der Polynome erhöhten. Wir erhielten verrückte ... ... Entscheidungsgrenzen, wo wir Overfitting sahen, was wirklich schlimmes Overfitting war. Overfitting für Polynome 6. Grades und Polynome ... 20. Grades für die Features, das ist ein technischer Term, den ich benutze. Ich verwende den Begriff verrückte Entscheidungsgrenze. Lasst es uns verrücktes Overfitting nennen. Wirklich schlimmes Zeug. Also versuchen wir überkomplexe Modelle zu vermeiden. Wie wir bei Entscheidungsbäumen sehen werden, ... können Modelle sehr schnell überkomplex werden. [Musik]