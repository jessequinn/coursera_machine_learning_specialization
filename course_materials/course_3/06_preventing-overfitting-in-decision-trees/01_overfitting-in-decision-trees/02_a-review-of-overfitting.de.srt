1
00:00:00,000 --> 00:00:04,599
[Musik]

2
00:00:04,599 --> 00:00:08,385
Nun haben wir die Grundlagen von Entscheidungsbäumen kennengelernt. Entscheidungsbäume sind eine erstaunliche Art von ...

3
00:00:08,385 --> 00:00:12,186
... Klassifizierern, die eine Reihe verschiedener Typen von Daten verwendet werden können.

4
00:00:12,186 --> 00:00:15,274
Entscheidungsbäume sind jedoch hochanfällig für Overfitting.

5
00:00:15,274 --> 00:00:19,946
Lasst uns in diesem Modul ein wenig erforschen, wie wir Overfitting im ...

6
00:00:19,946 --> 00:00:21,819
... Kontext von Entscheidungsbäumen verhindern können.

7
00:00:23,360 --> 00:00:28,020
Zur Erinnerung, wir werden weiterhin das System zur Evaluation der Kreditanträge ...

8
00:00:28,020 --> 00:00:32,590
... als Beispiel verwenden, wo Kreditanträge eingehen und ...

9
00:00:32,590 --> 00:00:38,120
... wir in der Lage sein werden vorherzusagen, ob es ein sicherer oder unsicherer Kredit ist.

10
00:00:38,120 --> 00:00:39,940
Das ist die Entscheidung, die wir versuchen zu machen.

11
00:00:41,340 --> 00:00:44,600
Von diesem Kreditantrag werden wir den Entscheidungsbaum lernen.

12
00:00:44,600 --> 00:00:46,520
Das erlaubt uns den Baum entlang nach unten zu traversieren und ...

13
00:00:46,520 --> 00:00:52,220
... eine Vorhersage darüber zu treffen, ob ein bestimmter Kredit riskant oder sicher ist.

14
00:00:52,220 --> 00:00:54,630
Die Eingaben werden die x i sein und ...

15
00:00:54,630 --> 00:00:58,790
... die Ausgaben werden die y i sein, die wir auf Basis der Daten vorhersagen.

16
00:01:00,580 --> 00:01:04,610
Lasst uns zunächst ein paar Minuten Overfitting wiederholen und dann erforschen, ...

17
00:01:04,610 --> 00:01:08,150
... wie es in Entscheidungsbäumen in Erscheinung tritt, was sehr heftig sein wird.

18
00:01:10,360 --> 00:01:15,608
Wir erinnern uns: Overfitting ist das Symptom des Unterschieds zwischen Fehler auf den Trainingsdaten, ...

19
00:01:15,608 --> 00:01:21,102
... der mit komplexeren Modellen abnimmt, und dem wahren ...

20
00:01:21,102 --> 00:01:26,854
... Fehler, der bei zu hoher steigender Modellkomplexität ansteigt.

21
00:01:26,854 --> 00:01:33,121
Genauer gesagt tritt Overfitting dann auf, wenn wir ein Modell w Dach haben, ...

22
00:01:33,121 --> 00:01:37,434
... das einen Trainingsfehler hat, der geringer ist, als der wahre Fehler.

23
00:01:37,434 --> 00:01:41,238
Aber es gab ein anderes Modell oder andere Modell-Parameter, w Stern, ...

24
00:01:41,238 --> 00:01:46,457
... das vielleicht einen höheren Trainingsfehler hat, aber einen niedrigeren wahren Fehler.

25
00:01:46,457 --> 00:01:49,290
Das ist das Overfitting-Problem.

26
00:01:49,290 --> 00:01:55,580
Dann wählen wir auf irgendeine Art und Weise ein weniger komplexes Modell, um diese Art von Overfitting zu vermeiden.

27
00:01:55,580 --> 00:02:00,659
Wir sahen diesen Effekt bei logistischer Regression ziemlich deutlich, als wir ...

28
00:02:00,659 --> 00:02:06,141
... den Grad der Polynome erhöhten. Wir erhielten verrückte ...

29
00:02:06,141 --> 00:02:11,398
... Entscheidungsgrenzen, wo wir Overfitting sahen, was wirklich schlimmes Overfitting war.

30
00:02:11,398 --> 00:02:16,781
Overfitting für Polynome 6. Grades und Polynome ...

31
00:02:16,781 --> 00:02:22,362
20. Grades für die Features, das ist ein technischer Term, den ich benutze.

32
00:02:22,362 --> 00:02:28,836
Ich verwende den Begriff verrückte Entscheidungsgrenze. Lasst es uns verrücktes Overfitting nennen.

33
00:02:28,836 --> 00:02:31,402
Wirklich schlimmes Zeug.

34
00:02:31,402 --> 00:02:34,500
Also versuchen wir überkomplexe Modelle zu vermeiden.

35
00:02:34,500 --> 00:02:36,803
Wie wir bei Entscheidungsbäumen sehen werden, ...

36
00:02:36,803 --> 00:02:39,640
können Modelle sehr schnell überkomplex werden.

37
00:02:39,640 --> 00:02:44,819
[Musik]