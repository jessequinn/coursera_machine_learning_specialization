1
00:00:00,000 --> 00:00:04,599
[MÚSICA]

2
00:00:04,599 --> 00:00:08,385
Hemos visto los aspectos básicos de los
árboles de decisión, los cuales son un tipo

3
00:00:08,385 --> 00:00:12,186
clasificador que pueden ser usados en un
rango de diferentes tipos de datos.

4
00:00:12,186 --> 00:00:15,274
Sin embargo, los árboles de decisión
son muy propensos al sobreajuste.

5
00:00:15,274 --> 00:00:19,946
Entonces, vamos a profundizar un poco en este
módulo en cómo podemos evitar el sobreajuste en

6
00:00:19,946 --> 00:00:21,819
el contexto de árboles de decisión.

7
00:00:23,360 --> 00:00:28,020
Y como recordatorio, vamos a seguir
utilizando como ejemplo nuestro sistema

8
00:00:28,020 --> 00:00:32,590
de evaluación de solicitud de préstamo,
donde los datos de préstamos llegarán y

9
00:00:32,590 --> 00:00:38,120
vamos a ser capaces de predecir si
el préstamo es seguro o arriesgado.

10
00:00:38,120 --> 00:00:39,940
Y así, esa es la toma de decisión
que estamos intentando hacer.

11
00:00:41,340 --> 00:00:44,600
Y de esa solicitud de préstamo,
vamos a aprender el árbol de decisión

12
00:00:44,600 --> 00:00:46,520
que nos permite
recorrer el árbol y

13
00:00:46,520 --> 00:00:52,220
realizar la predicción de si un
préstamo en particular es seguro o arriesgado.

14
00:00:52,220 --> 00:00:54,630
Y así, la entrada será xi, y

15
00:00:54,630 --> 00:00:58,790
la salida va a ser ŷi 
que vamos a predecir de los datos.

16
00:01:00,580 --> 00:01:04,610
Primero, hagamos una revisión
del sobreajuste y luego veamos

17
00:01:04,610 --> 00:01:08,150
en cómo ocurre en árboles de decisión,
lo cual, anticipo, va a ser realmente malo.

18
00:01:10,360 --> 00:01:15,608
Como recordamos, el sobreajuste es el hecho
que separa el error de entrenamiento,

19
00:01:15,608 --> 00:01:21,102
el cual tiende a cero a medida incrementamos
la complejidad de nuestro modelo, y el error

20
00:01:21,102 --> 00:01:26,854
verdadero, el cual disminuye con la complejidad
del modelo, pero luego se incrementa.

21
00:01:26,854 --> 00:01:33,121
Más específicamente, el sobreajuste
sucede cuando acabamos con un modelo ŵ,

22
00:01:33,121 --> 00:01:37,434
que tiene error de entrenamiento bajo,
pero error verdadero alto.

23
00:01:37,434 --> 00:01:41,238
Pero había otro modelo, o 
parámetros de modelo, w* ,

24
00:01:41,238 --> 00:01:46,457
que tenía tal vez error de entrenamiento mas alto,
pero definitivamente error verdadero mas pequeño.

25
00:01:46,457 --> 00:01:49,290
Y ése es el problema del sobreajuste.

26
00:01:49,290 --> 00:01:55,580
Y cuando de alguna manera, elegimos un modelo
con menor complejidad para evitar ese tipo de sobreajuste.

27
00:01:55,580 --> 00:02:00,659
Vimos este efecto, en regresión logística, 
muy pronunciadamente a medida que

28
00:02:00,659 --> 00:02:06,141
incrementamos el grado del polinomio,
obteníamos una frontera de decisión más y más

29
00:02:06,141 --> 00:02:11,398
irregular, donde vimos sobreajuste,
lo cual era un mal sobreajuste aquí

30
00:02:11,398 --> 00:02:16,781
Pero el sobreajuste para polinomios de 
grado seis y luego polinomios

31
00:02:16,781 --> 00:02:22,362
de grado 20 para las características,
este es un término técnico que uso.

32
00:02:22,362 --> 00:02:28,836
Creo  que utilizo frontera de decisión "loca",
pero llamémosle sobreajuste "loco".

33
00:02:28,836 --> 00:02:31,402
Algo realmente malo.

34
00:02:31,402 --> 00:02:34,500
Y por eso, estamos intentando
evitar modelos excesivamente complejos.

35
00:02:34,500 --> 00:02:36,803
Y como veremos con árboles de decisión,

36
00:02:36,803 --> 00:02:39,640
los modelos pueden hacerse
demasiado complejos muy rapidamente.

37
00:02:39,640 --> 00:02:44,819
[MÚSICA]