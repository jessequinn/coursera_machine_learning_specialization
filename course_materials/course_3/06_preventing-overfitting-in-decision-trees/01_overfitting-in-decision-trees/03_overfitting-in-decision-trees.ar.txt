[موسيقى] مع كل ذلك في الاعتبار، دعونا نتعمق ونراقب كيف يظهر الاحتواء في سياق تسلسل القرار. عند بدأ تعلم تسلسل القرار، نتعلم فرع القرار، وهو حد بسيط للغاية بين البيانات. دعنا نفترض أن المثال البسيط سيستخدم خط عمودي أو خط أفقي. في هذه الحالة، عندما تكون القرارات بالنسبة إلى x (1) سواء كانت أقل من -0.07 أو أكبر من 0.07. فإذا كانت أقل، سوف نتوقع -1 وهذا يتوافق مع هذا الجانب من حدود القرار، إذا كان أكبر من -0.07 سوف يكون التنبؤ 1+ سوف يكون في الجانب الأيمن من حدود القرار. تلك هي حدود قرار بسيطة للغاية، مع تسلسل قرار من الدرجة الأولى أو فرع قرار، لكن كلما زدنا في العمق، يصبح هذا الوضع أكثر تعقيدًا. كما تصبح حدود القرار أكثر تعقيدًا. لذلك يمكننا أن نرى، كلما زاد العمق، زاد
تعقيد حدود القرار. حتى نصل إلى هذا الشيء المعقد هنا، حيث يكون العمق 10. وهكذا، فإن عمق حد القرار العاشر ليس به خطأ في التدريب. لذا فإن خطأ التدريب عند هذه النقطة ينخفض من 0.22 وهو مرتفع نسبياً لفرع القرار من 0.13 إلى 0.10، وهو عمق من الدرجة الثانية وتسلسل القرار من الدرجة الثالثة، الذي لا بأس به حتى تسلسل القرار المعقد بعمق 10، الذي لا يحتوي على خطأ تدريبي. وهذا ينبغي أن يكون علامة تحذير كبيرة بالنسبة لك. لنمثل ذلك بوجه حزين. لقد رأينا أن مع تسلسل القرار، كلما زاد العمق، قل خطأ التدريب حتى نصل إلى نقطة حيث يمكن لخطأ التدريب أن يصل إلى الصفر، وكثيرا ما يحدث ذلك، ففي هذه الحالة
في تسلسل القرار ذي العمق 10. وقد تفكر، أن تسلسل القرار بعمق 10، هو شيء عظيم، فهو ليس به خطأ تدريبي، لذلك فهو تسلسل القرار المثالي. لكن في الواقع، هو ليس تسلسل قرار مثالي. وكما نعلم، على الرغم من أن خطأ التدريب هو صفر، الخطأ الحقيقي يمكن أن يتصاعد. ولذا قد يكون تسلسل القرار به الاحتواء المبالغ. لذا من الجيد أن نخطو خطوة إلى الوراء وأن نفهم حقاً بشكل أفضل لماذا يميل الخطأ التدريبي لتسلسل القرار إلى الانهيار بسرعة في وجود العمق. ﻟﻧﺄﺧذ ھذا اﻟﻣﺛﺎل اﻟﺑﺳﯾط ﺣﯾث ﻧﺗﻌﻠم فرع اﻟﻘرار. لدينا 40 نقطة بيانات، مثلما كنا نستخدم، 22 منهم كانوا قروضًا آمنة، و 18 منها كانت محفوفة بالمخاطر. وقد اخترنا أولاً أن نقسم على الائتمان. والسؤال الآن هو لماذا اخترنا أن نقسم على الائتمان أولاً؟ لماذا كانت هذه الميزة الأولى التي اخترناها؟ والسبب الذي جعلنا نختار التقسيم على الائتمان أولاً هو أنه قام بتحسين الخطأ التدريبي أكثر. حيث قام بتحسين خطأ التدريب من 0.45 إلى 0.20. كان هذا أول انشقاق جيد. الآن، إذا عدنا وراجعنا خوارزمية لاختيار أفضل ميزة للانقسام ثم نحاول كل الميزات الممكنة ونختار ما تقلل الخطأ التدريبي أكثر من غيرها. وهكذا في كل خطوة على مدار تلك العملية، نقوم بإضافة الميزات التي تعمل على تقليل الخطأ في التدريب ثم نقوم بإضافة ميزات تقلل من خطأ التدريب. ثم نقوم بإضافة ميزات تقلل من خطأ التدريب. وفي النهاية سنقود خطأ التدريب إلى الصفر. ما لم نحصل بالطبع على بعض النقاط، حيث لا يمكننا أن نخفض خطأ التدريب لنفاد الميزات التي يتم عليها التقسيم لدينا نقاط إيجابية فوق نقاط سلبية، لكن هذه ملاحظة جانبية. الأهم من ذلك هو أن نتذكر أن الخطأ في التدريب يميل إلى الانخفاض، أكثر فأكثر. وهكذا فإن الانخفاض المرتبط بزيادة العمق هو ما يؤدي إلى انخفاض الخطأ في التدريب، والذي يؤدي غالبًا إلى هذه التسلسلات المعقدة جدًا، التي هي عرضة للاحتواء المبالغ بشكل كبير. وهنا مجموعة بيانات واقعية، من بيانات القروض. حيث لاحظنا في الواقع أن الاحتواء المبالغ هو مشكلة كبيرة سيئة. فإذا أخذنا عمق التسلسل،وحولناه إلى عمق 18، سنرى أن خطأ التدريب قد انخفض كثيرًا. والخط الأزرق، الذي قلل خطأ التدريب بنسبة 8 بالمائة، والذي يعتبر منخفضًا جدًا. ومع ذلك، إذا نظرت إلى خطأ مجموعة التحقق، فسترى أنه ليس جيدًا. ربما هذا حوالي 39 ٪، مما يعني أن هناك فجوة كبيرة بين الاثنين. التي سنقوم بتمييزه كنوع من أنواع الاحتواء المبالغ. إذا تمكنت بطريقة ما من اختيار أفضل عمق لتسلسل القرار، والذي في هذه الحالة كان عمق سبعة. ستلاحظ أنه في هذه الحالة، خطأ التحقق أقل من 35٪ بقدر قليل. بمعنى آخر، إذا كان يمكنني اختيار العمق المناسب، فسأحصل على خطأ في التدريب بنسبة 39%. ولكن، إذا تركته حتى خطأ تدريب منخفض للغاية، أحصل على خطأ في التحقق بنسبة 39٪. اتباع المسار لآخره هو فكرة سيئة. يجب التوقف مبكرًا. [موسيقى]