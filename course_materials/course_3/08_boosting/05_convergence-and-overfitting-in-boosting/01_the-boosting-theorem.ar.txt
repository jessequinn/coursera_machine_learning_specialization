[موسيقى] فيما يلي سنستغرق بضع دقائق،
للتحدث بشأن نظرية AdaBoost الشهيرة وماهي آثارها العملية،
التي تكون مثيرة للاهتمام جدًا. لذلك، إذا كنت تتذكر، الطريقة التي ظهر
بها التعزيز هي أنه رواد "كيرنز" و"فاليانت" هذا السؤال الشهير، هل يمكنك دمج
مصنفات ضعيفة لإنشاء آخر قوي؟ في الواقع، فاليانت هو
فائز بجائرة توزنغ، لذلك هو أستاذ شهير للغاية بجامعة هارفارد. وبعد عامين توصل شابيري لفكرة التعزيز،
مما غير مجال التعلم الآلي حقًا. وهكذا، إذا نظرت إلى تكرار التعزيز في المحور x هنا، في خطأ
التدريب لمجموعة البيانات الخاصة بنا. نلاحظ تأثير عملي للغاية
نراه في كثير من بيانات التعزيز. سنبدأ بخطأ تدريب سيئ حقًا. لذا فإن أول جذع قرار له
خطأ تدريب يبلغ 22.5%. لذا، ليس جيدًا على الإطلاق. وبعد ثلاثين مرة من التكرار،
كما شاهدنا منذ قليل، وسنقوم بالحصول على كافة نقاط البيانات
بشكل صحيح في بياناتنا التدريب وقد أظهرت لك نوعًا من حد
القرار المجنون لهذا. لذلك نرى انتقالًا سلسًا حيث يميل
خطأ التصنيف للانخفاض إلى أسفل. ويميل إلى الانخفاض وتذهب القيمة إلى الصفر،
ويبقى في الواقع عند مستوى الصفر. وهذه فكرة رئيسية
لنظرية التعزيز. لذا نرى هنا نظرية AdaBoost الشهيرة
الذي يحدد جميع الخيارات التي قمنا بها في الخوارزمية وكان لديها
الكثير من الأثر حقًا على التعلم الآلي. وتقول النظرية بشكل أساسي إنه
تحت بعض الظروف التقنية تكون جميع النظريات مثل هذا. الآن إذا كنت لتكون قادرًا على القول بأن بعض
القيود تنطبق، راجع المتجر للحصول على التفاصيل. ولذا نقول تحت بعض الشروط الفنية، خطأ التدريب للتصنيف
يذهب إلى الصفر، وكذلك عدد تكرارات العدد وبعض النماذج التي تعتبر
أن T تذهب إلى اللانهاية. لذلك وبعبارة أخرى وبشكل تصويري
سنرى أن نتوقع خطأ التدريب، وهو هذا الشيء هنا على المحور y،
للانتقال في نهاية المطاف إلى الصفر. والآن في نهاية المطاف. قد تتذبذب قليلًا
في الوسط، ومع ذلك، ما تقوله النظرية هو إنه يميل إلى
الانخفاض عمومًا، ويصبح في نهاية المطاف صفر،
ثم يستقر عند مستوى الصفر. ولذلك سنرى هذا الارتفاع في البداية،
ثم تذبذب، تذبذب، ولكن يميل إلى الذهاب إلى أسفل، ومن ثم يصل إلى قيمة
معينة، وعادةً ما تكون صفرًا، وتستمر عند الصفر. الآن، اسمحوا لي بأن أقوم بأخذ دقيقة
للحديث عن بدايات المسافات التقنية تلك بالتفاصيل. وقد تبين أن الشرط التقني هو
شيء مهم جدًا. وهو يقول إنه في كل تكرار T،
يمكننا أن نجد متعلمًا ضعيفًا. لذا فإن جذع القرار له خطأ مرجح
يبلغ على الأقل أقل قليلًا من 0.5. لذا فهي على الأقل أفضل من العشوائية. وهذا ما تقوله النظرية. ويبدو أن من البديهي، أنه سيظهر لنا
بواسطة التصنيف أن هذا أفضل من العشوائية،
حتى مع البيانات. ولكن اتضح بأن هذا
ليس ممكنًا دائمًا. لذا، ها هو مثال متناقض،
وهو حقًا مثال متناقض متطرف، ولكن هناك أمثلة أخرى حيث
من غير الممكن. إذن، على سبيل المثال،
إذا كانت لديك نقطة سلبية هنا، ولديك نقطة إيجابية فوقها،
فلن يكون هناك فرع قرار يمكنه أن يفصل هذه النقطة الإيجابية
على رأس هذه النقطة السلبية. لذا قد لا تتم تلبية شروط خوارزمية التعزيز، من نظرية AdaBoost للتعزيز. ومع ذلك، على الرغم من ذلك، عادة ما يأخذ
التعزيز خطأ التدريب الخاص بك إلى الصفر أو في مكان منخفض جدًا إذا كان عدد مرات التكرار
سيذهب إلى اللانهاية. لذا، نلاحظ ممارسة التناقص
على الرغم من أن هناك بعض الشروط التقنية في النظريات، حيث إنها
قد لا تذهب بالضبط إلى الصفر، ولكن قد تصبح حقًا، منخفضة للغاية. [موسيقى]