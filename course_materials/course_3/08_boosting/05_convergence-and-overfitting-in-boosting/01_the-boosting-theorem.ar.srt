1
00:00:00,159 --> 00:00:04,579
[موسيقى]

2
00:00:04,579 --> 00:00:08,798
فيما يلي سنستغرق بضع دقائق،
للتحدث بشأن نظرية AdaBoost الشهيرة

3
00:00:08,798 --> 00:00:13,470
وماهي آثارها العملية،
التي تكون مثيرة للاهتمام جدًا.

4
00:00:13,470 --> 00:00:18,930
لذلك، إذا كنت تتذكر، الطريقة التي ظهر
بها التعزيز هي أنه رواد "كيرنز" و"فاليانت"

5
00:00:18,930 --> 00:00:23,250
هذا السؤال الشهير، هل يمكنك دمج
مصنفات ضعيفة لإنشاء آخر قوي؟

6
00:00:24,350 --> 00:00:27,000
في الواقع، فاليانت هو
فائز بجائرة توزنغ، لذلك

7
00:00:27,000 --> 00:00:30,120
هو أستاذ شهير للغاية بجامعة هارفارد.

8
00:00:30,120 --> 00:00:33,510
وبعد عامين توصل شابيري

9
00:00:33,510 --> 00:00:37,840
لفكرة التعزيز،
مما غير مجال التعلم الآلي حقًا.

10
00:00:37,840 --> 00:00:43,260
وهكذا، إذا نظرت إلى تكرار

11
00:00:43,260 --> 00:00:47,480
التعزيز في المحور x هنا، في خطأ
التدريب لمجموعة البيانات الخاصة بنا.

12
00:00:47,480 --> 00:00:52,590
نلاحظ تأثير عملي للغاية
نراه في كثير من بيانات التعزيز.

13
00:00:52,590 --> 00:00:56,630
سنبدأ بخطأ تدريب سيئ حقًا.

14
00:00:56,630 --> 00:01:01,760
لذا فإن أول جذع قرار له
خطأ تدريب يبلغ 22.5%.

15
00:01:01,760 --> 00:01:03,150
لذا، ليس جيدًا على الإطلاق.

16
00:01:04,790 --> 00:01:08,230
وبعد ثلاثين مرة من التكرار،
كما شاهدنا منذ قليل،

17
00:01:08,230 --> 00:01:11,460
وسنقوم بالحصول على كافة نقاط البيانات
بشكل صحيح في بياناتنا التدريب

18
00:01:11,460 --> 00:01:14,150
وقد أظهرت لك نوعًا من حد
القرار المجنون لهذا.

19
00:01:14,150 --> 00:01:19,852
لذلك نرى انتقالًا سلسًا حيث يميل
خطأ التصنيف للانخفاض إلى أسفل.

20
00:01:19,852 --> 00:01:24,060
ويميل إلى الانخفاض وتذهب القيمة إلى الصفر،
ويبقى في الواقع عند مستوى الصفر.

21
00:01:24,060 --> 00:01:26,800
وهذه فكرة رئيسية
لنظرية التعزيز.

22
00:01:28,920 --> 00:01:35,930
لذا نرى هنا نظرية AdaBoost الشهيرة
الذي يحدد جميع الخيارات

23
00:01:35,930 --> 00:01:41,130
التي قمنا بها في الخوارزمية وكان لديها
الكثير من الأثر حقًا على التعلم الآلي.

24
00:01:41,130 --> 00:01:44,660
وتقول النظرية بشكل أساسي إنه
تحت بعض الظروف التقنية

25
00:01:44,660 --> 00:01:46,510
تكون جميع النظريات مثل هذا.

26
00:01:46,510 --> 00:01:50,700
الآن إذا كنت لتكون قادرًا على القول بأن بعض
القيود تنطبق، راجع المتجر للحصول على التفاصيل.

27
00:01:50,700 --> 00:01:54,090
ولذا نقول تحت بعض الشروط الفنية،

28
00:01:54,090 --> 00:02:00,070
خطأ التدريب للتصنيف
يذهب إلى الصفر، وكذلك عدد

29
00:02:00,070 --> 00:02:05,960
تكرارات العدد وبعض النماذج التي تعتبر
أن T تذهب إلى اللانهاية.

30
00:02:05,960 --> 00:02:10,830
لذلك وبعبارة أخرى وبشكل تصويري
سنرى أن نتوقع خطأ التدريب،

31
00:02:10,830 --> 00:02:15,200
وهو هذا الشيء هنا على المحور y،
للانتقال في نهاية المطاف إلى الصفر.

32
00:02:16,700 --> 00:02:18,910
والآن في نهاية المطاف.

33
00:02:18,910 --> 00:02:22,410
قد تتذبذب قليلًا
في الوسط، ومع ذلك،

34
00:02:22,410 --> 00:02:26,390
ما تقوله النظرية هو إنه يميل إلى
الانخفاض عمومًا،

35
00:02:26,390 --> 00:02:30,160
ويصبح في نهاية المطاف صفر،
ثم يستقر عند مستوى الصفر.

36
00:02:30,160 --> 00:02:33,200
ولذلك سنرى هذا الارتفاع في البداية،
ثم تذبذب، تذبذب، ولكن

37
00:02:33,200 --> 00:02:37,580
يميل إلى الذهاب إلى أسفل، ومن ثم يصل إلى قيمة
معينة، وعادةً ما تكون صفرًا، وتستمر عند الصفر.

38
00:02:38,690 --> 00:02:41,190
الآن، اسمحوا لي بأن أقوم بأخذ دقيقة
للحديث عن

39
00:02:41,190 --> 00:02:43,140
بدايات المسافات التقنية تلك بالتفاصيل.

40
00:02:43,140 --> 00:02:48,670
وقد تبين أن الشرط التقني هو
شيء مهم جدًا.

41
00:02:48,670 --> 00:02:52,890
وهو يقول إنه في كل تكرار T،
يمكننا أن نجد متعلمًا ضعيفًا.

42
00:02:52,890 --> 00:02:57,200
لذا فإن جذع القرار له خطأ مرجح
يبلغ على الأقل

43
00:02:57,200 --> 00:02:58,130
أقل قليلًا من 0.5.

44
00:02:58,130 --> 00:03:00,400
لذا فهي على الأقل أفضل من العشوائية.

45
00:03:00,400 --> 00:03:01,660
وهذا ما تقوله النظرية.

46
00:03:03,230 --> 00:03:06,630
ويبدو أن من البديهي، أنه سيظهر لنا
بواسطة التصنيف

47
00:03:06,630 --> 00:03:09,520
أن هذا أفضل من العشوائية،
حتى مع البيانات.

48
00:03:09,520 --> 00:03:12,120
ولكن اتضح بأن هذا
ليس ممكنًا دائمًا.

49
00:03:12,120 --> 00:03:17,490
لذا، ها هو مثال متناقض،
وهو حقًا مثال متناقض متطرف،

50
00:03:17,490 --> 00:03:20,120
ولكن هناك أمثلة أخرى حيث
من غير الممكن.

51
00:03:20,120 --> 00:03:22,670
إذن، على سبيل المثال،
إذا كانت لديك نقطة سلبية هنا،

52
00:03:22,670 --> 00:03:25,910
ولديك نقطة إيجابية فوقها،
فلن يكون هناك فرع قرار

53
00:03:25,910 --> 00:03:29,070
يمكنه أن يفصل هذه النقطة الإيجابية
على رأس هذه النقطة السلبية.

54
00:03:29,070 --> 00:03:33,380
لذا قد لا تتم تلبية شروط خوارزمية التعزيز،

55
00:03:33,380 --> 00:03:35,270
من نظرية AdaBoost للتعزيز.

56
00:03:35,270 --> 00:03:40,120
ومع ذلك، على الرغم من ذلك، عادة ما يأخذ
التعزيز خطأ التدريب الخاص بك إلى الصفر أو

57
00:03:40,120 --> 00:03:44,900
في مكان منخفض جدًا إذا كان عدد مرات التكرار
سيذهب إلى اللانهاية.

58
00:03:44,900 --> 00:03:48,193
لذا، نلاحظ ممارسة التناقص
على الرغم من أن هناك بعض

59
00:03:48,193 --> 00:03:51,531
الشروط التقنية في النظريات، حيث إنها
قد لا تذهب بالضبط إلى الصفر، ولكن قد تصبح حقًا،

60
00:03:51,531 --> 00:03:52,078
منخفضة للغاية.

61
00:03:52,078 --> 00:03:56,309
[موسيقى]