[موسيقى] الآن لنأخذ هذا المثال الثالث الذي
استخدمناه لتوضيح خوارزميات تعلم آلي مختلفة في هذه الوحدة
واستكشاف ذلك في سياق AdaBoost. وسوف تعطينا الكثير من التبصر بشأن
كيفية عمل التعزيز في الممارسة العملية. لذا بالنسبة للتصنيف الأول f1، نحن نعمل مباشرة من البيانات الأصلية،
حيث يكون لجميع النقاط نفس الترجيح. هذا صحيح. لذا فإن عملية التعلم لدينا
سوف تكون التعلم القياسي. لذلك لن يتغير شيء في
خوارزمية التعلم الخاص بك نظرًا لكل نقطة بيانات لها نفس الوزن. وفي هذه الحالة نتعلم فروع
القرارات، لذا فها هي حدود القرار التي تقوم بأفضل ما يمكنها لمحاولة
فصل الأمثلة الإيجابية من الأمثلة السلبية. وهي تنقسم عند حوالي 0 تمامًا. وهي في الواقع ناقص 0.07، إذا كنت تتذكر
من تصنيف شجرة القرار. إذن، هذا هو أول فرع للقرار، f1. الآن لمعرفة فرع القرار الثاني، f2 علينا إعادة ترجيح البيانات لدينا استنادًا إلى
مقدار ما قام به f1، وإلى أي مدى قام f1 بأداء جيد. لذا فسنقوم بإلقاء نظرة على
حدود القرار لدينا وسنقوم بترجيح نقاط البيانات
التي كانت أخطاء أعلى. وهنا في الصورة سأقوم بالإشارة إليها
بعلامات جمع وطرح أكبر. لذا إذا نظرتم إلى نقاط البيانات هنا
على اليسار، فقد كانت الأخطاء أو الناقص على هذا الجانب،
وعلامة الزائد هذه هنا. فقد كانت أيضًا أخطاء،
لذا قمنا بزيادة ترجيحنا وقمنا بتخفيض ترجيه جميع الآخرين،
ونحن نرى أن علامات الزائد هنا قد أصبحت هنا أكبر وعلامات الناقص
في هذه المنطقة أصبحت أكبر. إذًا هذه هي الطريقة التي نقوم من خلالها
بتحديث ترجيحنا. والآن دعونا ننظر إلى الخطوة التالية. تعلم التصنيف f2 في التكرار الثاني
استنادًا إلى هذه البيانات المرجحة. باستخدام البيانات المرجحة، سوف
نتعلم فرع القرار التالي. ويمكنك أن ترى أنه لا يزال لدينا الآن
انقسام عمودي، ولدينا انقسام أفقي، وهو
تقسيم أفضل للبيانات المرجحة. التقسيم لهذه الترجيحات على اليسار وهو رائع نوعًا ما. إذًا في التكرار الأول،
قررنا التقسيم عند x 1. وفي التكرار الثاني قمنا بالتقسيم
عند x2 وهو x2 أكبر من أو أقل من
1.3 أو نحو ذلك. وسترى أنه نجح في الحصول على جميع علامات الناقص
بشكل صحيح في الأعلى ولكنه قام ببعض ببعض الأخطاء في علامات الناقص في الجزء السفلي، ولكنه
حصل على علامات الزائد بشكل صحيح في الجزء السفلي. لذا بدلًا من الانقسام العمودي هنا،
لدينا الآن انقسام أفقي. إذًا تعلمنا الآن أن هناك
جذع القرار f1 وf2، والسؤال هنا هو كيف يمكننا الجمع بينهما؟ لذا إذا تابعت العمل باستخدام صيغة AdaBoost
فسترى أن w hat 1 ترجيح فرع القرار الأول سيكون 0.61، ثم w hat 2 ستكون 0.53. لذا فنحن نثق بفرع القرار الأول
أكثر قليلًا مما نثق في الثاني وهو ما يبدو منطقيًا. والثاني لا يبدو أنه جيد بنفس الدرجة،
ولكن عند إضافتهما معًا، تبدأ في الحصول على حد قرار
مثير جدًا للاهتمام. لذا تحصل على النقاط أعلى اليسار هنا
حيث نعتقد بالتأكيد أن y hat ناقص 1،
لذا بالتأكيد علامات سالبة. وفي أسفل اليمين هنا، يوجد بعض علامات الزائد المؤكدة حيث y hat يساوي زائد 1. ومن ثم للمنطقتين الأخريين، يمكننا أن نفكر في هذه
كمناطق أكبر من عدم اليقين. لذا فإن هذه غير مؤكدة
وهو ما يبدو منطقيًا في الوقت الحالي، ولكن بينما تضيف المزيد من فروع القرارات
فسكون أكثر تأكدًا من أن بعض النقاط في الجزء السفلي الأيسر من الطبقة
تكون سلبية وفي أعلى اليمين سلبية. الآن، إذا أبقينا أرقامنا تذهب حتى 30 تكرارًا
فإن أول شيء نلاحظه هو أننا نحصل على جميع نقاط البيانات
بشكل صحيح، لذا يكون خطأ التدريب لدينا هو 0. والشيء الثاني الذي ستلاحظه،
وسأقوم هنا باستخدام مصطلح تقني لهذا، هو أن حد
القرار مجنون. وهذا هو مصطلحنا التقني، ومن ثم إذا قمت
بجمع هذه الدواخل اثنين نكتشف أننا حقًا لا نثق في هذا التصنيف، وربما كنا نقوم بإفراط مطابقة البيانات. لذا تناسبها تمامًا في السلسلة في وقتٍ لاحق، فقد لا تؤدي بشكلٍ جيد مع القليل من الخطأ. إذًا فإن إفراط المطابقة الذي سيحدث في التعزيز، هو ما سنتحدث عنه لاحقًا، لذلك دعونا تأخذ نفسا عميقًا،
ونلخص ما قمنا به حتى الآن. لقد قمنا بوصف التصنيفات البسيطة،
وقلنا إننا سنتعلم التصنيفات البسيطة ونأخذ الدورة
بينها للقيام بالتنبؤات. ومن ثم يمكننا وصف
خوارزمية AdaBoost هذه، وهو نهج بسيط جدًا لتعلم تصنيف غير بسيط باستخدام
أسلوب التعزيز هذا حيث تقوم بتعزيز ترجيح
نقاط البيانات عندما نقوم بالأخطاء. وهي بسيطة للتنفيذ من الناحية العملية. [موسيقى]