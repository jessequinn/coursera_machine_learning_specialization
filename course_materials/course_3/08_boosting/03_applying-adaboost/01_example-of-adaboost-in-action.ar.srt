1
00:00:00,000 --> 00:00:05,421
[موسيقى]

2
00:00:05,421 --> 00:00:09,378
الآن لنأخذ هذا المثال الثالث الذي
استخدمناه لتوضيح

3
00:00:09,378 --> 00:00:13,290
خوارزميات تعلم آلي مختلفة في هذه الوحدة
واستكشاف ذلك في سياق AdaBoost.

4
00:00:13,290 --> 00:00:16,890
وسوف تعطينا الكثير من التبصر بشأن
كيفية عمل التعزيز في الممارسة العملية.

5
00:00:18,420 --> 00:00:20,770
لذا بالنسبة للتصنيف الأول f1،

6
00:00:20,770 --> 00:00:25,066
نحن نعمل مباشرة من البيانات الأصلية،
حيث يكون لجميع النقاط نفس الترجيح.

7
00:00:25,066 --> 00:00:27,844
هذا صحيح.

8
00:00:27,844 --> 00:00:32,120
لذا فإن عملية التعلم لدينا
سوف تكون التعلم القياسي.

9
00:00:33,440 --> 00:00:36,280
لذلك لن يتغير شيء في
خوارزمية التعلم الخاص بك

10
00:00:36,280 --> 00:00:38,250
نظرًا لكل نقطة بيانات لها نفس الوزن.

11
00:00:38,250 --> 00:00:43,330
وفي هذه الحالة نتعلم فروع
القرارات، لذا فها هي حدود القرار

12
00:00:43,330 --> 00:00:48,140
التي تقوم بأفضل ما يمكنها لمحاولة
فصل الأمثلة الإيجابية من الأمثلة السلبية.

13
00:00:48,140 --> 00:00:50,044
وهي تنقسم عند حوالي 0 تمامًا.

14
00:00:51,740 --> 00:00:57,120
وهي في الواقع ناقص 0.07، إذا كنت تتذكر
من تصنيف شجرة القرار.

15
00:00:57,120 --> 00:00:59,610
إذن، هذا هو أول فرع للقرار، f1.

16
00:00:59,610 --> 00:01:03,100
الآن لمعرفة فرع القرار الثاني، f2

17
00:01:03,100 --> 00:01:08,680
علينا إعادة ترجيح البيانات لدينا استنادًا إلى
مقدار ما قام به f1، وإلى أي مدى قام f1 بأداء جيد.

18
00:01:08,680 --> 00:01:12,790
لذا فسنقوم بإلقاء نظرة على
حدود القرار لدينا

19
00:01:12,790 --> 00:01:17,170
وسنقوم بترجيح نقاط البيانات
التي كانت أخطاء أعلى.

20
00:01:17,170 --> 00:01:21,710
وهنا في الصورة سأقوم بالإشارة إليها
بعلامات جمع وطرح أكبر.

21
00:01:21,710 --> 00:01:26,640
لذا إذا نظرتم إلى نقاط البيانات هنا
على اليسار، فقد كانت الأخطاء

22
00:01:26,640 --> 00:01:30,500
أو الناقص على هذا الجانب،
وعلامة الزائد هذه هنا.

23
00:01:31,638 --> 00:01:34,710
فقد كانت أيضًا أخطاء،
لذا قمنا بزيادة ترجيحنا

24
00:01:34,710 --> 00:01:38,200
وقمنا بتخفيض ترجيه جميع الآخرين،
ونحن نرى أن علامات الزائد هنا

25
00:01:38,200 --> 00:01:42,690
قد أصبحت هنا أكبر وعلامات الناقص
في هذه المنطقة أصبحت أكبر.

26
00:01:42,690 --> 00:01:44,490
إذًا هذه هي الطريقة التي نقوم من خلالها
بتحديث ترجيحنا.

27
00:01:45,940 --> 00:01:49,640
والآن دعونا ننظر إلى الخطوة التالية.

28
00:01:49,640 --> 00:01:55,720
تعلم التصنيف f2 في التكرار الثاني
استنادًا إلى هذه البيانات المرجحة.

29
00:01:56,930 --> 00:02:00,120
باستخدام البيانات المرجحة، سوف
نتعلم فرع القرار التالي.

30
00:02:00,120 --> 00:02:02,730
ويمكنك أن ترى أنه لا يزال لدينا الآن
انقسام عمودي،

31
00:02:02,730 --> 00:02:07,288
ولدينا انقسام أفقي، وهو
تقسيم أفضل للبيانات المرجحة.

32
00:02:07,288 --> 00:02:12,888
التقسيم لهذه الترجيحات على اليسار

33
00:02:12,888 --> 00:02:15,873
وهو رائع نوعًا ما.

34
00:02:15,873 --> 00:02:20,040
إذًا في التكرار الأول،
قررنا التقسيم عند x 1.

35
00:02:20,040 --> 00:02:23,378
وفي التكرار الثاني قمنا بالتقسيم
عند x2

36
00:02:23,378 --> 00:02:27,930
وهو x2 أكبر من أو أقل من
1.3 أو نحو ذلك.

37
00:02:27,930 --> 00:02:33,300
وسترى أنه نجح في الحصول على جميع علامات الناقص
بشكل صحيح في الأعلى ولكنه قام ببعض

38
00:02:33,300 --> 00:02:36,750
ببعض الأخطاء في علامات الناقص في الجزء السفلي، ولكنه
حصل على علامات الزائد بشكل صحيح في الجزء السفلي.

39
00:02:38,110 --> 00:02:42,820
لذا بدلًا من الانقسام العمودي هنا،
لدينا الآن انقسام أفقي.

40
00:02:42,820 --> 00:02:45,070
إذًا تعلمنا الآن أن هناك
جذع القرار f1 وf2،

41
00:02:45,070 --> 00:02:48,170
والسؤال هنا هو كيف يمكننا الجمع بينهما؟

42
00:02:48,170 --> 00:02:52,410
لذا إذا تابعت العمل باستخدام صيغة AdaBoost
فسترى أن w hat 1

43
00:02:52,410 --> 00:02:58,150
ترجيح فرع القرار الأول سيكون 0.61،

44
00:02:58,150 --> 00:02:59,090
ثم w hat 2 ستكون 0.53.

45
00:02:59,090 --> 00:03:02,410
لذا فنحن نثق بفرع القرار الأول
أكثر قليلًا مما نثق في الثاني

46
00:03:02,410 --> 00:03:03,510
وهو ما يبدو منطقيًا.

47
00:03:03,510 --> 00:03:06,720
والثاني لا يبدو أنه جيد بنفس الدرجة،
ولكن عند إضافتهما معًا،

48
00:03:06,720 --> 00:03:09,960
تبدأ في الحصول على حد قرار
مثير جدًا للاهتمام.

49
00:03:09,960 --> 00:03:15,270
لذا تحصل على النقاط أعلى اليسار هنا
حيث نعتقد بالتأكيد

50
00:03:15,270 --> 00:03:21,650
أن y hat ناقص 1،
لذا بالتأكيد علامات سالبة.

51
00:03:21,650 --> 00:03:29,070
وفي أسفل اليمين هنا، يوجد بعض علامات الزائد المؤكدة حيث y hat يساوي زائد 1.

52
00:03:29,070 --> 00:03:32,800
ومن ثم للمنطقتين الأخريين،

53
00:03:32,800 --> 00:03:37,320
يمكننا أن نفكر في هذه
كمناطق أكبر من عدم اليقين.

54
00:03:37,320 --> 00:03:44,480
لذا فإن هذه غير مؤكدة
وهو ما يبدو منطقيًا في الوقت الحالي،

55
00:03:44,480 --> 00:03:47,510
ولكن بينما تضيف المزيد من فروع القرارات
فسكون أكثر تأكدًا من أن بعض

56
00:03:47,510 --> 00:03:52,040
النقاط في الجزء السفلي الأيسر من الطبقة
تكون سلبية وفي أعلى اليمين سلبية.

57
00:03:52,040 --> 00:03:57,260
الآن، إذا أبقينا أرقامنا تذهب حتى 30 تكرارًا
فإن أول شيء

58
00:03:57,260 --> 00:04:01,870
نلاحظه هو أننا نحصل على جميع نقاط البيانات
بشكل صحيح، لذا يكون خطأ التدريب لدينا هو 0.

59
00:04:01,870 --> 00:04:08,376
والشيء الثاني الذي ستلاحظه،
وسأقوم هنا باستخدام مصطلح تقني

60
00:04:08,376 --> 00:04:12,392
لهذا، هو أن حد
القرار مجنون.

61
00:04:12,392 --> 00:04:16,818
وهذا هو مصطلحنا التقني، ومن ثم إذا قمت
بجمع هذه الدواخل اثنين نكتشف

62
00:04:16,818 --> 00:04:19,769
أننا حقًا لا نثق في هذا التصنيف،

63
00:04:19,769 --> 00:04:22,200
وربما كنا نقوم بإفراط مطابقة البيانات.

64
00:04:24,370 --> 00:04:25,890
لذا تناسبها تمامًا في السلسلة في وقتٍ لاحق،

65
00:04:25,890 --> 00:04:29,520
فقد لا تؤدي بشكلٍ جيد مع القليل من الخطأ.

66
00:04:29,520 --> 00:04:34,050
إذًا فإن إفراط المطابقة الذي سيحدث في التعزيز،

67
00:04:34,050 --> 00:04:35,930
هو ما سنتحدث عنه لاحقًا،

68
00:04:37,180 --> 00:04:40,320
لذلك دعونا تأخذ نفسا عميقًا،
ونلخص ما قمنا به حتى الآن.

69
00:04:40,320 --> 00:04:44,280
لقد قمنا بوصف التصنيفات البسيطة،
وقلنا إننا سنتعلم

70
00:04:44,280 --> 00:04:47,440
التصنيفات البسيطة ونأخذ الدورة
بينها للقيام بالتنبؤات.

71
00:04:47,440 --> 00:04:49,550
ومن ثم يمكننا وصف
خوارزمية AdaBoost هذه،

72
00:04:49,550 --> 00:04:52,360
وهو نهج بسيط جدًا لتعلم

73
00:04:52,360 --> 00:04:55,200
تصنيف غير بسيط باستخدام
أسلوب التعزيز هذا

74
00:04:55,200 --> 00:04:59,030
حيث تقوم بتعزيز ترجيح
نقاط البيانات عندما نقوم بالأخطاء.

75
00:04:59,030 --> 00:05:05,235
وهي بسيطة للتنفيذ من الناحية العملية.

76
00:05:05,235 --> 00:05:06,669
[موسيقى]