1
00:00:00,000 --> 00:00:04,585
[موسيقى]

2
00:00:04,585 --> 00:00:08,752
والآن بعد أن تحدثنا حول عدة نماذج،
دعونا نتعمق في

3
00:00:08,752 --> 00:00:14,330
مسألة التعزيز بشكل أكثر عمومية، ثم بعد ذلك
نعرض مثالًا محددًا حول خوارزمية التعزيز.

4
00:00:14,330 --> 00:00:18,000
فكر في إحدى مشكلات التعلم حيث
نأخذ بعض البيانات

5
00:00:18,000 --> 00:00:21,630
ونتعرف على أحد المصنفات والذي يعطينا بدوره
بعض المخرجات والتي يرمز لها (f(x،

6
00:00:21,630 --> 00:00:26,205
والتي نستخدمها للتنبؤ ببعض البيانات لذلك
نقول أن ((y^ = sign(f(x.

7
00:00:27,280 --> 00:00:28,920
لذا، إذا أخذنا بعض البيانات

8
00:00:28,920 --> 00:00:32,520
وحاولنا إجراء تنبؤ، فحينها قد نقابل
أحد فروع القرارات.

9
00:00:32,520 --> 00:00:35,720
لذا، لنفترض أننا نحاول تقسيم الدخل،
فننظر إلى

10
00:00:35,720 --> 00:00:40,020
الأشخاص ذوي الدخل الأكبر من 100000، والأشخاص
ذوي الدخل الأقل من 100000،

11
00:00:40,020 --> 00:00:42,320
وكيف نحصل على التنبؤات
الفعلية التي نقوم بها؟

12
00:00:42,320 --> 00:00:44,432
حسنًا، سننظر إلى صفوف
البيانات لدينا،

13
00:00:44,432 --> 00:00:49,015
وتحديدًا الصفوف حيث يكون الدخل أكبر من 100000
ونلاحظ أنه بالنسبة لتلك الصفوف،

14
00:00:49,015 --> 00:00:54,820
فهناك ثلاثة قروض آمنة وقرض واحد به مخاطرة، لذا فسيكون
التنبؤ لدينا مبن على أن y^ = آمن.

15
00:00:54,820 --> 00:01:00,369
والآن، إذا نظرنا إلى الدخول التي أقل من 100000،
فسنلاحظ أن لدينا أربعة قروض آمنة

16
00:01:00,369 --> 00:01:05,730
وثلاثة قروض بها مخاطرة، لذا فمرة أخرى نحن نجري التنبؤ على كون
العملية آمنة، لذلك في كلا الجانبين سنجري التنبؤ على الأمان.

17
00:01:05,730 --> 00:01:08,890
وكما يتضح من فرعي القرار اللذين أنشأناهما،
أن الأمر يبدو حسنًا، ولكن

18
00:01:08,890 --> 00:01:10,830
الأمر ليس رائعًا بالنسبة للبيانات.

19
00:01:10,830 --> 00:01:14,277
وبالتالي، فإن فروع القرار ليست
كافية لتحتوي المعلومات

20
00:01:14,277 --> 00:01:15,760
لذا، فنحن بحاجة إلى بيانات أخرى.

21
00:01:15,760 --> 00:01:18,320
وما سيقوم به التعزيز هو،
أخذ فرع القرار هذا،

22
00:01:18,320 --> 00:01:21,660
وتقييمه والنظر إلى مدى كفاءته
بالنسبة للبيانات لدينا

23
00:01:21,660 --> 00:01:26,420
ثم يتعرف على فرع القرار التالي وعلى
مصنف ضعيف آخر، ثم بعد ذلك

24
00:01:26,420 --> 00:01:30,410
يقوم المصنف بالتركيز على نقاط البيانات التي فشل
المصنف الأول في التركيز عليها.

25
00:01:31,410 --> 00:01:33,803
وبعبارة أخرى، هذه نقطة
هامة للغاية.

26
00:01:33,803 --> 00:01:37,938
بعبارة أخرى، سننظر إلى النقاط التي لا نزال
نرتكب بها أخطاء حتى الآن

27
00:01:37,938 --> 00:01:40,782
وسنقوم بزيادة نسبة أو تأثير

28
00:01:40,782 --> 00:01:44,610
أو مدى اهتمامنا بهذه النقاط التي
نرتكب بها الأخطاء.

29
00:01:44,610 --> 00:01:47,530
ثم نتعرف على مصنف آخر
يهتم بتلك النقاط

30
00:01:47,530 --> 00:01:49,620
التي نرتكب أخطاء بها ثم نتعرف
على مصنف آخر وآخر.

31
00:01:49,620 --> 00:01:52,750
وكما سنرى في نهاية المطاف، فقد قمنا في
الواقع بتجميع مصنف هائل.

32
00:01:54,120 --> 00:01:57,460
ما الذي يعنيه التعرف على نقاط البيانات
التي نرتكب بها الأخطاء؟

33
00:01:57,460 --> 00:02:00,640
ما يعنيه هو أننا سنقوم بتعيين الترجيح

34
00:02:00,640 --> 00:02:05,255
αi وهو رقم موجب، إلى كل نقطة بيانات
في مجموعة البيانات لدينا.

35
00:02:06,300 --> 00:02:10,740
ويعني هذا الترجيح عندما يكون مرتفعًا، أن
نقطة البيانات تلك ذات أهمية.

36
00:02:10,740 --> 00:02:15,370
لذا، يمكننا تخيل أحد مشكلات التعلم حيث
يكون لدينا بيانات ليست مشابهة

37
00:02:15,370 --> 00:02:19,570
لتلك التي تناولناها حتى الآن في هذه الدورة، ولكن
لدينا بيانات مع ترجيحات مرتبطة بها.

38
00:02:19,570 --> 00:02:21,760
والآن سنتعلم من البيانات المرجحة.

39
00:02:21,760 --> 00:02:25,350
ما يوجد لنتعلمه من البيانات المرجحة، طريقة
التفكير في ذلك الأمر هو أن

40
00:02:25,350 --> 00:02:29,810
ترجيحات αi تلك تتوافق نوعًا ما

41
00:02:29,810 --> 00:02:33,140
مع نقاط البيانات التي تحتسب أكثر من مرة
واحدة إذا كانت أكبر من واحد.

42
00:02:33,140 --> 00:02:34,840
على سبيل المثال، إذا كانت αi تساوي 2،

43
00:02:34,840 --> 00:02:37,686
فيمكنك الاعتقاد في أن نقطة البيانات
هذه تحتسب مرتين.

44
00:02:37,686 --> 00:02:40,570
وإذا كانت αi تساوي النصف فيمكنك أن تعتقد
أن نقطة بيانات هذه يتم احتسابها

45
00:02:40,570 --> 00:02:41,890
كنصف نقطة بيانات.

46
00:02:41,890 --> 00:02:45,308
ولكن كل شيء في خوارزمية
التعلم يظل تمامًا كما هو.

47
00:02:45,308 --> 00:02:49,686
الأمر فقط أنه بدلًا من احتساب عدد نقاط البيانات
فنحن نحتسب ترجيحات نقاط البيانات.

48
00:02:49,686 --> 00:02:50,300
.

49
00:02:50,300 --> 00:02:53,240
وهذا ما يحدث في نهج فرع
القرار لدينا؟

50
00:02:53,240 --> 00:02:57,406
لذا، فقد كان لدينا فرع القرار الأول،
والذي لم يكن رائعًا خاصةً 

51
00:02:57,406 --> 00:03:01,850
بالنسبة للأشخاص ذوي الدخل المنخفض، لذا فقد
قمنا بالتعرف على الترجيح الأعلى

52
00:03:01,850 --> 00:03:06,453
بالنسبة للنقاط التي نرتكب بها الأخطاء، والآن
نتعرف على فرع القرار الجديد.

53
00:03:06,453 --> 00:03:10,377
لذا، فمرة أخرى سنقوم بتقسيم الدخل إلى أكبر
من 100000 وأصغر من 100000.

54
00:03:10,377 --> 00:03:14,167
وعندما ننظر إلى تصنيفات القرارات التي نجريها،
بالنسبة للدخل الأكبر

55
00:03:14,167 --> 00:03:18,305
من 100000، فما نفعله هو أننا نقوم بجمع
ترجيح نقاط البيانات التي تشكل مخاطرة.

56
00:03:18,305 --> 00:03:20,315
ولأن الدخل أكبر من 100000،

57
00:03:20,315 --> 00:03:24,221
فإننا في هذه الحالة سنقوم بجمع 0.5 و0.8

58
00:03:24,221 --> 00:03:29,653
و 0.7 حيث يصل مجموعهم إلى 2،
ثم بعد ذلك

59
00:03:29,653 --> 00:03:33,970
بالنسبة للنقاط التي بها مخاطرة فالمجموع 1.2، لذا
فسنقوم بإجراء التنبؤ حيث تكون y^ = آمن.

60
00:03:33,970 --> 00:03:36,440
لذا، فهذا هو المجموع المرجح
لنقاط البيانات.

61
00:03:36,440 --> 00:03:40,270
وبالنسبة للدخل الأقل من 100000،
فتطبق نفس الفكرة.

62
00:03:40,270 --> 00:03:42,430
سنأخذ نقاط البيانات، ونبحث عن
النقاط التي بها مخاطرة

63
00:03:42,430 --> 00:03:44,930
والنقاط الآمنة، ونقوم بحساب مجموع
ترجيح هذه البيانات.

64
00:03:44,930 --> 00:03:48,038
وسنلاحظ أن إجمالي الترجيح بالنسبة للقروض
التي بها مخاطرة هو 6.5،

65
00:03:48,038 --> 00:03:53,570
في حين أن إجمالي الترجيح للقروض الآمنة هو 3، لذلك فنحن
سنجري التنبؤ بناءً على القروض التي بها مخاطرة.

66
00:03:53,570 --> 00:03:56,140
لذا، فإن فرع القرار هذا سيكون
أفضل بعض الشيء.

67
00:03:56,140 --> 00:03:59,440
وبالتالي، سنقوم بجمع فرع القرار هذا مع
الفرع السابق وغيرهما وغيرهما

68
00:03:59,440 --> 00:04:03,340
حتى نقوم بإنشاء مصنف المجموعات هذا.

69
00:04:04,380 --> 00:04:08,440
والآن، فإن تلك الفكرة حول التعلم من البيانات المرجحة
لا تتمحور فقط حول فروع القرارات.

70
00:04:08,440 --> 00:04:13,608
فقد اتضح أن معظم خوارزميات التعلم
الآلي تتقبل البيانات المرجحة.

71
00:04:13,608 --> 00:04:17,120
لذا، سأوضح باختصار شديد ما يحدث
إذا كنت تجري انحدارًا لوجستيًا

72
00:04:17,120 --> 00:04:19,410
وكانت لديك بيانات مرجحة.

73
00:04:19,410 --> 00:04:22,460
إذا نظرت إلى المعادلة الموجودة بالأسفل هنا،
فستجد أنها تمامًا تلك المعادلة

74
00:04:22,460 --> 00:04:26,250
المتعلقة بمشتقة الانحدار اللوجستي، أو
دالة التحديث التي سنقوم بها.

75
00:04:26,250 --> 00:04:30,500
وهذا هو الأمر الذي ستقوم بتطبيقه إذا كنت
تتعامل مع نموذج للانحدار اللوجستي.

76
00:04:30,500 --> 00:04:32,250
وتقول الآن، حسنًا، إن كل ما لدي
هي تلك البيانات المرجحة،

77
00:04:32,250 --> 00:04:34,480
وعلي إعادة تنفيذ كل
شيء من البداية.

78
00:04:34,480 --> 00:04:36,190
يا إلهي، يا إلهي، يا إلهي.

79
00:04:36,190 --> 00:04:38,192
واتضح في النهاية أن الأمر
في غاية البساطة.

80
00:04:38,192 --> 00:04:39,500
يجب عليك النظر إلى منتصف المعادلة،

81
00:04:39,500 --> 00:04:42,950
حيث لدينا مجموع نقاط البيانات هنا.

82
00:04:42,950 --> 00:04:44,880
والآن، نحن نقوم ببساطة بحساب
مجموع نقاط البيانات.

83
00:04:44,880 --> 00:04:47,740
سنقوم فقط بحساب الترجيح لقيمة
كل نقطة بيانات.

84
00:04:47,740 --> 00:04:53,920
لذا، فنحن فقط في حاجة إلى إضافة ذلك الترجيح αi إلى
كل حد من حدود المجموع، وينتهي الأمر.

85
00:04:53,920 --> 00:04:56,040
والآن، يصبح لدينا الانحدار اللوجستي
للبيانات المرجحة.

86
00:04:57,520 --> 00:05:00,640
لذا، فقد قمنا بعرض مثالين حول فروع القرارات
والانحدار اللوجستي، ولكن

87
00:05:00,640 --> 00:05:03,640
بشكل عام، فيمكن معرفة
البيانات بسهولة.

88
00:05:03,640 --> 00:05:06,230
لذا، فيمكن إظهار التعزيز
كخوارزمية جشعة

89
00:05:06,230 --> 00:05:07,820
بالنسبة للتعرف على مجموعة
من البيانات.

90
00:05:07,820 --> 00:05:11,326
فنحن نقوم بتهيئة المصنف الأول من خلال البيانات،
ولنفترض أنه (f1(x،

91
00:05:11,326 --> 00:05:17,150
فإذا كان لديك (f1(x فقط، فيمكنك إجراء التنبؤ على (sign(f1
لتصبح النتيجة التي ستحصل عليها من y^.

92
00:05:17,150 --> 00:05:19,580
وبعد ذلك، ستقوم بإعادة حساب
ترجيح البيانات لديك.

93
00:05:19,580 --> 00:05:23,730
وبحساب ترجيح المزيد من نقاط البيانات، فنحن نرتكب أخطاء،
وحيث يرتكب المصنف f1 الأخطاء.

94
00:05:23,730 --> 00:05:26,632
والآن سنقوم بوضع مصنف
آخر وهو f2

95
00:05:26,632 --> 00:05:30,744
وسنقوم بالتعرف على معاملات كل واحد
من هذه المصنفات، ثم يأتي التنبؤ

96
00:05:30,744 --> 00:05:36,154
بالنسبة لخطوتين من كل ذلك وسيكون
w^1 f1 + w^2 f2.

97
00:05:36,154 --> 00:05:38,360
وحاصل ضرب ذلك في sign سيكون y^.

98
00:05:39,770 --> 00:05:44,656
وذلك هو نوعًا ما فكرة عمل التعزيز، حيث تستمر
في إضافة مصنفات جديدة،

99
00:05:44,656 --> 00:05:49,458
وتقوم بتحسين الترجيحات للتركيز على

100
00:05:49,458 --> 00:05:54,367
نقاط بيانات أكثر صعوبة، ثم تتعرف على المعاملات
الموجودة بين المصنفات المختلفة.

101
00:05:54,367 --> 00:05:58,909
[موسيقى]