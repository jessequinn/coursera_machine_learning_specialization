1
00:00:00,000 --> 00:00:04,680
[Musik]

2
00:00:04,680 --> 00:00:08,562
Dieses Modul behandelt wir einen speziellen Boosting-Algorithmus namens ...

3
00:00:08,562 --> 00:00:09,710
AdaBoost

4
00:00:09,710 --> 00:00:13,340
AdaBoost ist einer der frühen Machine-Learning-Algorithmen für Boosting.

5
00:00:13,340 --> 00:00:14,490
Er ist extrem nützlich.

6
00:00:14,490 --> 00:00:16,710
Er ist sehr einfach zu implementieren.

7
00:00:16,710 --> 00:00:18,400
Und er ist sehr effektiv.

8
00:00:18,400 --> 00:00:18,940
Es gibt andere.

9
00:00:18,940 --> 00:00:23,087
Ich werde einen anderen interessanten Algorithmus gegen Ende des Moduls erwähnen, aber ...

10
00:00:23,087 --> 00:00:24,825
... lasst und mit AdaBoost beginnen.

11
00:00:26,040 --> 00:00:29,332
Dies ist der berühmte AdaBoost-Algorithmus, ...

12
00:00:29,332 --> 00:00:35,690
... der von Freund und Schapire 1999 entwickelt wurde. Es ist ein erstaunlich nützlicher Algorithmus.

13
00:00:35,690 --> 00:00:38,280
Wir beginnen, indem wir jeden Datensatz als gleichwertig betrachten.

14
00:00:38,280 --> 00:00:40,040
Man weiß nicht, welche Datensätze für den Algorithmus schwierig und welche einfach sind.

15
00:00:40,040 --> 00:00:41,310
Sie haben alle die gleiche Gewichtung.

16
00:00:41,310 --> 00:00:46,500
Also können wir ihnen allen die gleiche Gewichtung zuweisen. Wir fangen an, indem wir ihnen das Gewicht 1 durch N geben, ...

17
00:00:46,500 --> 00:00:51,480
... da dies alles ein wenig besser funktionieren lässt.

18
00:00:51,480 --> 00:00:54,995
Wir werden auf späteren Folien diskutieren, warum das so ist, aber ...

19
00:00:54,995 --> 00:00:58,745
... wir fangen damit an, dass alle Datensätze die gleiche Gewichtung haben. Das wird einheitliche Gewichtung genannt.

20
00:00:58,745 --> 00:01:01,455
In diesem Fall ist α i 1 durch N.

21
00:01:01,455 --> 00:01:06,595
Für jede Iteration des AdaBoost-Algorithmus, also während wir den ersten ...

22
00:01:07,705 --> 00:01:10,555
Entscheidungsbaum-Stumpf oder den ersten simplen Klassifizierer oder den ersten gewichteten Klassifizierer, ...

23
00:01:10,555 --> 00:01:12,595
den zweiten oder den dritten oder irgendeinen bis zur T-ten Iteration lernen, werden wir die α i aktualisieren.

24
00:01:12,595 --> 00:01:17,665
Wir lernen f t anhand der mit α i gewichteten Datensätze.

25
00:01:19,030 --> 00:01:22,775
Das sind die Daten. Die Gewichtung ist am Anfang 1 durch N, aber ...

26
00:01:22,775 --> 00:01:24,771
sie verändert sich mit der Zeit.

27
00:01:24,771 --> 00:01:31,132
Dann berechnen wir den Koeffizienten w Dach t für ...

28
00:01:31,132 --> 00:01:37,720
diesen neuen Klassifizierer f t, den wir lernten.

29
00:01:37,720 --> 00:01:40,770
Dann berechnen wir die Gewichtungen α i neu.

30
00:01:42,070 --> 00:01:47,213
So fahren wir fort. Sind wir fertig, so sagen wir, dass die Vorhersage y Dach, ...

31
00:01:47,213 --> 00:01:52,860
... das Signum der Kombination von f1, f2, f3, ...

32
00:01:52,860 --> 00:01:57,844
... f4, gewichtet mit den zuvor gelernten Koeffizienten ist.

33
00:01:59,210 --> 00:02:02,730
Es gibt demnach 2 fundamentale Probleme, die zu addressieren sind, ...

34
00:02:02,730 --> 00:02:05,240
... wenn wir über AdaBoost nachdenken.

35
00:02:05,240 --> 00:02:10,616
Eines ist, wie wir den Koeffizienten w Dach t berechnen.

36
00:02:10,616 --> 00:02:16,125
Lasst uns dies, in diesem Modul, das "Problem 1" nennen.

37
00:02:16,125 --> 00:02:22,330
Problem 1 ist die Frage, wie viel Vertrauen in einzelne f t haben.

38
00:02:23,840 --> 00:02:26,830
Vertrauen wir einem f t sehr, so sollte es eine sehr hohe Gewichtung haben.

39
00:02:26,830 --> 00:02:29,720
Falls wir f t nicht sehr vertrauen, ...

40
00:02:29,720 --> 00:02:34,220
sollten wir ihm eine geringe Gewichtung, oder einen niedrigen Koeffizienten zuweisen.

41
00:02:34,220 --> 00:02:39,850
"Problem 2" ist, wie wir die Gewichtung für die Datensätze neu berechnen.

42
00:02:39,850 --> 00:02:41,311
Lasst uns das "Problem 2" nennen.

43
00:02:41,311 --> 00:02:46,891
Problem 2 ist also die Frage wie ...

44
00:02:46,891 --> 00:02:52,031
... gewichten wir Fehler vergleichsweise stärker?

45
00:02:52,031 --> 00:02:54,291
Wir wollen die Gewichtung von Fehlern verstärken.

46
00:02:54,291 --> 00:02:58,669
Im Hauptteil dieses Moduls werden wir darüber sprechen, wie ...

47
00:02:58,669 --> 00:03:02,398
wir w Dach t berechnen und wie wir α i aktualisieren und ...

48
00:03:02,398 --> 00:03:07,610
... es wird ziemlich einfach werden, relativ intuitiv und sehr nützlich.

49
00:03:07,610 --> 00:03:11,869
[Musik]