[موسيقى] >>وأخيرًا، فنحن في حاجة إلى معالجة هذه المشكلة التقنية التي
أشرنا إليها عندما قمنا بتوحيد ترجيحات نقاط البيانات لتبدأ من 1 وصولًا إلى N، وذلك عندما
كانت لدينا ترجيحات موحدة. والتي ينبغي أن تقوم بتوحيد ترجيحات نقاط
البيانات من خلال التكرار، فعلى سبيل المثال، إذا أخذت نقطة البيانات Xi،
على افتراض أنها غالبًا ما تمثل خطأ، حيث نقوم بضرب الترجيح في رقم
موجب مرة أخرى، مرارًا وتكرارًا. لنفترض أننا سنضربه في 2 مرارًا وتكرارًا،
حتى يصبح الترجيح كبيرًا للغاية. ومن ناحية أخرى، يمكنك أن تأخذ نقطة البيانات Xi
والتي غالبًا ما تمثل شيئًا صحيحًا، وتقوم بضربها بعددٍ ما أقل من 1، ولنفترض
أننا سنضربها في نصف. ثم تستمر في ضرب القيمة في نصف مرارًا وتكرارًا،
حتى يمكن أن يصبح هذا الترجيح، صغيرًا للغاية. وبالتالي، يمكن أن تؤدي هذه المشكلة إلى حالة من
عدم الاستقرار العددي في هذا النهج. وكما تعلم، قد لا يتمكن جهاز الكمبيوتر لديك من التصرف على
نحوٍ جيد في وجود هذا الكم الجنوني من البيانات المرجحة. لذا، فما يجب القيام به، أنه فيما يلي
كل تكرار، يجب أن نقوم بتوحيد ترجيحات جميع نقاط
البيانات لتصل إلى واحد. فبشكل أساسي، نقوم بقسمة كل αi على مجموع
نقاط البيانات مضروبًا في αi، لذا، فإن هذا النهج يحافظ على الترجيحات في نطاق معقول،
ويتجنب عدم الاستقرار العددي. لذا، دعونا نلخص خوارزمية AdaBoost. حيث نبدأ بالترجيحات المتعادلة
والموحدة والمتساوية بالنسبة لكل نقاط البيانات. ونتعرف على المصنف Ft. ونحاول إيجاد المعامل لهذا المصنف، استنادًا إلى مدى
جودته فيما يتعلق بالخطأ المرجح. ثم نقوم بتحديث الترجيحات لتستوعب أخطاء
بنسبة أكثر من الأشياء الصحيحة لدينا. وأخيرًا، ثم نوحد الترجيحات عن طريق قسمة كل
قيمة على المجموع الكلي للترجيحات. وتوحيد الترجيحات هذا هو أمر
مهم من الناحية العملية. لذا، كل ما يتعلق بخوارزمية AdaBoost،
فهي رائعة وتعمل بشكل جيد للغاية، وسهلة الاستخدام حقًا. >> [موسيقى]