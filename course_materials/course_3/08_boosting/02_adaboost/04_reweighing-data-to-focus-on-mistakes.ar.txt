[صوت] لقد بدأنا مع
وجود ألفا بشكل موحد، كمثل جميع نقاط التدريب، واحد فوق n، ونريد الآن تغييرها للتركيز أكثر على تلك نقاط البيانات الصعبة،
 حيث نقوم بالأخطاء. ولهذا، فإن السؤال، أين ينتج أخطاء عن ft، أو متى تكون نقاط البيانات ft صحيحة؟ إذا كان يوجد في ft نقاط بيانات محددة، تكون فيها xi صحيحة،
 فإننا نود تقليل ألفا i، لأننا حصلنا عليها صحيحة. ولكن في حالة حصلنا على xi خاطئة،
 فإننا سنود زيادة ألفا i لدينا، ولهذا سنقوم بتصنيف منازلنا في أسلوب القرار التالي،
 ويؤدي بشكل أفضل في تلك المدخلات المحددة. ومجددًا، فإن نظرية AdaBoost
 تقدم إلينا بصيغة تهويل بشكل ما، حول كيفية تحديث الأحجام بالنسبة لi. ولكنك إن تفحصناها للحظات،
فسنرى أنها بديهية للغاية، وأنه يوجد شيء ما جيد. ولهذا، دعونا نلقِ نظرة سريعة عليها. ولهذا، فهي توضح حصول ألفا i على تحديث،
 يعتمد على إذا ما كانت ft قد حصلت على نقاط البيانات بشكل صحيح،
 وبشكل صائب، أو أنها قامت به بشكل خطأ. وفي هذه الحالة،
 فإننا سنلاحظ أننا سنقوم بزيادة حجم نقاط البيانات عندما نقوم بأخطاء
، وسنقوم بتقليل حجمها عندما نقوم بذلك على الوجه الصحيح. دعونا نلقِ نظرة على هذا. دعونا نستعرض واحد xi ودعونا نفترض
 أننا حصلنا عليه بشكل صحيح. ولهذا، فهي على السطر العلوي، ولاحظ أن المعادلة تعتمد عليها، مهما كانت المعاملات التي تم تعيينها لهذا المصنف، ولهذا، كان المصنف جيدًا. قمنا فقط بتغيير الطريقة للأكثر، ولكن إذا كانت المصنفات سيئة،
 فسنقوم بتغيير الأحجام للأقل. لذا دعونا نقل إن المصنف كان جيدًا،
وقمنا بإعطاء حجمه 2.3. لذا، ما نقوم به هنا، هو أننا نقوم برؤية الصيغة، نقوم بضرب ألفا i في e مرفوعًا لأس (Ŵ (t-،
 والذي يكون 2.3. وإذا استخدمت الآلة الحاسبة، فستلاحظ أن الناتج 0.1. ولهذا، قمنا بنقل نقاط البيانات إلى الجانب الصحيح الخاص بنا، وقمنا بضرب الحجم لنقاط البيانات تلك في 0.1،
 ولذا قمنا بالقسمة على 10. إذًا، ما التأثير الذي يُحدثه ذلك؟ سنقوم بتقليل أهمية نقاط البيانات هذه، ff, xi, وyi، ولذا، فهي نقاط البيانات المحددة تلك. ولذا، دعونا نلقِ نظرة، على حالة
تكون فيها نقاط البيانات صحيحة، لأن التكلفة التي تعلمناها عشوائية. ولهذا، فإنها ستبقى صفرًا،
 كما قمنا بمناقشة ذلك في الشرائح القليلة السابقة. ولهذا، فإن الحجم الكلي لها 0.5 هو الحجم 0. ونقوم في هذه الحالة، بضرب المعامل L5،
 في e مرفوعة لأس -0، ويساوي ذلك 1، فماذا يعنيه ذلك؟ يعني أنني عندما أقوم بجعل الأهمية
 لنقاط البيانات تلك متساوية، فإن ذلك يبدو منطقيًا، ولهذا، تم تصنيف ذلك على أنه مروع،
وأعطيناه حجمًا يساوي 0، وسنقوم بتجاهلها، وبما أننا سنقوم بتجاهلها،
 فلن تقوم بتغيير أي شيء حول كيفية تقييم جميع نقاط البيانات،
 وسنقوم فقط بالإبقاء عليها كما هي، كما لو أنه لم يحدث أي شيء،
 حيث لم يتغير أي شيء في كامل مجموعتها. ودعونا الآن نلقِ نظرة على الحالة الأخرى،
عندما نقوم بأخطاء بالفعل، ولهذا دعونا نقل إن لدينا xi غير صحيح،
لأننا قمنا بخطأ. ونحن في هذه الحالة، على الخط الثاني هنا، حيث إن كان مصنفًا جيدًا،
فسيكون به Ŵt يساوي 2.3، ومن ثم، فإننا سنقوم بضرب الحجم بكل مرفوع لأس يساوي 2.3. ولهذا فإن e مرفوعة لأس 2.3، حيث إن قمت بعملية
حسابية تساوي 9.98، فستكون بالتقريب 10. ولذا، يعد ذلك أكبر بعشر مرات. ولهذا، ما نقوم به هو زيادة الأهمية لهذا الخطأ بشكل جلي. ولهذا فإن المصنف التالي سيهتم بشكل أكبر بنقاط البيانات المحددة تلك، حيث إنها كانت خاطئة. وأخيرًا، وبشكل سريع جدًا، ما الذي يحدث إذا قمنا بخطأ، ولكن يكون لدينا هذا المصنف العشوائي
 والذي به حجم يساوي 0، والذي لا نآبه له. ولذا فإن ناتج الضرب هنا يساوي e مرفوعًا ل0. والذي مجددًا يساوي 1،
 مما يعني أن الأهمية لكلٍ من نقاط البيانات تلك متساوية. ولهذا، فهذا أمر جيد جدًا، حيث نرى الآن
هذا التحديث الرائع من AdaBoost، والذي يسبب زيادة منطقية لأحجام نقاط البيانات، حيث نقوم بالأخطاء ويقلل من تلك التي لا نقوم فيها بأخطاء في المحاكي وسنقوم باستخدامها في خوارزمية AdaBoost الخاصة بنا. ولذا، إن قمنا بتحديث الخوارزمية الخاصة بنا، أو قمنا بتكديسها من خلال أحجام موحدة،
 فسنتعلم المصنف f من t. قمنا بتحديثها، أو حساب معاملها Ŵt. ويمكننا الآن تحديث الأحجام لنقاط البيانات، ألفا i. باستخدام الصيغة المبسطة من الشريحة السابقة
 والتي تزيد من حجم الأخطاء وتقلل الأحجام للتصنيفات الصحيحة. [موسيقى]