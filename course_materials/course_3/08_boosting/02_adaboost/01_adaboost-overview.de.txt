[Musik] Dieses Modul behandelt wir einen speziellen Boosting-Algorithmus namens ... AdaBoost AdaBoost ist einer der frühen Machine-Learning-Algorithmen für Boosting. Er ist extrem nützlich. Er ist sehr einfach zu implementieren. Und er ist sehr effektiv. Es gibt andere. Ich werde einen anderen interessanten Algorithmus gegen Ende des Moduls erwähnen, aber ... ... lasst und mit AdaBoost beginnen. Dies ist der berühmte AdaBoost-Algorithmus, ... ... der von Freund und Schapire 1999 entwickelt wurde. Es ist ein erstaunlich nützlicher Algorithmus. Wir beginnen, indem wir jeden Datensatz als gleichwertig betrachten. Man weiß nicht, welche Datensätze für den Algorithmus schwierig und welche einfach sind. Sie haben alle die gleiche Gewichtung. Also können wir ihnen allen die gleiche Gewichtung zuweisen. Wir fangen an, indem wir ihnen das Gewicht 1 durch N geben, ... ... da dies alles ein wenig besser funktionieren lässt. Wir werden auf späteren Folien diskutieren, warum das so ist, aber ... ... wir fangen damit an, dass alle Datensätze die gleiche Gewichtung haben. Das wird einheitliche Gewichtung genannt. In diesem Fall ist α i 1 durch N. Für jede Iteration des AdaBoost-Algorithmus, also während wir den ersten ... Entscheidungsbaum-Stumpf oder den ersten simplen Klassifizierer oder den ersten gewichteten Klassifizierer, ... den zweiten oder den dritten oder irgendeinen bis zur T-ten Iteration lernen, werden wir die α i aktualisieren. Wir lernen f t anhand der mit α i gewichteten Datensätze. Das sind die Daten. Die Gewichtung ist am Anfang 1 durch N, aber ... sie verändert sich mit der Zeit. Dann berechnen wir den Koeffizienten w Dach t für ... diesen neuen Klassifizierer f t, den wir lernten. Dann berechnen wir die Gewichtungen α i neu. So fahren wir fort. Sind wir fertig, so sagen wir, dass die Vorhersage y Dach, ... ... das Signum der Kombination von f1, f2, f3, ... ... f4, gewichtet mit den zuvor gelernten Koeffizienten ist. Es gibt demnach 2 fundamentale Probleme, die zu addressieren sind, ... ... wenn wir über AdaBoost nachdenken. Eines ist, wie wir den Koeffizienten w Dach t berechnen. Lasst uns dies, in diesem Modul, das "Problem 1" nennen. Problem 1 ist die Frage, wie viel Vertrauen in einzelne f t haben. Vertrauen wir einem f t sehr, so sollte es eine sehr hohe Gewichtung haben. Falls wir f t nicht sehr vertrauen, ... sollten wir ihm eine geringe Gewichtung, oder einen niedrigen Koeffizienten zuweisen. "Problem 2" ist, wie wir die Gewichtung für die Datensätze neu berechnen. Lasst uns das "Problem 2" nennen. Problem 2 ist also die Frage wie ... ... gewichten wir Fehler vergleichsweise stärker? Wir wollen die Gewichtung von Fehlern verstärken. Im Hauptteil dieses Moduls werden wir darüber sprechen, wie ... wir w Dach t berechnen und wie wir α i aktualisieren und ... ... es wird ziemlich einfach werden, relativ intuitiv und sehr nützlich. [Musik]