1
00:00:00,000 --> 00:00:04,680
[MÚSICA]

2
00:00:04,680 --> 00:00:08,562
En éste módulo vamos a hablar
sobre el algoritmo de boosting llamado

3
00:00:08,562 --> 00:00:09,710
AdaBoost.

4
00:00:09,710 --> 00:00:13,340
AdaBoost es uno de los primeros algoritmos
de aprendizaje automático para boosting.

5
00:00:13,340 --> 00:00:14,490
Es extremadamente útil.

6
00:00:14,490 --> 00:00:16,710
Es muy fácil de implementar.

7
00:00:16,710 --> 00:00:18,400
Y es bastante eficaz.

8
00:00:18,400 --> 00:00:18,940
Hay otros.

9
00:00:18,940 --> 00:00:23,087
Voy a mencionar uno interesante
al final del módulo, pero

10
00:00:23,087 --> 00:00:24,825
empecemos con AdaBoost.

11
00:00:26,040 --> 00:00:29,332
Este es el famoso algoritmo AdaBoost,

12
00:00:29,332 --> 00:00:35,690
el cual fue creado por Freund y Schapire
en 1999, un algoritmo increíblemente útil.

13
00:00:35,690 --> 00:00:38,280
Se comienza viendo cada dato
de la misma manera.

14
00:00:38,280 --> 00:00:40,040
no se sabe cuáles son difíciles y cuáles fáciles.

15
00:00:40,040 --> 00:00:41,310
Todos tienen el mismo peso.

16
00:00:41,310 --> 00:00:46,500
Se puede empezar con todos los pesos igual a 1.
Nosotros empezaremos con 1/N,

17
00:00:46,500 --> 00:00:51,480
porque este valor hace el trabajo un poco mejor.

18
00:00:51,480 --> 00:00:54,995
Y vamos explicar el por qué
un poco mas adelante, pero

19
00:00:54,995 --> 00:00:58,745
iniciamos todos los datos
con peso uniforme.

20
00:00:58,745 --> 00:01:01,455
En este caso, alfa es 1/N

21
00:01:01,455 --> 00:01:06,595
Y luego para cada iteración de AdaBoost,
a medida que se aprende la primera

22
00:01:07,705 --> 00:01:10,555
decisión o el primer clasificador simple
o el primer clasificador de peso,

23
00:01:10,555 --> 00:01:12,595
el segundo, el tercero,
y así hasta T.

24
00:01:12,595 --> 00:01:17,665
Lo que hacemos es aprender ft con el
dato de peso alfa i

25
00:01:19,030 --> 00:01:22,775
Es decir, esos datos son 
los pesos que inician en 1/N pero

26
00:01:22,775 --> 00:01:24,771
cambian con el tiempo.

27
00:01:24,771 --> 00:01:31,132
Luego calculamos el coeficiente ŵt para

28
00:01:31,132 --> 00:01:37,720
este nuevo clasificador ft que aprendimos.

29
00:01:37,720 --> 00:01:40,770
Y luego recalculamos
los pesos alfa i.

30
00:01:42,070 --> 00:01:47,213
Continuamos, y una vez que terminamos
decimos que la predicción ŷ es 

31
00:01:47,213 --> 00:01:52,860
el signo de la combinación
ponderada de f1, f2, f3

32
00:01:52,860 --> 00:01:57,844
f4, ponderados por esos coeficientes
que aprendimos antes.

33
00:01:59,210 --> 00:02:02,730
Entonces, hay dos problemas fundamentales
que necesitamos abordar

34
00:02:02,730 --> 00:02:05,240
cuando estamos pensando en AdaBoost.

35
00:02:05,240 --> 00:02:10,616
El primero es, cómo calcular
el coeficiente ŵ.t,

36
00:02:10,616 --> 00:02:16,125
llamémosle problema 1 en nuestro módulo de hoy.

37
00:02:16,125 --> 00:02:22,330
el cual es ¿cuánto  confío en f.t en este caso?

38
00:02:23,840 --> 00:02:26,830
Si confío mucho en f.t, debería darle un peso muy alto.

39
00:02:26,830 --> 00:02:29,720
Si confío muy poco en f.t, 

40
00:02:29,720 --> 00:02:34,220
debería darle un coeficiente muy bajo.

41
00:02:34,220 --> 00:02:39,850
Y luego, el problema 2 es
¿cómo recalcular este peso en los datos?

42
00:02:39,850 --> 00:02:41,311
Llamémosle problema 2.

43
00:02:41,311 --> 00:02:46,891
Y entonces, el problema 2 es

44
00:02:46,891 --> 00:02:52,031
¿Cómo más pesamos los errores?

45
00:02:52,031 --> 00:02:54,291
Así que queremos incrementar
los pesos de los errores.

46
00:02:54,291 --> 00:02:58,669
En la parte principal de este módulo,
vamos a hablar sobre cómo

47
00:02:58,669 --> 00:03:02,398
calcular ŵ.t y cómo actualizar los alfa.i, y

48
00:03:02,398 --> 00:03:07,610
será bastante sencillo, relativamente intuitivo
y extremadamente útil.

49
00:03:07,610 --> 00:03:11,869
[MÚSICA]