[موسيقى] يستخدم AdaBoost ذلك، بصيغة بها قدر أقل من الثقة،
 لاكتشاف ما الذي يجب أن تكون عليه Ŵt ، ولكنها ستكن بديهية للغاية إذا تناولتها بقدر أكير من التفصيل. وتشتق هذه الصيغة من نظرية مشهورة، ألا وهي نظرية AdaBoost، والتي أود أن أتطرق إليها باختصار شديد مع انتهاء هذه الوحدة. ولكنها صيغة تسمح لك بالعثور على مصنفات تقوم بأداء أفضل وأفضل وتساعد على التعزيز للحصول على الحل الأمثل. ولذا، دعونا نتناول تلك بقدر أكبر بقليل من التفصيل، من خلال استعراض بعض الحالات الممكنة. ولذا، فإن السؤال هو، هل (Ft) مناسبة؟ في حال كانت Ft جيدة مناسبة للغاية. فسيكون بها خطأ أقل بالفعل فيما يخص بيانات التدريب،
 والذي يطلق عليه الخطأ المرجح. ولذا، وعلى سبيل المثال، إذا كان الخطأ المرجح
يساوي 0.01، فمن ثم فهو يعتبر مصنِف مناسب. والسؤال هو، ما الذي يحدث لهذا المصطلح المتوسط الشهير عندما يساوي الخطأ المرجح 0.01. ولهذا فإن المصطلح في المنتصف سيكون 1-0.01 ÷ 0.01، والذي يساوي 99. وبعد ذلك، ولتكملة Ŵt، سنقوم بأخذ ناتج ضرب 1/2 في اللوغاريتم للعدد 99، ولهذا، إن قمت بضرب 1/2 في اللوغاريتم للعدد 99، فستحصل على 2.3. ولهذا، فإن ذلك يعتبر مصنِفًا ممتازًا، وقد أعطيناه
ترجيحًا بنسبة 2.3، والذي يعتبر ترجيحًا مرتفعًا. والآن، دعونا نرى ماذا يحدث، إذا كان المُخرج لدينا مصنِف عشوائي، ولذا، فإننا كما قلنا إن المصنِف العشوائي،
به خطأ مُرجح يساوي 0.5 فهو بذلك شيء لا يمكن الوثوق به. ولذا، إذا قمت بتلك العملية الحسابية 1-0.5 ÷ 0.5 فستحصل على العدد السحري 1. وفي حال نظرت إلى 1/2 اللوغاريتم 1، فماذا سيكون اللوغاريتم 1؟ فسيكون 0، ولذا فإن Ŵt ستكون 0. ولذا، ما تعلمناه هو أن إذا كان المصنف عشوائي، فلن يقوم بأي شيء ذي معنى، وسنقوم بحسابه باعتباره صفرًا. ويكأن لسان حالنا يقول، أنت فظيع، سنقوم بتجاهلك، وقد يكون لديك زملاء يشبهون ذلك نوعًا ما، ممن يقولون أشياءً عشوائية، ولا تثق أبدًا بما يقولونه، وتضع ترجيح يساوي 0 أمام آرائِهم. ولذا، هذا ما سيقوم به AdaBoost أيضًا. والآن سيكون لدينا حالة مثيرة جدًا جدًا. دعونا نفترض أن التصنيف فظيع،
 وسيكون به خطأ مرجح يساوي 0.99. ولذا، فإن كل ما به تقريبًا خطأ، ولذا يعد أسوأ من العشوائي، ودعونا نرى ماذا يحدث للمصطلح
 في المنتصف هنا في المعادلة التي لدينا، أنت تحصل على 1-0.99 ÷ 0.99، ويساوي الناتج 0.01، وخمن ماذا قد يحدث عندما تأخذ 1/2 اللوغاريتم والذي يساوي 0.01؟ ستحصل على -2.3. وعندما رأيت ذلك أولاً، قلت يا للروعة،
 إن نظرية AdaBoost هذه رائعة، ولكني استغرقت لحظات، لفهم ماذا حدث للتو. لقد حصلنا على هذا المصنف الفظيع. ولكننا أعطيناه ترجيحًا مرتفعًا
للغاية وهو 2.3، ولكن مع علامة سالبة، فلماذا ذلك؟ لأن المصنِف الفظيع جدًا، قد يكون فظيعًا
ولكن إن أخذنا 1-Ft، فإننا سنقوم بالضبط بعكس ما يُقال،
ولذا سيكون مصنِفًا رائعًا، وبصيغة أخرى، إذا قمنا بعكس المصنف، فسيحدث ذلك بشكل رائع. وتقوم نظرية AdaBoost بذلك تلقائيًا لك، وهذا مجددًا، يعتبر نوعًا من
استخدام تناظر الصديق، فقد يكون لديك، والذي يكون صاحب آراء جيدة تمامًا، ولكنها تبدو جميعها خاطئة. ولذا، نقوم بالضبط بعكس ما يقوله الشخص. وربما يكون ذلك،
 مثل أن تسمع لوالديك أو أي شخص ما أو بعض الأصدقاء وتقول حسنًا، حسنًا،
 يجب أن أقوم بكذا، ولكنك في قرارة نفسك ستقوم بعكسه. ومن خلال القيام بذلك، قد أقوم بأشياء رائعة في العالم. ويقوم AdaBoost تلقائيًا باكتشاف ذلك لك، ويعتبر ذلك رائعًا. والآن، دعونا نعود مرة أخرى لخوارزمية AdaBoost
 والتي تحدثنا عنها قبل ذلك، ومن خلال هذا الجزء من هذه الوحدة، سنقوم بالتدقيق حول كيفية يتم حساب المعامل Ŵt، ولاحظنا أنه يمكن حساب ذلك من خلال هذه الصيغة البسيطة. وقمنا بحساب الخطأ المُقدر لـ Ft 
 وقلنا إن Ŵt تساوي 1/2 اللوغاريتم والذي به 1 - الخطأ المرجح ومقسومًا على الخطأ المرجح. ومن خلال ذلك، فسيكون لدينا Ŵt، وسيمكن التركيز على كيفية
التوصل إلى Alpha IS، ونحن نود أن تكون α I أن تكون مرتفعة،
 عندما تقوم Ft بأخطاء أو (كلام غير مسموع ) [موسيقى]