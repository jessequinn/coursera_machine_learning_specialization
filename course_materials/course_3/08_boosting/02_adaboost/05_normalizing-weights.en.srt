1
00:00:00,000 --> 00:00:04,712
[MUSIC]

2
00:00:04,712 --> 00:00:09,481
>> Finally, we need to address a technical
issue that we hinted at when we normalized

3
00:00:09,481 --> 00:00:14,480
the weights of data points start to 1
over n, when we had uniform weights.

4
00:00:14,480 --> 00:00:18,882
Which is they should be normalizing
weights of the data points throughout

5
00:00:18,882 --> 00:00:23,426
the iterations ,so for example if you
take data point xi, suppose that it is

6
00:00:23,426 --> 00:00:28,041
often a mistake where multiplying its
weight by a positive number again, and

7
00:00:28,041 --> 00:00:29,112
again and again.

8
00:00:29,112 --> 00:00:34,450
Let's say 2 times 2 times 2 times 2, and
that weight of a can get extremely large.

9
00:00:34,450 --> 00:00:38,290
On the other hand, if you take the data
point xi that's often correct,

10
00:00:38,290 --> 00:00:41,570
you multiply by some number less than one,
so say a half.

11
00:00:41,570 --> 00:00:44,578
So you keep going by a half, a half,
a half, and that weight can get really,

12
00:00:44,578 --> 00:00:45,870
really small.

13
00:00:45,870 --> 00:00:52,220
And so this problem can lead to
numerical instabilities in the approach.

14
00:00:52,220 --> 00:00:57,430
And so your computer might not behave very
well with all this crazy weighted data.

15
00:00:57,430 --> 00:01:02,240
And so what we do is after each iteration,
we go ahead and

16
00:01:02,240 --> 00:01:07,710
normalize the weights of all
data points to add up to one.

17
00:01:07,710 --> 00:01:13,370
So basically we divide each alpha i by
the sum of the data points of alpha i,

18
00:01:13,370 --> 00:01:17,690
so this approach keeps

19
00:01:18,880 --> 00:01:23,890
the weights in a reasonable range and
then avoids numerical instability.

20
00:01:23,890 --> 00:01:26,290
So let's summarize the AdaBoost algorithm.

21
00:01:26,290 --> 00:01:30,380
We start with, even weights,
uniform weights, equal weights for

22
00:01:30,380 --> 00:01:31,460
all data points.

23
00:01:31,460 --> 00:01:33,850
We learn a classify F of T.

24
00:01:33,850 --> 00:01:39,870
We find its coefficient depending on how
good it is in terms of weighted error.

25
00:01:39,870 --> 00:01:45,760
And then we update the weights to weigh
mistakes more than things we got correct.

26
00:01:45,760 --> 00:01:46,830
And finally,

27
00:01:46,830 --> 00:01:52,300
we normalize the weights by dividing each
value by this total sum of the weights.

28
00:01:52,300 --> 00:01:55,610
And this normalization is
of practical importance.

29
00:01:55,610 --> 00:02:00,456
So this is the whole AdaBoost algorithm,
it's beautiful, works extremely well,

30
00:02:00,456 --> 00:02:01,629
really easy to use.

31
00:02:01,629 --> 00:02:06,339
>> [MUSIC]