[موسيقى] في هذه الوحدة، سنقوم بالحديث عن
خوارزمية تعزيز محددة تسمى AdaBoost. AdaBoost هي واحدة من أوائل
خوارزميات التعلم الآلي لتعزيزه. وهي مفيدة للغاية. وبسيطة للغاية في تنفيذها. وكذلك فعالة جدًا. وهناك آخرون. سأذكر واحدة أخرى مثيرة للاهتمام
في نهاية الوحدة ولكن دعونا نبدأ مع AdaBoost. هذه هي خوارزمية AdaBoost الشهيرة، والتي أنشئت بواسطة فرويند وشابيري
في عام 1999، وهي خوارزمية مدهشة ومفيدة. لذا يمكنك البدء برؤية كل
نقطة بيانات بنفس الطريقة. إذًا أنت لا تعرف أيها الأصعب،
وأيها الأسهل. فجميعها لها نفس الوزن. لذا حتى تتمكن من البدء في العمل عليها،
سوف نبدأ معها بوزن 1 على N، لأن ذلك يجعل كل شيء
يعمل بشكلٍ أفضل قليلًا نوعًا ما. وسوف نتحدث عن السبب بعد بضع شرائح،
ولكن نبدأ مع كافة نقاط البيانات لها
نفس ما يسمى بالوزن الموحد. لذا في هذه الحالة α تساوي واحدًا على n. ثم لكل تكرار من AdaBoost،
بينما تتابع، ستكتشف أول ختم قرار أو أول مصنف بسيط
أو مصنف الوزن الأول، والمصنف الثاني، أو
الثالث حتى الوصول إلى T. ما نقوم به هو أننا نحدد ft
على البيانات المرجحة α i. إذًا هذه هي البيانات،
هي الأوزان التي تبدأ مع 1 على N ولكن تصبح مختلفة على مر الزمن. ثم نحسب المعامل w hat t لهذا التصنيف الجديد ft الذي حددناه. وبعد ذلك يمكننا إعادة حساب الأوزان α i. ثم نستمر في العمل، وأخيرً بمجرد الانتهاء
نقول إن التنبؤ y hat هو علامة الجمع المرجح لـ f1 وf2 وf3 وf4 مرجحة بهذه المعاملات
التي تعلمناها مما سبق. لذلك هناك اثنان من المسائل الأساسية
التي نكون بحاجة إلى معالجتها عندما نفكر في AdaBoost. واحدة هي كيف يمكنك حساب
المعامل w hat t، دعونا ندعو تلك المسألة 1
في وحدتنا اليوم. وبالتالي فإن المسألة 1 هي ما مدى
ثقتي في ftF في هذه الحالة؟ لذا إن وثقت في ft كثيرًا،
ينبغي أن أعطى وزنًا عاليًا جدًا. وإذا وثقت ft بشكل قليل جدًا، ينبغي إعطاء وزن منخفض جدًا،
أو معامل منخفض جدًا، كما يجب القول. ثم هناك المسألة 2 حول كيف يمكنك
إعادة حساب هذا الوزن في نقاط البيانات؟ لنطلق على هذا المسألة 2. وهكذا، تكون المسألة 2 هنا هي كيف نرجح الأخطاء بشكل أكبر؟ حسنًا نحن نرغب في زيادة
ترجيحات الأخطاء. وبالتالي في الجزء الرئيسي من هذه الوحدة،
سنتحدث بشأن كيفية حسابك لقيمة w hat t
وكيف يمكننا تحديث قيم α i وسيكون الأمر بسيطًا للغاية
وبديهي بشكل نسبي ومفيدًا جدًا. [موسيقى]