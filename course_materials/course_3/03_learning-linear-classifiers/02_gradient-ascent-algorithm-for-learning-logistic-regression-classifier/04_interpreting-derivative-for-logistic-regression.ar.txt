[موسيقى] حسنًا، فقد رأينا مثال رقمي حول كيفية
حساب تلك المشتقات. إنها عمليات حسابية بسيطة للغاية. والآن، دعونا نعد إلى تفسير ما
يقصد بالمشتقة. لنفترض أن لدينا معلمة محددة
نحاول تحديثها،وهي Wj ولدينا نقطة بيانات محددة وهي xi،
وبالنظر إلى قيمتها يتضح أن في xi فإن (hj(xi
تساوي القيمة 1. لنفترض أن هذا ما قد حدث، لذا، ما القيمة التي لدى نقطة البيانات هذه
على المستوى الكلي للمشتقة؟ فإذا أطلقنا على هذه القيمة المسمى Δi، واخترت أن تكون قيمة (hj(xi هي 1
 حتى لا نقلق بعد ذلك حيال قيمة السمة، ونتمكن من التركيز
على الاختلاف فقط. ودعونا نفسر هذا الاختلاف. لذا، دعونا ننظر أولَا إلى الحالة حيث
تكون قيمة yi موجبة، حيث أن هذا مثال تدريب إيجابي. ويؤكد النموذج الذي لدينا أن هذا
مثال تدريب إيجابي. لذا، يفترض أن الاحتمالية y = +1،
تساوي 1 تقريبًا. لنفسر ما يحدث هنا. في هذه الحالة فإن Δi ستكون مساوية للفرق بين 1، حيث أن دالة المؤشر مستبعدة هنا،
والاحتمالية هنا والتي تساوي 1 تقريبًا، وبالتالي فهذه
المعادلة ستساوي صفرًا تقريبًا. وبعبارة أخرى، فإن نقطة
البيانات موجبة، ويعمل النموذج من منطلق أن هذه
نقطة بيانات موجبة. لذا، فكون القيمة هنا هي صفر، فإن هذا
يعني لا تغير أي شيء. ووفقًا لمنظور هذه المعلمة، لا
يجب تغيير أي شيء. مما يجعل الأمر منطقيًا. لقد فهمت الأمر بشكل صحيح. دعونا نرى ما قد يحدث إذا فهمت
الأمر بشكل خاطئ تمامًا. وهذه هي الحالة الثانية، حيث يوجد
مثال تدريب إيجابي ولكن الاحتمالية y = +1 تساوي القيمة 0 تقريبًا. ونحن نفهم مثال التدريب هذا
بشكل خاطئ تمامًا. في هذه الحالة فإن Δi هي الفرق بين المؤشر والذي هو القيمة 1 والاحتمالية هنا تساوي القيمة 0، لذا فإن
المعادلة هنا تساوي 1 تقريبًا. لذا، فإن Δi كبيرة حقًا. وماذا يعني هذا؟ هذا يعني أن نقطة البيانات تلك
تتطلب أن نقوم بزيادة المعامل Wj، وهو المعامل
الذي حاولنا زيادته. ويشير ذلك إلى دفع المشتقة لأعلى، وإضافة القيمة
دلتا التي تجعل من المعادلة إيجابية. وتذكر أن إذا كانت قيمة (hj(x
تساوي رقمًا موجبًا وتم ضربها في معامل أكبر، لأننا
جعلنا قيمتها أكبر قليلًا، فهذا يعني أن مجموع hi يصبح أكبر، مما يعني أن الاحتمالية (y=+1|xi,w) قيمتها تزيد أيضًا. وهذا أمر بديهي للغاية. لذا، إذا فهمنا نقطة البيانات بشكل خاطئ،
فسنحصل على Δi الموجبة هذه مما يؤدي إلى زيادة المعلمة، مما
يؤدي إلى زيادة المجموع مما يؤدي في الخطوة التالية إلى جعل الاحتمالية الموجبة لنقطة البيانات
هذه ذات قيمة أعلى. لذا، سنذهب في الاتجاه الصحيح. والآن، دعونا نخوض في هذا الأمر بشكل أسرع قليلًا،
حيث يمكننا النظر في حالة حيث تكون قيمة yi سالبة، وسنحصل على نفس
القيمة ولكن بعلامة معكوسة. إذن، على سبيل المثال، إذا
كانت قيمة yi سالبة ولكني أفهم عملية التنبؤ بشكل صحيح،
حيث أن هذه نقطة بيانات سالبة. فسنرى أن Δi تساوي تقريبًا القيمة 0، مما يعني ألا
تغير أي شيء. فهذا منطقي، لقد حصلت على كل شيء بشكل صحيح،
فلما قد أغير أي شيء؟ ومع ذلك، في حالة كون نقطة
البيانات سالبة، ولكن عملية التنبؤ التي قمت بها كانت بناءً على
نقطة بيانات إيجابية، فإن Δi ستكون الفرق بين المؤشر والذي
في تلك الحالة يساوي 0، والاحتمالية والتي تساوي 1 تقريبًا. لذا فإن Δi هذه ستكون سالبة،
وتساوي تقريبًا القيمة -1. مما يؤدي إلى تناقص قيمة Wj مما يعني أن مجموع القيمة xi سيتناقص وسيؤدي إلى تناقص قيمة الاحتمالية (y=+1|xi,w) . رائع. إذا كانت نقطة بيانات سالبة وتم فهمها بشكل خاطئ،
فإننا سنقلل قيمة المعامل لجعل احتمالية كونه موجبًا أقل وزيادة احتمالية كونه سالبًا. رائع. فقد خضنا خلال القليل من التفسير حول
كيف يمكن لمعامل التدرج هذا أن يساعدنا في دفع المعاملات بقيم أكبر
بالنسبة لأمثلة التدريب الموجبة. ودفعها بقيم أٌصغر في حالات
أمثلة التدريب السالبة، وهو بالضبط ما كنا نريده بالنسبة
لدالة المجموع تلك.