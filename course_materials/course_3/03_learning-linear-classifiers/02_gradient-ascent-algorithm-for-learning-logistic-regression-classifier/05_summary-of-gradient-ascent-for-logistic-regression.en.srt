1
00:00:00,000 --> 00:00:04,720
[MUSIC]

2
00:00:04,720 --> 00:00:09,100
And now, we have the complete gradient
ascent algorithm for logistic regression.

3
00:00:09,100 --> 00:00:14,795
Extremely simple,
extremely useful and very cool.

4
00:00:14,795 --> 00:00:18,770
So we're going to start
from some point say, w0 and

5
00:00:18,770 --> 00:00:24,829
we're just going to follow the gradient
here until we get to the optimal here,

6
00:00:24,829 --> 00:00:29,847
which we're gonmna and
we're going to stop when the norm

7
00:00:29,847 --> 00:00:36,100
of the gradient is sufficiently small
with respect to tolerance parameter.

8
00:00:36,100 --> 00:00:40,080
Epsilon, like we discussed
in the regression case.

9
00:00:40,080 --> 00:00:45,600
And every time we take an iteration,
we go feature by feature or

10
00:00:45,600 --> 00:00:49,570
coefficient by coefficient
compute that partial derivative,

11
00:00:49,570 --> 00:00:51,560
which is back to coefficient j.

12
00:00:51,560 --> 00:00:55,910
So what is the derivative
with respect coefficient and

13
00:00:55,910 --> 00:01:01,147
then we just update
the coefficient by wj(t+1)

14
00:01:01,147 --> 00:01:06,810
by wjt plus the stub size times this
derivative that we just computed.

15
00:01:08,970 --> 00:01:13,645
So this is notation for
the derivative l at t,

16
00:01:13,645 --> 00:01:17,218
which respects the parameter wj.

17
00:01:17,218 --> 00:01:21,922
And at the core of the derivative,
the only little computation that we

18
00:01:21,922 --> 00:01:26,466
have to do Is this one over here,
which is computing the probability

19
00:01:26,466 --> 00:01:30,638
that the data point has value
plus one with the parameters wt.

20
00:01:30,638 --> 00:01:32,590
So, what's that equal to?

21
00:01:32,590 --> 00:01:35,381
Well, this is just exactly
the logistic regression model.

22
00:01:35,381 --> 00:01:38,516
So, I'm going to do a change of
colors transformation here and

23
00:01:38,516 --> 00:01:41,500
just do this in blue so it will stand out.

24
00:01:41,500 --> 00:01:47,781
This last little bit is just
one over 1+e to the -w,

25
00:01:47,781 --> 00:01:52,400
which is whatever it is at iteration t.

26
00:01:53,800 --> 00:01:58,965
Transpose a h(of xi).

27
00:02:01,115 --> 00:02:04,803
So I just compute that prediction,
whatever my model thinks.

28
00:02:04,803 --> 00:02:08,518
I subtract to the indicators
this a positive example,

29
00:02:08,518 --> 00:02:11,922
multiplied by the feature value and
there you go.

30
00:02:11,922 --> 00:02:13,435
Some more data points.

31
00:02:13,435 --> 00:02:18,256
Simple algorithm, really useful,
really, really useful.

32
00:02:18,256 --> 00:02:22,129
[MUSIC]