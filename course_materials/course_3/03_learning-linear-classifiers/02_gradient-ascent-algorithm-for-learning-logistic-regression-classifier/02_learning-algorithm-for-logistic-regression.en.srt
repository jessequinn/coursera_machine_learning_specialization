1
00:00:00,058 --> 00:00:04,547
[MUSIC]

2
00:00:04,547 --> 00:00:05,420
Okay, this is exciting.

3
00:00:05,420 --> 00:00:09,840
We reminded ourselves of gradient
descent and what that looks like.

4
00:00:09,840 --> 00:00:13,050
We figured out what
the discretion model is.

5
00:00:13,050 --> 00:00:16,870
We talked about maximum likelihood
functions and estimation.

6
00:00:16,870 --> 00:00:19,260
And we can solve quite a bit of the math.

7
00:00:19,260 --> 00:00:23,380
Now, lets dive in and
look at the actual learning algorithm for

8
00:00:23,380 --> 00:00:24,240
logistic regression.

9
00:00:24,240 --> 00:00:26,110
That learning algorithm's
going to be extremely simple.

10
00:00:27,130 --> 00:00:30,888
And so, in spite of the math we've seen,
we are going to come up algorithm that you

11
00:00:30,888 --> 00:00:32,968
can implement in just
a couple lines of code.

12
00:00:34,512 --> 00:00:38,404
To, this final graded ascent algorithm for
logistic regression,

13
00:00:38,404 --> 00:00:42,924
all we have to do is fine the derivative,
with respect to each parameter, wj.

14
00:00:42,924 --> 00:00:45,690
So what's the derivative for
likelihood function with respect to wj?

15
00:00:45,690 --> 00:00:48,160
By the way, this derivatives side note,

16
00:00:48,160 --> 00:00:51,670
going to be of the log of the likelihood
function, the log likelihood.

17
00:00:51,670 --> 00:00:53,580
And I'll explain a little
bit more why that is,

18
00:00:53,580 --> 00:00:58,490
in an advanced section of this
module later on, an optional one.

19
00:00:58,490 --> 00:01:00,830
But just think about it as a derivative
of the likelihood for now.

20
00:01:02,320 --> 00:01:06,310
Now, the derivative of the likelihood
is going to be equal to the sum

21
00:01:06,310 --> 00:01:08,930
over the data points, so we're going
to consider that each data point has

22
00:01:08,930 --> 00:01:12,620
a contribution to the derivative,
some only want to make the derivative big,

23
00:01:12,620 --> 00:01:16,610
some like to make the derivative smaller,
but we're going to sum over the data

24
00:01:16,610 --> 00:01:24,350
points of the difference between what's
called the indicator function that

25
00:01:24,350 --> 00:01:29,410
a data point is plus 1, so indicator
of whether this data point is positive.

26
00:01:29,410 --> 00:01:32,510
So I'm going to specify
it at the bottom here.

27
00:01:34,090 --> 00:01:35,290
So let's get our ink.

28
00:01:36,550 --> 00:01:43,410
So this indicated function
simply say output 1 if

29
00:01:43,410 --> 00:01:48,020
yi is truly a positive example.

30
00:01:48,020 --> 00:01:55,160
And output 0 if yi is a negative example.

31
00:01:55,160 --> 00:01:57,190
So that's the indicator is.

32
00:01:57,190 --> 00:02:02,050
So this is this definition over here.

33
00:02:02,050 --> 00:02:04,180
So is this a positive example?

34
00:02:04,180 --> 00:02:09,253
And the second term, so
there's a difference between that and

35
00:02:09,253 --> 00:02:11,646
whatever my model predicts,

36
00:02:11,646 --> 00:02:16,639
that how much my model thus far predict,
that xi is positive.

37
00:02:20,508 --> 00:02:25,181
In other words, if this is the difference
between the truth, is this a positive

38
00:02:25,181 --> 00:02:30,700
example and the likelihood that my model
science to being a positive example.

39
00:02:30,700 --> 00:02:34,490
And we're going to weigh
it by the feature value xj.

40
00:02:34,490 --> 00:02:36,960
So this, for example,
is the number of awesomes.

41
00:02:36,960 --> 00:02:40,260
So if later point has many awesomes,

42
00:02:40,260 --> 00:02:43,430
then it has more contributions of the the
derivative because that coefficient's

43
00:02:43,430 --> 00:02:45,310
going to be multiplied by say 20 awesomes.

44
00:02:45,310 --> 00:02:50,500
If you have zero awesomes, then there's
no contribution here because weather

45
00:02:50,500 --> 00:02:54,325
the coefficient is high or low doesn't
make any difference is the data point.

46
00:02:54,325 --> 00:02:55,685
So that's why we weigh that way.

47
00:02:56,805 --> 00:03:01,007
So this is exactly the derivative that's
very simple that we're going to implement.

48
00:03:01,007 --> 00:03:05,829
[MUSIC]