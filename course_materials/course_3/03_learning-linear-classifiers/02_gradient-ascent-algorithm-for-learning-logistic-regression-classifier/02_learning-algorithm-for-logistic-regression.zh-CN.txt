[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community 好的，多么令人兴奋 让我们回忆一下梯度下降，它讲了些什么 离散模型是 最大化似然函数和估计值 我们可以解决一些数学问题 现在，让我们深入学习并看一看 逻辑回归的实际学习算法 这个学习算法相当的简单 因此，尽管我们已经了解了一些数学表达式，现在我们将 通过编写几行代码来实现该算法 逻辑回归的最终梯度下降算法 我们需要做的就是对每一个参数wj求偏导 那似然函数对参数wj求偏导是什么样的 另外，求该导数 需要对似然函数求对数，即对数似然 我讲解释一下为什么是这样 在稍后的高级课程部分，这是选修的 对似然函数求导 似然函数的导数等于 数据点的和，我们将考虑每个数据点 是否对导数有影响，一些会使导数变大 一些会使导数变小，但我们要对他们求和 指示函数表示后的结果是不同的 数据点加1，表明该点是正的 所以我要在底部指定它 让我们拿笔标记一下 这个函数表明输出为1 如果yi为正样本 如果yi为负样本输出为0 这就是指示函数的意思 这就是此处的定义 这是正样本吗？ 第二部分，指示函数值和 无模型的预测结果不同 模型预测xi为正的概率是多少 换句话说，这与真实值不同 这是一个正样本，模型的似然也将是一个正例 对特征值xj加权重 例如，这是awesomes的个数 所以如果之后的点有很多awesomes 将会对导数值产生较大影响 因为系数将扩大20倍 如果我们有0个awesomes，将不会有任何影响 因为无论系数大还是小都不会有什么区别 这就是我们这样加权的原因 这就是我们将要实现的求导 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community