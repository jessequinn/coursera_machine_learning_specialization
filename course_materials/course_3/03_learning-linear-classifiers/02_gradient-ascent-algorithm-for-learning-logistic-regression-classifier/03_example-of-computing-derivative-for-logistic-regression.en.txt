[MUSIC] So, let's see a little example we're
going to go through by hand and show what the derivative would be for
a particular parameter. We have a model and it says that so
far at iteration t w0 is equal to 0, w1 is equal to 1,
and w2 equal to minus 2. This is our current estimate wt. And the question is,
how we're going to modify it? How are we going to
compute the derivative? And we're going to compute the partial
derivative of the likelihood function with respect to the parameter w1. And the parameter w1 is going
to multiply the first feature, which is h1 of x which,
in our case, is number of awesomes. So, what is the contribution that each
one of these data points will have to the derivative with the equation above. That's what we're trying to do. So if you look at the question above, for each one of these we're going to make a
prediction according to the current model as to whether that data point
is actually a plus one. So for the first one, which has two
awesomes and one awful we get to the tree label plus one, but our model
only assigns 0.5 probability to it. So not perfect. What contribution does that
have to the derivative? Well, let's just go through
the equation above. So, h1 of xi is the number
on the left here, is two. And it multiplies the difference
between the indicator function. Is this a positive example? Yes, it is a positive example. And, the probability that we
get from data which is 0.5. So the contribution here is 2 times
(1-0.5) which is exactly equal to 1. Let's look at the next data point. The next data point the h1
of x has value zero. So in this case w1 is not important to it,
but now just let's go through. This is multiplying the indicator,
is this a positive example? In this case it's not, so it's zero minus
the probability that the model signs for this to be a positive label
which in this case is 0.02, and the total here contribution
is going to be zero. It's interesting for this particular data
point, the model gets it extremely right, it says the probability's positive
is very, very, very small, 0.02, which means it's very likely to be
a negative example which in fact it is, it's two awfuls, zero awesomes. Now, let's look at the third example. It's a bit of an unusual example. It has three awesomes and three awfuls. And it is actually a negative example, and the model thinks it is pretty
likely to be a negative example, so it's 0.05 probability of being positive,
so 95% chances of being negative. So once the contribution says to the
derivative, okay so let's look at h1 of x, in this case is three. It multiplies the difference
between the indicator that there is a positive example. So in this case it's not so it's zero. Minus the probability assigned to
the positive example which is 0.05. If you multiply this together,
you get minus 0.15, so that's the contribution,
contribution is negative to the gradient. Now for the final example,
we have four awesomes, one awful, so four is h1(x), is a positive example so
the indicator has value one. And the probability the model
thinks it's positive is 0.88. So the contribution here is four. That multiplies 1- 0.88,
which is 0.12 times 4 is 0.48. Great, and so the total derivative is just going to be the sum of each one
of these contributions added up together. So, the derivative of l computed at wt, with respect to the parameter w1. It's just going to be the sum
of these four contributions. So 1+0-0.15+0.48 which is equal to if you add it all together 1.33. And now I'm going to do a little
change of colors transformation here, let's go to a color. Let's write the update rule for
the parameter w1. We computed this derivative,
let's do the update. So w1 at the next iteration,
iteration t+1, is going to be equal to w1 at iteration t. Plus eta, the step size that multiplies the derivative of the likelihood function, at wt, with respect to the parameter w1. Which is the one that we're updating here. Now let's suppose we set eta to be 0.1. It's a bit of a large number. But it makes our updates
here a little simple. So what does this update rule says? w1 at iteration t + 1 is w1 iteration t, which was 1 + eta 0.1. That multiplies the derivative
we just computed right here, 1.33, which means that the new parameter
is going to be equal to 1.133. We just made our parameter a little
bit bigger, which is great. [MUSIC]