1
00:00:00,000 --> 00:00:04,064
[MUSIC]

2
00:00:04,064 --> 00:00:08,480
All right, we saw a numeric example of
how those derivatives get computed.

3
00:00:08,480 --> 00:00:11,630
It's really simple computations.

4
00:00:11,630 --> 00:00:15,030
Now let's go back and interpret
a little bit what the derivative means.

5
00:00:15,030 --> 00:00:20,920
So let's say we have a particular
parameter that we're trying to update,

6
00:00:20,920 --> 00:00:25,830
w j, and we have a particular data point
x i, want to look at its contribution and

7
00:00:25,830 --> 00:00:30,400
it turns out that for x i,
h j(x i) has value one.

8
00:00:30,400 --> 00:00:32,750
Say that that's what happened, so

9
00:00:32,750 --> 00:00:37,980
what is the contribution that this data
point has on the total derivative?

10
00:00:37,980 --> 00:00:42,890
So if you call this
contribution here delta i, and

11
00:00:42,890 --> 00:00:47,450
I chose hj of xi t1 because then we don't

12
00:00:47,450 --> 00:00:51,150
have to worry too much about future value,
we can just focus on that difference.

13
00:00:51,150 --> 00:00:53,180
Let's interpret that difference.

14
00:00:53,180 --> 00:00:57,810
So, let's first look at
the case where yi is positive,

15
00:00:58,830 --> 00:01:01,850
it's a positive training example.

16
00:01:01,850 --> 00:01:07,490
And, our model, it's pretty sure that
it's a positive training example.

17
00:01:07,490 --> 00:01:13,030
So, it says the probability that y equals
+1, it's approximately equal to one.

18
00:01:13,030 --> 00:01:15,060
So let's interpret what happens.

19
00:01:15,060 --> 00:01:19,850
So, in this case delta i is going

20
00:01:19,850 --> 00:01:25,210
to be equal to the difference between one,

21
00:01:25,210 --> 00:01:29,430
the indicator function is paused there and
the probability here,

22
00:01:29,430 --> 00:01:35,230
which is approximately one, so this whole
thing is going to be approximately zero.

23
00:01:35,230 --> 00:01:39,960
In other words,
the data points are positive,

24
00:01:39,960 --> 00:01:43,070
the model thinks it's
a positive data point.

25
00:01:43,070 --> 00:01:50,271
So that the contribution here is
zero means don't change anything.

26
00:01:50,271 --> 00:01:54,510
From the perspective of this parameter,
you shouldn't change anything.

27
00:01:56,260 --> 00:01:57,900
Which makes sense.

28
00:01:57,900 --> 00:01:59,070
You get it right.

29
00:01:59,070 --> 00:02:02,450
Let's see what happens if
you get it completely wrong.

30
00:02:02,450 --> 00:02:07,540
So this is the second case where
it's a positive training example but

31
00:02:07,540 --> 00:02:10,800
the probability y = +1 is approximately 0.

32
00:02:10,800 --> 00:02:14,300
We are getting this training
example totally wrong.

33
00:02:14,300 --> 00:02:18,846
In this case delta i is
the difference between

34
00:02:18,846 --> 00:02:23,040
the indicator, which has a value 1 and

35
00:02:23,040 --> 00:02:28,590
the probability here has a value 0 and
so it's approximately 1.

36
00:02:28,590 --> 00:02:30,420
So delta i is really big.

37
00:02:30,420 --> 00:02:33,990
And so what is this imply?

38
00:02:33,990 --> 00:02:38,560
This implies that this data
point wants us to increase

39
00:02:40,930 --> 00:02:45,500
the coefficient, w j,
the one that we just tried to increase.

40
00:02:45,500 --> 00:02:52,990
It says push the derivative up, add this
little delta that makes it more positive.

41
00:02:52,990 --> 00:02:58,270
And remember that if xhj
is a positive number and is

42
00:02:58,270 --> 00:03:02,080
getting multiplied by a bigger coefficient
because we just made it a little bigger,

43
00:03:02,080 --> 00:03:07,676
that implies that the score of hi

44
00:03:07,676 --> 00:03:14,490
becomes larger and
that implies in a sense the probability

45
00:03:14,490 --> 00:03:20,630
that y is equal to +1 given xi and
w increases.

46
00:03:20,630 --> 00:03:22,070
And that is extremely intuitive.

47
00:03:23,110 --> 00:03:26,910
So, if we're getting the data point really
wrong, we get this positive delta i,

48
00:03:26,910 --> 00:03:29,430
which is going to increase my parameter,
which is is going to increase my score,

49
00:03:29,430 --> 00:03:32,060
which makes in the next step.

50
00:03:32,060 --> 00:03:36,090
In the next iteration, the probability
this data point's positive higher.

51
00:03:37,140 --> 00:03:39,130
So we're going the right direction.

52
00:03:39,130 --> 00:03:44,060
Now, let's go through this a little
quicker, we can look at the case where

53
00:03:44,060 --> 00:03:47,390
yi is negative and you just get
the same thing with flipped sign.

54
00:03:47,390 --> 00:03:50,450
So, for example, If yi is negative but

55
00:03:50,450 --> 00:03:55,060
I'm getting the prediction right,
so this is a negative data point.

56
00:03:55,060 --> 00:03:58,757
We'll see that delta i
is approximately equal

57
00:03:58,757 --> 00:04:02,410
to 0 which implies don't change anything.

58
00:04:02,410 --> 00:04:06,920
It makes sense, I've got everything right,
why would I change anything?

59
00:04:06,920 --> 00:04:12,430
However, in the case where
the data points were negative, but

60
00:04:12,430 --> 00:04:17,840
the prediction I've made was that there
were positive data point, then delta i

61
00:04:17,840 --> 00:04:22,760
would be the difference between
the indicator which in this case is 0,

62
00:04:22,760 --> 00:04:25,960
and the probability,
which is approximately 1.

63
00:04:25,960 --> 00:04:30,500
So this delta i would be negative,
would be approximately -1.

64
00:04:30,500 --> 00:04:36,752
Which would lead wj to decrease

65
00:04:36,752 --> 00:04:41,320
which would imply that

66
00:04:41,320 --> 00:04:47,090
the score of xi decreases and

67
00:04:47,090 --> 00:04:52,620
it leads to the probability

68
00:04:52,620 --> 00:04:57,187
of y=+1 given x i and

69
00:04:57,187 --> 00:05:00,814
w, to decrease.

70
00:05:00,814 --> 00:05:01,930
Yay.

71
00:05:01,930 --> 00:05:07,260
It fits the negative datapoint and we get
it wrong, we decrease the coefficient

72
00:05:07,260 --> 00:05:10,870
to make that probability of
it being positive smaller and

73
00:05:10,870 --> 00:05:12,280
increase the probability
it will be negative.

74
00:05:13,710 --> 00:05:14,240
So cool.

75
00:05:14,240 --> 00:05:19,340
We've gone through a little bit of
interpretation of how this gradient

76
00:05:19,340 --> 00:05:24,930
helps us push the coefficients bigger for
positive training example.

77
00:05:24,930 --> 00:05:29,001
And more negative, smaller for
negative training examples,

78
00:05:29,001 --> 00:05:32,698
which is exactly what we wanted for
that score function.