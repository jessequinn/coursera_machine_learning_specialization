[MUSIC] Okay, this is exciting. We reminded ourselves of gradient
descent and what that looks like. We figured out what
the discretion model is. We talked about maximum likelihood
functions and estimation. And we can solve quite a bit of the math. Now, lets dive in and
look at the actual learning algorithm for logistic regression. That learning algorithm's
going to be extremely simple. And so, in spite of the math we've seen,
we are going to come up algorithm that you can implement in just
a couple lines of code. To, this final graded ascent algorithm for
logistic regression, all we have to do is fine the derivative,
with respect to each parameter, wj. So what's the derivative for
likelihood function with respect to wj? By the way, this derivatives side note, going to be of the log of the likelihood
function, the log likelihood. And I'll explain a little
bit more why that is, in an advanced section of this
module later on, an optional one. But just think about it as a derivative
of the likelihood for now. Now, the derivative of the likelihood
is going to be equal to the sum over the data points, so we're going
to consider that each data point has a contribution to the derivative,
some only want to make the derivative big, some like to make the derivative smaller,
but we're going to sum over the data points of the difference between what's
called the indicator function that a data point is plus 1, so indicator
of whether this data point is positive. So I'm going to specify
it at the bottom here. So let's get our ink. So this indicated function
simply say output 1 if yi is truly a positive example. And output 0 if yi is a negative example. So that's the indicator is. So this is this definition over here. So is this a positive example? And the second term, so
there's a difference between that and whatever my model predicts, that how much my model thus far predict,
that xi is positive. In other words, if this is the difference
between the truth, is this a positive example and the likelihood that my model
science to being a positive example. And we're going to weigh
it by the feature value xj. So this, for example,
is the number of awesomes. So if later point has many awesomes, then it has more contributions of the the
derivative because that coefficient's going to be multiplied by say 20 awesomes. If you have zero awesomes, then there's
no contribution here because weather the coefficient is high or low doesn't
make any difference is the data point. So that's why we weigh that way. So this is exactly the derivative that's
very simple that we're going to implement. [MUSIC]