<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Predicting sentiment from product reviews
 </h1>
 <p>
  The goal of this assignment is to explore logistic regression and feature engineering with existing GraphLab Create functions.
 </p>
 <p>
  In this assignment, you will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative. You will:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Use SFrames to do some feature engineering
   </p>
  </li>
  <li>
   <p>
    Train a logistic regression model to predict the sentiment of product reviews.
   </p>
  </li>
  <li>
   <p>
    Inspect the weights (coefficients) of a trained logistic regression model.
   </p>
  </li>
  <li>
   <p>
    Make a prediction (both class and probability) of sentiment for a new product review.
   </p>
  </li>
  <li>
   <p>
    Given the logistic regression weights, predictors and ground truth labels, write a function to compute the
    <strong>
     accuracy
    </strong>
    of the model.
   </p>
  </li>
  <li>
   <p>
    Inspect the coefficients of the logistic regression model and interpret their meanings.
   </p>
  </li>
  <li>
   <p>
    Compare multiple logistic regression models.
   </p>
  </li>
 </ul>
 <h2 level="2">
  If you are doing the assignment with IPython Notebook
 </h2>
 <p>
  An IPython Notebook has been provided below to you for this assignment. This notebook contains the instructions, quiz questions and partially-completed code for you to use as well as some cells to test your code.
 </p>
 <p>
  <strong>
   Make sure that you are using the latest version GraphLab Create.
  </strong>
 </p>
 <h2 level="2">
  What you need to download
 </h2>
 <h3 level="3">
  If you are using GraphLab Create:
 </h3>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the Amazon product review data In SFrame format:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="vS08FZb5EeaGYhKBa-7RCg" name="amazon_baby.gl">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the companion IPython Notebook:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="vS_Ts5b5EealEQopi6zfyA" name="module-2-linear-classifier-assignment-blank.ipynb">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Save both of these files in the same directory (where you are calling IPython notebook from) and unzip the data file.
   </p>
  </li>
  <li>
   <p>
    Follow the instructions contained in the IPython notebook.
   </p>
  </li>
 </ul>
 <h3 level="3">
  If you are not using GraphLab Create
 </h3>
 <ul bullettype="bullets">
  <li>
   <p>
    If you are using SFrame, download the Amazon product review data in SFrame format:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="vS08FZb5EeaGYhKBa-7RCg" name="amazon_baby.gl">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    If you are using a different package, download the Amazon product review data in CSV format:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="vQgRKJcCEeaK1Q4gRyvE8A" name="amazon_baby.csv">
 </asset>
 <h2 level="2">
  If you are using GraphLab Create and the companion IPython Notebook
 </h2>
 <p>
  Open the companion IPython notebook and follow the instructions in the notebook.
 </p>
 <h2 level="2">
  If you are using other tools
 </h2>
 <p>
  <strong>
   This section is designed for people using tools other than GraphLab Create. Even though some instructions are specific to scikit-learn, most part of the assignment should be applicable to other tools as well. However, we highly suggest you use
   <a href="https://github.com/turi-code/SFrame">
    SFrame
   </a>
   since it is open source. In this part of the assignment, we describe general instructions, however we will tailor the instructions for SFrame and scikit-learn.
  </strong>
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    If you choose to use SFrame and scikit-learn, you should be able to follow the instructions here and complete the assessment.
    <strong>
     All code samples given here will be applicable to SFrame and scikit-learn
    </strong>
    .
   </p>
  </li>
  <li>
   <p>
    You are free to experiment with any tool of your choice, but
    <strong>
     some many not produce correct numbers for the quiz questions.
    </strong>
   </p>
  </li>
 </ul>
 <h3 level="3">
  Load Amazon dataset
 </h3>
 <p>
  <strong>
   1.
  </strong>
  Load the dataset consisting of baby product reviews on Amazon.com. Store the data in a data frame
  <strong>
   products
  </strong>
  . In SFrame, you would run
 </p>
 <pre language="python">import sframe
products = sframe.SFrame('amazon_baby.gl/')</pre>
 <p>
  <strong>
   Note:
  </strong>
  To install SFrame (without installing GraphLab Create), run
 </p>
 <pre language="sh">pip install sframe</pre>
 <h3 level="3">
  Perform text cleaning
 </h3>
 <p>
  <strong>
   2.
  </strong>
  We start by removing punctuation, so that words "cake." and "cake!" are counted as the same word.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Write a function
    <strong>
     remove_punctuation
    </strong>
    that strips punctuation from a line of text
   </p>
  </li>
  <li>
   <p>
    Apply this function to every element in the
    <strong>
     review
    </strong>
    column of
    <strong>
     products
    </strong>
    , and save the result to a new column
    <strong>
     review_clean
    </strong>
    .
   </p>
  </li>
 </ul>
 <p>
  Refer to your tool's manual for string processing capabilities. Python lets us express the operation in a succinct way, as follows:
 </p>
 <pre language="python">def remove_punctuation(text):
    import string
    return text.translate(None, string.punctuation) 

products['review_clean'] = products['review'].apply(remove_punctuation)</pre>
 <p>
  <strong>
   Aside
  </strong>
  . In this notebook, we remove all punctuation for the sake of simplicity. A smarter approach to punctuation would preserve phrases such as "I'd", "would've", "hadn't" and so forth. See
  <a href="https://www.cis.upenn.edu/~treebank/tokenization.html">
   this page
  </a>
  for an example of smart handling of punctuation.
 </p>
 <p>
  <strong>
   IMPORTANT
  </strong>
  . Make sure to fill n/a values in the
  <strong>
   review
  </strong>
  column with empty strings (if applicable). The n/a values indicate empty reviews. For instance, Pandas's the fillna() method lets you replace all N/A's in the
  <strong>
   review
  </strong>
  columns as follows:
 </p>
 <pre language="python">products = products.fillna({'review':''})  # fill in N/A's in the review column</pre>
 <h3 level="3">
  Extract Sentiments
 </h3>
 <p>
  <strong>
   3.
  </strong>
  We will
  <strong>
   ignore
  </strong>
  all reviews with
  <em>
   rating = 3
  </em>
  , since they tend to have a neutral sentiment. In SFrame, for instance,
 </p>
 <pre language="python">products = products[products['rating'] != 3]</pre>
 <p>
  <strong>
   4.
  </strong>
  Now, we will assign reviews with a rating of 4 or higher to be
  <em>
   positive
  </em>
  reviews, while the ones with rating of 2 or lower are
  <em>
   negative
  </em>
  . For the sentiment column, we use +1 for the positive class label and -1 for the negative class label. A good way is to create an anonymous function that converts a rating into a class label and then apply that function to every element in the
  <strong>
   rating
  </strong>
  column. In SFrame, you would use apply():
 </p>
 <pre language="python">products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating &gt; 3 else -1)</pre>
 <p>
  Now, we can see that the dataset contains an extra column called
  <strong>
   sentiment
  </strong>
  which is either positive (+1) or negative (-1).
 </p>
 <h3 level="3">
  Split into training and test sets
 </h3>
 <p>
  <strong>
   5.
  </strong>
  Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. If you are using SFrame, make sure to use seed=1 so that you get the same result as everyone else does. (This way, you will get the right numbers for the quiz.)
 </p>
 <pre language="python">train_data, test_data = products.random_split(.8, seed=1)</pre>
 <p>
  If you are not using SFrame, download the list of indices for the training and test sets:
 </p>
 <asset assettype="generic" extension="zip" id="ef4z1JcCEeadUQ4CCWyuXA" name="module-2-assignment-train-idx.json">
 </asset>
 <asset assettype="generic" extension="zip" id="ed_96ZcCEea_cQqzDLeQwg" name="module-2-assignment-test-idx.json">
 </asset>
 <p>
  . IMPORTANT: If you are using a programming language with 1-based indexing (e.g. R, Matlab), make sure to increment all indices by 1.
 </p>
 <p>
  Call the training and test sets
  <strong>
   train_data
  </strong>
  and
  <strong>
   test_data
  </strong>
  , respectively.
 </p>
 <h3 level="3">
  Build the word count vector for each review
 </h3>
 <p>
  <strong>
   6.
  </strong>
  We will now compute the word count for each word that appears in the reviews. A vector consisting of word counts is often referred to as
  <strong>
   bag-of-word features
  </strong>
  . Since most words occur in only a few reviews, word count vectors are sparse. For this reason, scikit-learn and many other tools use sparse matrices to store a collection of word count vectors. Refer to appropriate manuals to produce sparse word count vectors. General steps for extracting word count vectors are as follows:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Learn a vocabulary (set of all words) from the training data. Only the words that show up in the training data will be considered for feature extraction.
   </p>
  </li>
  <li>
   <p>
    Compute the occurrences of the words in each review and collect them into a row vector.
   </p>
  </li>
  <li>
   <p>
    Build a sparse matrix where each row is the word count vector for the corresponding review. Call this matrix
    <strong>
     train_matrix
    </strong>
    .
   </p>
  </li>
  <li>
   <p>
    Using the same mapping between words and columns, convert the test data into a sparse matrix
    <strong>
     test_matrix
    </strong>
    .
   </p>
  </li>
 </ul>
 <p>
  The following cell uses CountVectorizer in scikit-learn. Notice the
  <strong>
   token_pattern
  </strong>
  argument in the constructor.
 </p>
 <pre language="python">from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(token_pattern=r'\b\w+\b')
     # Use this token pattern to keep single-letter words
# First, learn vocabulary from the training data and assign columns to words
# Then convert the training data into a sparse matrix
train_matrix = vectorizer.fit_transform(train_data['review_clean'])
# Second, convert the test data into a sparse matrix, using the same word-column mapping
test_matrix = vectorizer.transform(test_data['review_clean'])</pre>
 <p>
  Keep in mind that the test data must be transformed in the same way as the training data.
 </p>
 <h3 level="3">
  Train a sentiment classifier with logistic regression
 </h3>
 <p>
  We will now use logistic regression to create a sentiment classifier on the training data.
 </p>
 <p>
  <strong>
   7.
  </strong>
  Learn a logistic regression classifier using the training data. If you are using scikit-learn, you should create an instance of the
  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">
   LogisticRegression class
  </a>
  and then call the method fit() to train the classifier. This model should use the sparse word count matrix (
  <strong>
   train_matrix
  </strong>
  )
  <strong>
  </strong>
  as features and the column
  <strong>
   sentiment
  </strong>
  of
  <strong>
   train_data
  </strong>
  as the target. Use the default values for other parameters. Call this model
  <strong>
   sentiment_model
  </strong>
  .
 </p>
 <p>
  <strong>
   8.
  </strong>
  There should be over 100,000 coefficients in this
  <strong>
   sentiment_model
  </strong>
  . Recall from the lecture that positive weights w_j correspond to weights that cause positive sentiment, while negative weights correspond to negative sentiment. Calculate the number of positive (&gt;= 0, which is actually nonnegative) coefficients.
 </p>
 <p>
  <strong>
   Quiz question:
  </strong>
  How many weights are &gt;= 0?
 </p>
 <h3 level="3">
  Making predictions with logistic regression
 </h3>
 <p>
  <strong>
   9.
  </strong>
  Now that a model is trained, we can make predictions on the
  <strong>
   test data
  </strong>
  . In this section, we will explore this in the context of 3 data points in the test data. Take the 11th, 12th, and 13th data points in the test data and save them to
  <strong>
   sample_test_data
  </strong>
  . The following cell extracts the three data points from the SFrame
  <strong>
   test_data
  </strong>
  and print their content:
 </p>
 <pre language="python">sample_test_data = test_data[10:13]
print sample_test_data</pre>
 <p>
  Let's dig deeper into the first row of the
  <strong>
   sample_test_data
  </strong>
  . Here's the full review:
 </p>
 <pre language="python">sample_test_data[0]['review']</pre>
 <p>
  That review seems pretty positive.
 </p>
 <p>
  Now, let's see what the next row of the
  <strong>
   sample_test_data
  </strong>
  looks like. As we could guess from the rating (-1), the review is quite negative.
 </p>
 <pre language="python">sample_test_data[1]['review']</pre>
 <p>
  <strong>
   10.
  </strong>
  We will now make a class prediction for the
  <strong>
   sample_test_data
  </strong>
  . The
  <strong>
   sentiment_model
  </strong>
  should predict
  <strong>
   +1
  </strong>
  if the sentiment is positive and
  <strong>
   -1
  </strong>
  if the sentiment is negative. Recall from the lecture that the score (sometimes called margin) for the logistic regression model is defined as:
 </p>
 <p hasmath="true">
  $$\mbox{score}_i = \mathbf{w}^\intercal h(\mathbf{x}_i)$$
 </p>
 <p hasmath="true">
  where $$h(\mathbf{x}_i)$$ represents the features for data point i. We will write some code to obtain the scores. For each row, the score (or margin) is a number in the range (-inf, inf). Use a pre-built function in your tool to calculate the score of each data point in
  <strong>
   sample_test_data
  </strong>
  . In scikit-learn, you can call the decision_function() function.
 </p>
 <p>
  <strong>
   Hint
  </strong>
  : You'd probably need to convert sample_test_data into the sparse matrix format first.
 </p>
 <pre language="python">sample_test_matrix = vectorizer.transform(sample_test_data['review_clean'])
scores = sentiment_model.decision_function(sample_test_matrix)
print scores</pre>
 <h3 level="3">
  Prediciting Sentiment
 </h3>
 <p>
  <strong>
   11.
  </strong>
  These scores can be used to make class predictions as follows:
 </p>
 <p hasmath="true">
  $$\hat{y}_i = \begin{cases} +1 &amp; \text{if } \mathbf{w}^\intercal h(\mathbf{x}_i) &gt; 0\\ -1 &amp;\text{if } \mathbf{w}^\intercal h(\mathbf{x}_i) \leq 0 \end{cases}$$
 </p>
 <p>
  Using scores, write code to calculate predicted labels for
  <strong>
   sample_test_data
  </strong>
  .
 </p>
 <p>
  <strong>
   Checkpoint
  </strong>
  : Make sure your class predictions match with the ones obtained from
  <strong>
   sentiment_model
  </strong>
  . The logistic regression classifier in scikit-learn comes with the
  <strong>
   predict
  </strong>
  function for this purpose.
 </p>
 <h3 level="3">
  Probability Predictions
 </h3>
 <p>
  <strong>
   12.
  </strong>
  Recall from the lectures that we can also calculate the probability predictions from the scores using:
 </p>
 <p hasmath="true">
  $$P(y_i = +1 | \mathbf{x}_i, \mathbf{w}) = \dfrac{1}{1+\exp{(-\mathbf{w}^\intercal h(\mathbf{x}_i))}}$$
 </p>
 <p>
  Using the scores calculated previously, write code to calculate the probability that a sentiment is positive using the above formula. For each row, the probabilities should be a number in the range
  <strong>
   [0, 1]
  </strong>
  .
 </p>
 <p>
  <strong>
   Checkpoint
  </strong>
  : Make sure your probability predictions match the ones obtained from
  <strong>
   sentiment_model
  </strong>
  .
 </p>
 <p>
  <strong>
   Quiz question:
  </strong>
  Of the three data points in
  <strong>
   sample_test_data
  </strong>
  , which one (first, second, or third) has the
  <strong>
   lowest probability
  </strong>
  of being classified as a positive review?
 </p>
 <h3 level="3">
  Find the most positive (and negative) review
 </h3>
 <p>
  <strong>
   13.
  </strong>
  We now turn to examining the full test dataset,
  <strong>
   test_data
  </strong>
  , and use sklearn.linear_model.LogisticRegression to form predictions on all of the test data points.
 </p>
 <p>
  Using the sentiment_model, find the 20 reviews in the entire
  <strong>
   test_data
  </strong>
  with the
  <strong>
   highest probability
  </strong>
  of being classified as a
  <strong>
   positive review
  </strong>
  . We refer to these as the "most positive reviews."
 </p>
 <p>
  To calculate these top-20 reviews, use the following steps:
 </p>
 <ol bullettype="numbers">
  <li>
   <p>
    Make probability predictions on
    <strong>
     test_data
    </strong>
    using the
    <strong>
     sentiment_model
    </strong>
    .
   </p>
  </li>
  <li>
   <p>
    Sort the data according to those predictions and pick the top 20.
   </p>
  </li>
 </ol>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Which of the following products are represented in the 20 most positive reviews?
 </p>
 <p>
  <strong>
   14.
  </strong>
  Now, let us repeat this exercise to find the "most negative reviews." Use the prediction probabilities to find the 20 reviews in the
  <strong>
   test_data
  </strong>
  with the
  <strong>
   lowest probability
  </strong>
  of being classified as a
  <strong>
   positive review
  </strong>
  . Repeat the same steps above but make sure you
  <strong>
   sort in the opposite order
  </strong>
  .
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Which of the following products are represented in the 20 most negative reviews?
 </p>
 <h3 level="3">
  Compute accuracy of the classifier
 </h3>
 <p>
  <strong>
   15.
  </strong>
  We will now evaluate the accuracy of the trained classifier. Recall that the accuracy is given by
 </p>
 <p hasmath="true">
  $$\mbox{accuracy} = \dfrac{\mbox{# correctly classified examples}}{\mbox{# total examples}}$$
 </p>
 <p>
  This can be computed as follows:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <strong>
     Step 1:
    </strong>
    Use the
    <strong>
     sentiment_model
    </strong>
    to compute class predictions.
   </p>
  </li>
  <li>
   <p>
    <strong>
     Step 2:
    </strong>
    Count the number of data points when the predicted class labels match the ground truth labels.
   </p>
  </li>
  <li>
   <p>
    <strong>
     Step 3:
    </strong>
    Divide the total number of correct predictions by the total number of data points in the dataset.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Quiz Question
  </strong>
  : What is the accuracy of the
  <strong>
   sentiment_model
  </strong>
  on the
  <strong>
   test_data
  </strong>
  ? Round your answer to 2 decimal places (e.g. 0.76).
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Does a higher accuracy value on the
  <strong>
   training_data
  </strong>
  always imply that the classifier is better?
 </p>
 <h3 level="3">
  Learn another classifier with fewer words
 </h3>
 <p>
  <strong>
   16.
  </strong>
  There were a lot of words in the model we trained above. We will now train a simpler logistic regression model using only a subet of words that occur in the reviews. For this assignment, we selected 20 words to work with. These are:
 </p>
 <pre language="python">significant_words = ['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', 
      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', 
      'work', 'product', 'money', 'would', 'return']</pre>
 <p>
  Compute a new set of word count vectors using only these words. The CountVectorizer class has a parameter that lets you limit the choice of words when building word count vectors:
 </p>
 <pre language="python">vectorizer_word_subset = CountVectorizer(vocabulary=significant_words) # limit to 20 words
train_matrix_word_subset = vectorizer_word_subset.fit_transform(train_data['review_clean'])
test_matrix_word_subset = vectorizer_word_subset.transform(test_data['review_clean'])</pre>
 <p>
  Compute word count vectors for the training and test data and obtain the sparse matrices
  <strong>
   train_matrix_word_subset
  </strong>
  and
  <strong>
   test_matrix_word_subset
  </strong>
  , respectively.
 </p>
 <h3 level="3">
  Train a logistic regression model on a subset of data
 </h3>
 <p>
  <strong>
   17.
  </strong>
  Now build a logistic regression classifier with
  <strong>
   train_matrix_word_subset
  </strong>
  as features and
  <strong>
   sentiment
  </strong>
  as the target. Call this model
  <strong>
   simple_model
  </strong>
  .
 </p>
 <p>
  <strong>
   18.
  </strong>
  Let us inspect the weights (coefficients) of the
  <strong>
   simple_model.
  </strong>
  First, build a table to store (word, coefficient) pairs. If you are using SFrame with scikit-learn, you can combine words with coefficients by running
 </p>
 <pre language="python">simple_model_coef_table = sframe.SFrame({'word':significant_words,
                                         'coefficient':simple_model.coef_.flatten()})</pre>
 <p>
  <strong>
  </strong>
  Sort the data frame by the coefficient value in descending order.
 </p>
 <p>
  <strong>
   Note
  </strong>
  : Make sure that the intercept term is excluded from this table.
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Consider the coefficients of
  <strong>
   simple_model
  </strong>
  . How many of the 20 coefficients (corresponding to the 20
  <strong>
   significant_words
  </strong>
  ) are positive for the
  <strong>
   simple_model
  </strong>
  ?
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Are the positive words in the
  <strong>
   simple_model
  </strong>
  also positive words in the
  <strong>
   sentiment_model
  </strong>
  ?
 </p>
 <h3 level="3">
  Comparing models
 </h3>
 <p>
  <strong>
   19.
  </strong>
  We will now compare the accuracy of the
  <strong>
   sentiment_model
  </strong>
  and the
  <strong>
   simple_model
  </strong>
  .
 </p>
 <p>
  First, compute the classification accuracy of the
  <strong>
   sentiment_model
  </strong>
  on the
  <strong>
   train_data.
  </strong>
 </p>
 <p>
  Now, compute the classification accuracy of the
  <strong>
   simple_model
  </strong>
  on the
  <strong>
   train_data.
  </strong>
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Which model (
  <strong>
   sentiment_model
  </strong>
  or
  <strong>
   simple_model
  </strong>
  ) has higher accuracy on the TRAINING set?
 </p>
 <p>
  <strong>
   20.
  </strong>
  Now, we will repeat this exercise on the
  <strong>
   test_data
  </strong>
  . Start by computing the classification accuracy of the
  <strong>
   sentiment_model
  </strong>
  on the
  <strong>
   test_data
  </strong>
  .
 </p>
 <p>
  Next, compute the classification accuracy of the
  <strong>
   simple_model
  </strong>
  on the
  <strong>
   test_data.
  </strong>
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Which model (
  <strong>
   sentiment_model
  </strong>
  or
  <strong>
   simple_model
  </strong>
  ) has higher accuracy on the TEST set?
 </p>
 <h3 level="3">
  Baseline: Majority class prediction
 </h3>
 <p>
  <strong>
   21.
  </strong>
  It is quite common to use the
  <strong>
   majority class classifier
  </strong>
  as the a baseline (or reference) model for comparison with your classifier model. The majority classifier model predicts the majority class for all data points. At the very least, you should healthily beat the majority class classifier, otherwise, the model is (usually) pointless.
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Enter the accuracy of the majority class classifier model on the
  <strong>
   test_data
  </strong>
  . Round your answer to two decimal places (e.g. 0.76).
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  : Is the
  <strong>
   sentiment_model
  </strong>
  definitely better than the majority class classifier (the baseline)?
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
