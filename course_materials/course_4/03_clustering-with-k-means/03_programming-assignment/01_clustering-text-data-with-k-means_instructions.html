<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Clustering text data with k-means
 </h1>
 <p>
  In this assignment you will
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Cluster Wikipedia documents using k-means
   </p>
  </li>
  <li>
   <p>
    Explore the role of random initialization on the quality of the clustering
   </p>
  </li>
  <li>
   <p>
    Explore how results differ after changing the number of clusters
   </p>
  </li>
  <li>
   <p>
    Evaluate clustering, both quantitatively and qualitatively
   </p>
  </li>
 </ul>
 <p>
  When properly executed, clustering uncovers valuable insights from a set of unlabeled documents.
 </p>
 <h2 level="2">
  If you are using GraphLab Create
 </h2>
 <p>
  An IPython Notebook has been provided below to you for this assignment. This notebook contains the instructions, quiz questions and partially-completed code for you to use as well as some cells to test your code.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the Wikipedia people dataset in SFrame format:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wbzPOZcWEeaXRw6bxrYUfA" name="people_wiki.gl">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the companion IPython notebook:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wdt7RJcWEea2tg7d5YqbXg" name="2_kmeans-with-text-data_blank.ipynb">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the NumPy array containing pre-computed list of clusters:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wcAEEpcWEea_cQqzDLeQwg" name="kmeans-arrays.npz">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Save all the files in the same directory (where you are calling IPython notebook from) and unzip the data file.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Open the companion IPython notebook and follow the instructions in the notebook. The instructions below do not apply to users of GraphLab Create.
  </strong>
 </p>
 <h2 level="2">
  If you are not using GraphLab Create
 </h2>
 <p>
  It is possible to complete this assignment without using GraphLab Create. The instructions below are geared towards Python users, but you are free to adapt them to your specific environment.
 </p>
 <p>
  <strong>
   Disclaimer
  </strong>
  . We have tested the assessment using the standard Python installation (with access to scikit-learn). However, the assessment may not be compatible with other tools (e.g. Matlab, R).
 </p>
 <h3 level="3">
  Download the dataset
 </h3>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the Wikipedia people dataset in SFrame format:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wbzPOZcWEeaXRw6bxrYUfA" name="people_wiki.gl">
 </asset>
 <p>
  For those experimenting with other tools:
 </p>
 <asset assettype="generic" extension="zip" id="Yumlh5cXEeaR7Qr3yid5Zg" name="people_wiki.csv">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the mapping between words and integer indices:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="i6sln5cXEeaGLgqdPwfqeA" name="people_wiki_map_index_to_word.gl">
 </asset>
 <p>
  For those experimenting with other tools:
 </p>
 <asset assettype="generic" extension="zip" id="i6oUa5cXEeahEhKiddgzxA" name="people_wiki_map_index_to_word.json">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the pre-processed set of TF-IDF scores:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="5iOzHJcXEeaMvRLid-X_BA" name="people_wiki_tf_idf.npz">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the NumPy array containing pre-computed list of clusters:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wcAEEpcWEea_cQqzDLeQwg" name="kmeans-arrays.npz">
 </asset>
 <h3 level="3">
  Import packages
 </h3>
 <pre language="python">import sframe                                                  # see below for install instruction
import matplotlib.pyplot as plt                                # plotting
import numpy as np                                             # dense matrices
from scipy.sparse import csr_matrix                            # sparse matrices
from sklearn.preprocessing import normalize                    # normalizing vectors
from sklearn.metrics import pairwise_distances                 # pairwise distances
import sys      
import os
%matplotlib inline</pre>
 <p>
  <strong>
   About SFrame
  </strong>
  . SFrame is a dataframe library Turi has released free-of-charge. Its source code is available
  <a href="https://github.com/turi-code/SFrame">
   here
  </a>
  . You may install SFrame via pip:
 </p>
 <pre language="sh">pip install --upgrade sframe</pre>
 <h3 level="3">
  Load data, extract features
 </h3>
 <p>
  To work with text data, we must first convert the documents into numerical features. As in the first assignment, let's extract TF-IDF features for each article.
 </p>
 <p>
  For your convenience, we extracted the TF-IDF vectors from the dataset. The vectors are packaged in a sparse matrix, where the i-th row gives the TF-IDF vectors for the i-th document. Each column corresponds to a unique word appearing in the dataset.
 </p>
 <pre language="python">def load_sparse_csr(filename):
    loader = np.load(filename)
    data = loader['data']
    indices = loader['indices']
    indptr = loader['indptr']
    shape = loader['shape']

    return csr_matrix( (data, indices, indptr), shape)

wiki = sframe.SFrame('people_wiki.gl/')
tf_idf = load_sparse_csr('people_wiki_tf_idf.npz')
map_index_to_word = sframe.SFrame('people_wiki_map_index_to_word.gl/')</pre>
 <p>
  <em>
   (Optional) Extracting TF-IDF vectors yourself
  </em>
  . We provide the pre-computed TF-IDF vectors to minimize potential compatibility issues. You are free to experiment with other tools to compute the TF-IDF vectors yourself. A good place to start is
  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">
   sklearn.TfidfVectorizer
  </a>
  . Note. Due to variations in
  <a href="https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)">
   tokenization
  </a>
  and other factors, your TF-IDF vectors may differ from the ones we provide. For the purpose the assessment, we ask you to use the vectors from people_wiki_tf_idf.npz.
 </p>
 <h3 level="3">
  Normalize all vectors
 </h3>
 <p>
  As discussed in the previous assignment, Euclidean distance can be a poor metric of similarity between documents, as it unfairly penalizes long articles. For a reasonable assessment of similarity, we should disregard the length information and use length-agnostic metrics, such as cosine distance.
 </p>
 <p>
  The k-means algorithm does not directly work with cosine distance, so we take an alternative route to remove length information: we normalize all vectors to be unit length. It turns out that Euclidean distance closely mimics cosine distance when all vectors are unit length. In particular, the squared Euclidean distance between any two vectors of length one is directly proportional to their cosine distance.
 </p>
 <p hasmath="true">
  We can prove this as follows. Let $$\mathbf{x}$$ and $$\mathbf{y}$$ be normalized vectors, i.e. unit vectors, so that $$\|\mathbf{x}\|=\|\mathbf{y}\|=1$$. Write the squared Euclidean distance as the dot product of $$(\mathbf{x}-\mathbf{y})$$ to itself:
 </p>
 <p hasmath="true">
  $$\|\mathbf{x}-\mathbf{y}\|^2 = (\mathbf{x} - \mathbf{y})^T(\mathbf{x} - \mathbf{y}) = \|\mathbf{x}\|^2 - 2(\mathbf{x}^T \mathbf{y}) + \|\mathbf{y}\|^2 = 2 - 2(\mathbf{x}^T \mathbf{y}) = 2\left(1 - \dfrac{\mathbf{x}^T\mathbf{y}}{\|\mathbf{x}\|\|\mathbf{y}\|}\right)$$
 </p>
 <p>
  This tells us that two
  <strong>
   unit vectors
  </strong>
  that are close in Euclidean distance are also close in cosine distance. Thus, the k-means algorithm (which naturally uses Euclidean distances) on normalized vectors will produce the same results as clustering using cosine distance as a distance metric.
 </p>
 <p>
  We import the
  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.normalize.html">
   normalize() function
  </a>
  from scikit-learn to normalize all vectors to unit length.
 </p>
 <pre language="python">tf_idf = normalize(tf_idf)</pre>
 <h3 level="3">
  Implement k-means
 </h3>
 <p>
  Let us implement the k-means algorithm. First, we choose an initial set of centroids. A common practice is to choose randomly from the data points.
 </p>
 <p>
  <strong>
   Note:
  </strong>
  We specify a seed here, so that everyone gets the same answer. In practice, we highly recommend to use different seeds every time (for instance, by using the current timestamp).
 </p>
 <pre language="python">def get_initial_centroids(data, k, seed=None):
    '''Randomly choose k data points as initial centroids'''
    if seed is not None: # useful for obtaining consistent results
        np.random.seed(seed)
    n = data.shape[0] # number of data points
        
    # Pick K indices from range [0, N).
    rand_indices = np.random.randint(0, n, k)
    
    # Keep centroids as dense format, as many entries will be nonzero due to averaging.
    # As long as at least one document in a cluster contains a word,
    # it will carry a nonzero weight in the TF-IDF vector of the centroid.
    centroids = data[rand_indices,:].toarray()
    
    return centroids</pre>
 <p>
  After initialization, the k-means algorithm iterates between the following two steps:
 </p>
 <ol bullettype="numbers">
  <li>
   <p>
    Assign each data point to the closest centroid.
   </p>
  </li>
  <li>
   <p>
    Revise centroids as the mean of the assigned data points.
   </p>
  </li>
 </ol>
 <p>
  In pseudocode, we iteratively do the following:
 </p>
 <pre language="python">cluster_assignment = assign_clusters(data, centroids)
centroids = revise_centroids(data, k, cluster_assignment)</pre>
 <p>
  <strong>
   Assigning clusters.
  </strong>
  How do we implement Step 1 of the main k-means loop above? First import pairwise_distances function from scikit-learn, which calculates Euclidean distances between rows of given arrays. See this documentation for more information.
 </p>
 <p>
  For the sake of demonstration, let's look at documents 100 through 102 as query documents and compute the distances between each of these documents and every other document in the corpus. In the k-means algorithm, we will have to compute pairwise distances between the set of centroids and the set of documents.
 </p>
 <pre language="python"># Get the TF-IDF vectors for documents 100 through 102.
queries = tf_idf[100:102,:]

# Compute pairwise distances from every data point to each query vector.
dist = pairwise_distances(tf_idf, queries, metric='euclidean')

print dist</pre>
 <p>
  This cell outputs a 2D array of form
 </p>
 <pre language="plain_text">[[ 1.41000789  1.36894636]
 [ 1.40935215  1.41023886]
 [ 1.39855967  1.40890299]
 ..., 
 [ 1.41108296  1.39123646]
 [ 1.41022804  1.31468652]
 [ 1.39899784  1.41072448]]</pre>
 <p>
  More formally, dist[i,j] is assigned the distance between the ith row of X (i.e., X[i,:]) and the jth row of Y (i.e., Y[j,:]).
 </p>
 <p>
  <strong>
   Checkpoint:
  </strong>
  For a moment, suppose that we initialize three centroids with the first 3 rows of tf_idf. Write code to compute distances from each of the centroids to all data points in tf_idf. Then find the distance between row 430 of tf_idf and the second centroid and save it to
  <strong>
   dist
  </strong>
  . Run the following cell to check your answer.
 </p>
 <pre language="python">'''Test cell'''
if np.allclose(dist, pairwise_distances(tf_idf[430,:], tf_idf[1,:])):
    print('Pass')
else:
    print('Check your code again')</pre>
 <p>
  <strong>
   Checkpoint:
  </strong>
  Next, given the pairwise distances, we take the minimum of the distances for each data point. Fittingly, NumPy provides an argmin function. See
  <a href="http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.argmin.html">
   this documentation
  </a>
  for details.
 </p>
 <p>
  Read the documentation and write code to produce a 1D array whose i-th entry indicates the centroid that is the closest to the i-th data point. Use the list of distances from the previous checkpoint and save them as distances. The value 0 indicates closeness to the first centroid, 1 indicates closeness to the second centroid, and so forth. Save this array as
  <strong>
   closest_cluster
  </strong>
  . Run the following cell to check your answer.
 </p>
 <p>
  <strong>
   Hint:
  </strong>
  the resulting array should be as long as the number of data points.
 </p>
 <pre language="python">'''Test cell'''
reference = [list(row).index(min(row)) for row in distances]
if np.allclose(closest_cluster, reference):
    print('Pass')
else:
    print('Check your code again')</pre>
 <p>
  <strong>
   Checkpoint:
  </strong>
  Let's put these steps together. First, initialize three centroids with the first 3 rows of tf_idf. Then, compute distances from each of the centroids to all data points in tf_idf. Finally, use these distance calculations to compute cluster assignments and assign them to cluster_assignment.  Run the following cell to check your code.
 </p>
 <pre language="python">if len(cluster_assignment)==59071 and \
   np.array_equal(np.bincount(cluster_assignment), np.array([23061, 10086, 25924])):
    print('Pass') # count number of data points for each cluster
else:
    print('Check your code again.')</pre>
 <p>
  Now we are ready to fill in the blanks in this function:
 </p>
 <pre language="python">def assign_clusters(data, centroids):
    
    # Compute distances between each data point and the set of centroids:
    # Fill in the blank (RHS only)
    distances_from_centroids = ...
    
    # Compute cluster assignments for each data point:
    # Fill in the blank (RHS only)
    cluster_assignment = ...
    
    return cluster_assignment</pre>
 <p>
  which is simply generalization of what we did above.
 </p>
 <p>
  <strong>
   Checkpoint
  </strong>
  . For the last time, let us check if Step 1 was implemented correctly. With rows 0, 2, 4, and 6 of tf_idf as an initial set of centroids, we assign cluster labels to rows 0, 10, 20, ..., and 90 of tf_idf. The resulting cluster labels should be [0, 1, 1, 0, 0, 2, 0, 2, 2, 1].
 </p>
 <pre language="python">if np.allclose(assign_clusters(tf_idf[0:100:10], tf_idf[0:8:2]), np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1])):
    print('Pass')
else:
    print('Check your code again.')</pre>
 <h3 level="3">
  Revising clusters
 </h3>
 <p>
  Let's turn to Step 2, where we compute the new centroids given the cluster assignments.
 </p>
 <p>
  SciPy and NumPy arrays allow for filtering via Boolean masks. For instance, we filter all data points that are assigned to cluster 0 by writing "data[cluster_assignment==0,:]".
 </p>
 <p>
  To develop intuition about filtering, let's look at a toy example consisting of 3 data points and 2 clusters.
 </p>
 <pre language="python">data = np.array([[1., 2., 0.],
                 [0., 0., 0.],
                 [2., 2., 0.]])
centroids = np.array([[0.5, 0.5, 0.],
                      [0., -0.5, 0.]])</pre>
 <p>
  Let's assign these data points to the closest centroid.
 </p>
 <pre language="python">cluster_assignment = assign_clusters(data, centroids)
print cluster_assignment   # prints [0 1 0]</pre>
 <p>
  The expression "cluster_assignment==1" gives a list of Booleans that says whether each data point is assigned to cluster 1 or not. For cluster 0, the expression is "cluster_assignment==0".  In lieu of indices, we can put in the list of Booleans to pick and choose rows. Only the rows that correspond to a True entry will be retained.
 </p>
 <p>
  First, let's look at the data points (i.e., their values) assigned to cluster 1:
 </p>
 <pre language="python">print data[cluster_assignment==1]</pre>
 <p>
  The output makes sense since [0 0 0] is closer to [0 -0.5 0] than to [0.5 0.5 0].
 </p>
 <p>
  Now let's look at the data points assigned to cluster 0:
 </p>
 <pre language="python">print data[cluster_assignment==0]</pre>
 <p>
  Again, this makes sense since these values are each closer to [0.5 0.5 0] than to [0 -0.5 0].
 </p>
 <p>
  Given all the data points in a cluster, it only remains to compute the mean. Use
  <a href="http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.mean.html">
   np.mean()
  </a>
  . By default, the function averages all elements in a 2D array. To compute row-wise or column-wise means, add the axis argument. See the linked documentation for details.
 </p>
 <p>
  Use this function to average the data points in cluster 0:
 </p>
 <pre language="python">print data[cluster_assignment==0].mean(axis=0)</pre>
 <p>
  We are now ready to complete this function:
 </p>
 <pre language="python">def revise_centroids(data, k, cluster_assignment):
    new_centroids = []
    for i in xrange(k):
        # Select all data points that belong to cluster i. Fill in the blank (RHS only)
        member_data_points = ...
        # Compute the mean of the data points. Fill in the blank (RHS only)
        centroid = ...
        
        # Convert numpy.matrix type to numpy.ndarray type
        centroid = centroid.A1
        new_centroids.append(centroid)
    new_centroids = np.array(new_centroids)
    
    return new_centroids</pre>
 <p>
  <strong>
   Checkpoint
  </strong>
  . Let's check our Step 2 implementation. Letting rows 0, 10, ..., 90 of tf_idf as the data points and the cluster labels [0, 1, 1, 0, 0, 2, 0, 2, 2, 1], we compute the next set of centroids. Each centroid is given by the average of all member data points in corresponding cluster.
 </p>
 <pre language="python">result = revise_centroids(tf_idf[0:100:10], 3, np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1]))
if np.allclose(result[0], np.mean(tf_idf[[0,30,40,60]].toarray(), axis=0)) and \
   np.allclose(result[1], np.mean(tf_idf[[10,20,90]].toarray(), axis=0))   and \
   np.allclose(result[2], np.mean(tf_idf[[50,70,80]].toarray(), axis=0)):
    print('Pass')
else:
    print('Check your code')</pre>
 <h3 level="3">
  Assessing convergence
 </h3>
 <p>
  How can we tell if the k-means algorithm is converging? We can look at the cluster assignments and see if they stabilize over time. In fact, we'll be running the algorithm until the cluster assignments stop changing at all. To be extra safe, and to assess the clustering performance, we'll be looking at an additional criteria: the sum of all squared distances between data points and centroids. This is defined as
 </p>
 <p hasmath="true">
  $$J(\mathcal{Z},\mu) = \sum_{j=1}^k \sum_{i:z_i = j} \|\mathbf{x}_i - \mu_j\|^2.$$
 </p>
 <p>
  The smaller the distances, the more homogeneous the clusters are. In other words, we'd like to have "tight" clusters.
 </p>
 <pre language="python">def compute_heterogeneity(data, k, centroids, cluster_assignment):
    
    heterogeneity = 0.0
    for i in xrange(k):
        
        # Select all data points that belong to cluster i. Fill in the blank (RHS only)
        member_data_points = data[cluster_assignment==i, :]
        
        if member_data_points.shape[0] &gt; 0: # check if i-th cluster is non-empty
            # Compute distances from centroid to data points (RHS only)
            distances = pairwise_distances(member_data_points, [centroids[i]], metric='euclidean')
            squared_distances = distances**2
            heterogeneity += np.sum(squared_distances)
        
    return heterogeneity</pre>
 <h3 level="3">
  Combining into a single function
 </h3>
 <p>
  Once the two k-means steps have been implemented, as well as our heterogeneity metric we wish to monitor, it is only a matter of putting these functions together to write a k-means algorithm that
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Repeatedly performs Steps 1 and 2
   </p>
  </li>
  <li>
   <p>
    Tracks convergence metrics
   </p>
  </li>
  <li>
   <p>
    Stops if either no assignment changed or we reach a certain number of iterations.
   </p>
  </li>
 </ul>
 <pre language="python"># Fill in the blanks
def kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False):
    '''This function runs k-means on given data and initial set of centroids.
       maxiter: maximum number of iterations to run.
       record_heterogeneity: (optional) a list, to store the history of heterogeneity as function of iterations
                             if None, do not store the history.
       verbose: if True, print how many data points changed their cluster labels in each iteration'''
    centroids = initial_centroids[:]
    prev_cluster_assignment = None
    
    for itr in xrange(maxiter):        
        if verbose:
            print(itr)
        
        # 1. Make cluster assignments using nearest centroids
        # YOUR CODE HERE
        cluster_assignment = ...
            
        # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.
        # YOUR CODE HERE
        centroids = ...
            
        # Check for convergence: if none of the assignments changed, stop
        if prev_cluster_assignment is not None and \
          (prev_cluster_assignment==cluster_assignment).all():
            break
        
        # Print number of new assignments 
        if prev_cluster_assignment is not None:
            num_changed = np.sum(prev_cluster_assignment!=cluster_assignment)
            if verbose:
                print('    {0:5d} elements changed their cluster assignment.'.format(num_changed))   
        
        # Record heterogeneity convergence metric
        if record_heterogeneity is not None:
            # YOUR CODE HERE
            score = ...
            record_heterogeneity.append(score)
        
        prev_cluster_assignment = cluster_assignment[:]
        
    return centroids, cluster_assignment</pre>
 <h3 level="3">
  Plotting convergence metric
 </h3>
 <p>
  We can use the above function to plot the convergence metric across iterations.
 </p>
 <pre language="python">def plot_heterogeneity(heterogeneity, k):
    plt.figure(figsize=(7,4))
    plt.plot(heterogeneity, linewidth=4)
    plt.xlabel('# Iterations')
    plt.ylabel('Heterogeneity')
    plt.title('Heterogeneity of clustering over time, K={0:d}'.format(k))
    plt.rcParams.update({'font.size': 16})
    plt.tight_layout()</pre>
 <p>
  Let's consider running k-means with K=3 clusters for a maximum of 400 iterations, recording cluster heterogeneity at every step. Then, let's plot the heterogeneity over iterations using the plotting function above.
 </p>
 <pre language="python">k = 3
heterogeneity = []
initial_centroids = get_initial_centroids(tf_idf, k, seed=0)
centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,
                                       record_heterogeneity=heterogeneity, verbose=True)
plot_heterogeneity(heterogeneity, k)</pre>
 <p>
  <strong>
   Quiz Question.
  </strong>
  (True/False) The clustering objective (heterogeneity) is non-increasing for this example.
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  . Let's step back from this particular example. If the clustering objective (heterogeneity) would ever increase when running k-means, that would indicate: (choose one)
 </p>
 <ol bullettype="numbers">
  <li>
   <p>
    k-means algorithm got stuck in a bad local minimum
   </p>
  </li>
  <li>
   <p>
    There is a bug in the k-means code
   </p>
  </li>
  <li>
   <p>
    All data points consist of exact duplicates
   </p>
  </li>
  <li>
   <p>
    Nothing is wrong. The objective should generally go down sooner or later.
   </p>
  </li>
 </ol>
 <p>
  <strong>
   Quiz Question.
  </strong>
  Which of the cluster contains the greatest number of data points in the end? Hint: Use np.bincount() to count occurrences of each cluster label.
 </p>
 <ol bullettype="numbers">
  <li>
   <p>
    Cluster #0
   </p>
  </li>
  <li>
   <p>
    Cluster #1
   </p>
  </li>
  <li>
   <p>
    Cluster #2
   </p>
  </li>
 </ol>
 <h3 level="3">
  Beware of local minima
 </h3>
 <p>
  One weakness of k-means is that it tends to get stuck in a local minimum. To see this, let us run k-means multiple times, with different initial centroids created using different random seeds.
 </p>
 <p>
  <strong>
   Note:
  </strong>
  Again, in practice, you should set different seeds for every run. We give you a list of seeds for this assignment so that everyone gets the same answer.
 </p>
 <p>
  This may take several minutes to run.
 </p>
 <pre language="python">k = 10
heterogeneity = {}
import time
start = time.time()
for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:
    initial_centroids = get_initial_centroids(tf_idf, k, seed)
    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,
                                           record_heterogeneity=None, verbose=False)
    # To save time, compute heterogeneity only once in the end
    heterogeneity[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)
    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))
    sys.stdout.flush()
end = time.time()
print(end-start)</pre>
 <p>
  Notice the variation in heterogeneity for different initializations. This indicates that k-means sometimes gets stuck at a bad local minimum.
 </p>
 <p>
  <strong>
   Quiz Question.
  </strong>
  Another way to capture the effect of changing initialization is to look at the distribution of cluster assignments. Add a line to the code above to compute the size (# of member data points) of clusters for each run of k-means. Look at the size of the largest cluster (most # of member data points) across multiple runs, with seeds 0, 20000, ..., 120000. How much does this measure vary across the runs? What is the minimum and maximum values this quantity takes?
 </p>
 <p>
  One effective way to counter this tendency is to use
  <strong>
   k-means++
  </strong>
  to provide a smart initialization. This method tries to spread out the initial set of centroids so that they are not too close together. It is known to improve the quality of local optima and lower average runtime.
 </p>
 <pre language="python">def smart_initialize(data, k, seed=None):
    '''Use k-means++ to initialize a good set of centroids'''
    if seed is not None: # useful for obtaining consistent results
        np.random.seed(seed)
    centroids = np.zeros((k, data.shape[1]))
    
    # Randomly choose the first centroid.
    # Since we have no prior knowledge, choose uniformly at random
    idx = np.random.randint(data.shape[0])
    centroids[0] = data[idx,:].toarray()
    # Compute distances from the first centroid chosen to all the other data points
    squared_distances = pairwise_distances(data, centroids[0:1], metric='euclidean').flatten()**2
    
    for i in xrange(1, k):
        # Choose the next centroid randomly, so that the probability for each data point to be chosen
        # is directly proportional to its squared distance from the nearest centroid.
        # Roughtly speaking, a new centroid should be as far as from ohter centroids as possible.
        idx = np.random.choice(data.shape[0], 1, p=squared_distances/sum(squared_distances))
        centroids[i] = data[idx,:].toarray()
        # Now compute distances from the centroids to all data points
        squared_distances = np.min(pairwise_distances(data, centroids[0:i+1], metric='euclidean')**2,axis=1)
    
    return centroidsa fe</pre>
 <p>
  Let's now rerun k-means with 10 clusters using the same set of seeds, but always using k-means++ to initialize the algorithm.
 </p>
 <p>
  This may take several minutes to run.
 </p>
 <pre language="python">k = 10
heterogeneity_smart = {}
start = time.time()
for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:
    initial_centroids = smart_initialize(tf_idf, k, seed)
    centroids, cluster_assignment = kmeans(tf_idf, k, initial_centroids, maxiter=400,
                                           record_heterogeneity=None, verbose=False)
    # To save time, compute heterogeneity only once in the end
    heterogeneity_smart[seed] = compute_heterogeneity(tf_idf, k, centroids, cluster_assignment)
    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity_smart[seed]))
    sys.stdout.flush()
end = time.time()
print(end-start)</pre>
 <p>
  Let's compare the set of cluster heterogeneities we got from our 7 restarts of k-means using random initialization compared to the 7 restarts of k-means using k-means++ as a smart initialization. The following code produces a
  <a href="http://matplotlib.org/api/pyplot_api.html">
   box plot
  </a>
  for each of these methods, indicating the spread of values produced by each method.
 </p>
 <pre language="python">plt.figure(figsize=(8,5))
plt.boxplot([heterogeneity.values(), heterogeneity_smart.values()], vert=False)
plt.yticks([1, 2], ['k-means', 'k-means++'])
plt.rcParams.update({'font.size': 16})
plt.tight_layout()</pre>
 <p>
 </p>
 <img alt="" assetid="Psb9mn-SEeaNlA6zo4Pi2Q" src="data:image/png;base64,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"/>
 <p>
 </p>
 <p>
  A few things to notice from the box plot:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    On average, k-means++ produces a better clustering than Random initialization.
   </p>
  </li>
  <li>
   <p>
    Variation in clustering quality is smaller for k-means++.
   </p>
  </li>
 </ul>
 <p>
  In general, you should run k-means at least a few times with different initializations and then return the run resulting in the lowest heterogeneity. Let us write a function that runs k-means multiple times and picks the best run that minimizes heterogeneity. The function accepts an optional list of seed values to be used for the multiple runs; if no such list is provided, the current UTC time is used as seed values.
 </p>
 <pre language="python">def kmeans_multiple_runs(data, k, maxiter, num_runs, seed_list=None, verbose=False):
    heterogeneity = {}
    
    min_heterogeneity_achieved = float('inf')
    best_seed = None
    final_centroids = None
    final_cluster_assignment = None
    
    for i in xrange(num_runs):
        
        # Use UTC time if no seeds are provided 
        if seed_list is not None: 
            seed = seed_list[i]
            np.random.seed(seed)
        else: 
            seed = int(time.time())
            np.random.seed(seed)
        
        # Use k-means++ initialization
        # YOUR CODE HERE
        initial_centroids = ...
        
        # Run k-means
        # YOUR CODE HERE
        centroids, cluster_assignment = ...
        
        # To save time, compute heterogeneity only once in the end
        # YOUR CODE HERE
        heterogeneity[seed] = ...
        
        if verbose:
            print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))
            sys.stdout.flush()
        
        # if current measurement of heterogeneity is lower than previously seen,
        # update the minimum record of heterogeneity.
        if heterogeneity[seed] &lt; min_heterogeneity_achieved:
            min_heterogeneity_achieved = heterogeneity[seed]
            best_seed = seed
            final_centroids = centroids
            final_cluster_assignment = cluster_assignment
    
    # Return the centroids and cluster assignments that minimize heterogeneity.
    return final_centroids, final_cluster_assignment</pre>
 <h3 level="3">
  How to choose K
 </h3>
 <p>
  Since we are measuring the tightness of the clusters, a higher value of K reduces the possible heterogeneity metric by definition. For example, if we have N data points and set K=N clusters, then we could have 0 cluster heterogeneity by setting the N centroids equal to the values of the N data points. (Note: Not all runs for larger K will result in lower heterogeneity than a single run with smaller K due to local optima.) Let's see explore this general trend for ourselves by performing the following analysis.
 </p>
 <p>
  Use the kmeans_multiple_runs function to run k-means with five different values of K. For each K, use k-means++ and
  <strong>
   multiple run
  </strong>
  s to pick the best solution. In what follows, we consider K=2,10,25,50,100 and 7 restarts for each setting.
 </p>
 <p>
  IMPORTANT: The code block below will take about one hour to finish. We highly suggest that you use the arrays that we have computed for you. Side note: In practice, a good implementation of k-means would utilize parallelism to run multiple runs of k-means at once. For an example, see
  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html">
   scikit-learn's KMeans
  </a>
  .
 </p>
 <pre language="python">#def plot_k_vs_heterogeneity(k_values, heterogeneity_values):
#    plt.figure(figsize=(7,4))
#    plt.plot(k_values, heterogeneity_values, linewidth=4)
#    plt.xlabel('K')
#    plt.ylabel('Heterogeneity')
#    plt.title('K vs. Heterogeneity')
#    plt.rcParams.update({'font.size': 16})
#    plt.tight_layout()

#start = time.time()
#centroids = {}
#cluster_assignment = {}
#heterogeneity_values = []
#k_list = [2, 10, 25, 50, 100]
#seed_list = [0, 20000, 40000, 60000, 80000, 100000, 120000]

#for k in k_list:
#    heterogeneity = []
#    centroids[k], cluster_assignment[k] = kmeans_multiple_runs(tf_idf, k, maxiter=400,
#                                                               num_runs=len(seed_list),
#                                                               seed_list=seed_list,
#                                                               verbose=True)
#    score = compute_heterogeneity(tf_idf, k, centroids[k], cluster_assignment[k])
#    heterogeneity_values.append(score)

#plot_k_vs_heterogeneity(k_list, heterogeneity_values)

#end = time.time()
#print(end-start)</pre>
 <p>
  To use the pre-computed NumPy arrays, first download kmeans-arrays.npz as mentioned in the reading for this assignment and load them with the following code. Make sure the downloaded file is in the same directory as this notebook.
 </p>
 <pre language="python">def plot_k_vs_heterogeneity(k_values, heterogeneity_values):
    plt.figure(figsize=(7,4))
    plt.plot(k_values, heterogeneity_values, linewidth=4)
    plt.xlabel('K')
    plt.ylabel('Heterogeneity')
    plt.title('K vs. Heterogeneity')
    plt.rcParams.update({'font.size': 16})
    plt.tight_layout()

filename = 'kmeans-arrays.npz'

heterogeneity_values = []
k_list = [2, 10, 25, 50, 100]

if os.path.exists(filename):
    arrays = np.load(filename)
    centroids = {}
    cluster_assignment = {}
    for k in k_list:
        print k
        sys.stdout.flush()
        centroids[k] = arrays['centroids_{0:d}'.format(k)]
        cluster_assignment[k] = arrays['cluster_assignment_{0:d}'.format(k)]
        score = compute_heterogeneity(tf_idf, k, centroids[k], cluster_assignment[k])
        heterogeneity_values.append(score)
    
    plot_k_vs_heterogeneity(k_list, heterogeneity_values)

else:
    print('File not found. Skipping.')</pre>
 <p>
 </p>
 <p>
 </p>
 <img alt="" assetid="Lb_lskBUEeai_BLpE3ZESw" src="data:image/png;base64,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"/>
 <p>
  In the above plot we show that heterogeneity goes down as we increase the number of clusters. Does this mean we should always favor a higher K?
  <strong>
   Not at all!
  </strong>
  As we will see in the following section, setting K too high may end up separating data points that are actually pretty alike. At the extreme, we can set individual data points to be their own clusters (K=N) and achieve zero heterogeneity, but separating each data point into its own cluster is hardly a desirable outcome. In the following section, we will learn how to detect a K set "too large".
 </p>
 <h3 level="3">
  Visualize clusters of documents
 </h3>
 <p>
  Let's start visualizing some clustering results to see if we think the clustering makes sense. We can use such visualizations to help us assess whether we have set K too large or too small for a given application. Following the theme of this course, we will judge whether the clustering makes sense in the context of document analysis.
 </p>
 <p>
  What are we looking for in a good clustering of documents?
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Documents in the same cluster should be similar.
   </p>
  </li>
  <li>
   <p>
    Documents from different clusters should be less similar.
   </p>
  </li>
 </ul>
 <p>
  So a bad clustering exhibits either of two symptoms:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Documents in a cluster have mixed content.
   </p>
  </li>
  <li>
   <p>
    Documents with similar content are divided up and put into different clusters.
   </p>
  </li>
 </ul>
 <p>
  To help visualize the clustering, we do the following:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Fetch nearest neighbors of each centroid from the set of documents assigned to that cluster. We will consider these documents as being representative of the cluster.
   </p>
  </li>
  <li>
   <p>
    Print titles and first sentences of those nearest neighbors.
   </p>
  </li>
  <li>
   <p>
    Print top 5 words that have highest tf-idf weights in each centroid.
   </p>
  </li>
 </ul>
 <pre language="python">def visualize_document_clusters(wiki, tf_idf, centroids, cluster_assignment, k, map_index_to_word, display_content=True):
    '''wiki: original dataframe
       tf_idf: data matrix, sparse matrix format
       map_index_to_word: SFrame specifying the mapping betweeen words and column indices
       display_content: if True, display 8 nearest neighbors of each centroid'''
    
    print('==========================================================')

    # Visualize each cluster c
    for c in xrange(k):
        # Cluster heading
        print('Cluster {0:d}    '.format(c)),
        # Print top 5 words with largest TF-IDF weights in the cluster
        idx = centroids[c].argsort()[::-1]
        for i in xrange(5): # Print each word along with the TF-IDF weight
            print('{0:s}:{1:.3f}'.format(map_index_to_word['category'][idx[i]], centroids[c,idx[i]])),
        print('')
        
        if display_content:
            # Compute distances from the centroid to all data points in the cluster,
            # and compute nearest neighbors of the centroids within the cluster.
            distances = pairwise_distances(tf_idf, [centroids[c]], metric='euclidean').flatten()
            distances[cluster_assignment!=c] = float('inf') # remove non-members from consideration
            nearest_neighbors = distances.argsort()
            # For 8 nearest neighbors, print the title as well as first 180 characters of text.
            # Wrap the text at 80-character mark.
            for i in xrange(8):
                text = ' '.join(wiki[nearest_neighbors[i]]['text'].split(None, 25)[0:25])
                print('\n* {0:50s} {1:.5f}\n  {2:s}\n  {3:s}'.format(wiki[nearest_neighbors[i]]['name'],
                    distances[nearest_neighbors[i]], text[:90], text[90:180] if len(text) &gt; 90 else ''))
        print('==========================================================')</pre>
 <p>
  Let us first look at the 2 cluster case (K=2).
 </p>
 <pre language="python">visualize_document_clusters(wiki, tf_idf, centroids[2], cluster_assignment[2], 2, map_index_to_word)</pre>
 <p>
  Both clusters have mixed content, although cluster 1 is much purer than cluster 0:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Cluster 0: artists, songwriters, professors, politicians, writers, etc.
   </p>
  </li>
  <li>
   <p>
    Cluster 1: baseball players, hockey players, soccer (association football) players, etc.
   </p>
  </li>
 </ul>
 <p>
  Top words of cluster 1 are all related to sports, whereas top words of cluster 0 show no clear pattern.
 </p>
 <p>
  Roughly speaking, the entire dataset was divided into athletes and non-athletes. It would be better if we sub-divided non-athletes into more categories. So let us use more clusters. How about K=10?
 </p>
 <pre language="python">k = 10
visualize_document_clusters(wiki, tf_idf, centroids[k], cluster_assignment[k], k, map_index_to_word)</pre>
 <p>
  Clusters 0, 1, and 5 appear to be still mixed, but others are quite consistent in content.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Cluster 0: artists, actors, film directors, playwrights
   </p>
  </li>
  <li>
   <p>
    Cluster 1: soccer (association football) players, rugby players
   </p>
  </li>
  <li>
   <p>
    Cluster 2: track and field athletes
   </p>
  </li>
  <li>
   <p>
    Cluster 3: baseball players
   </p>
  </li>
  <li>
   <p>
    Cluster 4: professors, researchers, scholars
   </p>
  </li>
  <li>
   <p>
    Cluster 5: Austrailian rules football players, American football players
   </p>
  </li>
  <li>
   <p>
    Cluster 6: female figures from various fields
   </p>
  </li>
  <li>
   <p>
    Cluster 7: composers, songwriters, singers, music producers
   </p>
  </li>
  <li>
   <p>
    Cluster 8: ice hockey players
   </p>
  </li>
  <li>
   <p>
    Cluster 9: politicians
   </p>
  </li>
 </ul>
 <p>
  Clusters are now more pure, but some are qualitatively "bigger" than others. For instance, the category of scholars is more general than the category of baseball players. Increasing the number of clusters may split larger clusters. Another way to look at the size of cluster is to count the number of articles in each cluster.
 </p>
 <pre language="python">np.bincount(cluster_assignment[10])</pre>
 <p>
  <strong>
   Quiz Question
  </strong>
  . Which of the 10 clusters above contains the greatest number of articles?
 </p>
 <p>
  <strong>
   Quiz Question
  </strong>
  . Which of the 10 clusters contains the least number of articles?
 </p>
 <p>
  There appears to be at least some connection between the topical consistency of a cluster and the number of its member data points.
 </p>
 <p>
  Let us visualize the case for K=25. For the sake of brevity, we do not print the content of documents. It turns out that the top words with highest TF-IDF weights in each cluster are representative of the cluster.
 </p>
 <pre language="python">visualize_document_clusters(wiki, tf_idf, centroids[25], cluster_assignment[25], 25,
                            map_index_to_word, display_content=False) # turn off text for brevity</pre>
 <p>
  Looking at the representative examples and top words, we classify each cluster as follows. Notice the bolded items, which indicate the appearance of a new theme.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Cluster 0:
    <strong>
     lawyers, judges, legal scholars
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 1:
    <strong>
     professors, researchers, scholars (natural and health sciences)
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 2: ice hockey players
   </p>
  </li>
  <li>
   <p>
    Cluster 3: politicans
   </p>
  </li>
  <li>
   <p>
    Cluster 4:
    <strong>
     government officials
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 5: politicans
   </p>
  </li>
  <li>
   <p>
    Cluster 6:
    <strong>
     professors, researchers, scholars (social sciences and humanities)
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 7: Canadian politicians
   </p>
  </li>
  <li>
   <p>
    Cluster 8:
    <strong>
     car racers
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 9:
    <strong>
     economists
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 10: track and field athletes
   </p>
  </li>
  <li>
   <p>
    Cluster 11: females from various fields
   </p>
  </li>
  <li>
   <p>
    Cluster 12: (mixed; no clear theme)
   </p>
  </li>
  <li>
   <p>
    Cluster 13: baseball players
   </p>
  </li>
  <li>
   <p>
    Cluster 14:
    <strong>
     painters, sculptors, artists
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 15: Austrailian rules football players, American football players
   </p>
  </li>
  <li>
   <p>
    Cluster 16:
    <strong>
     musicians, composers
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 17: soccer (association football) players, rugby players
   </p>
  </li>
  <li>
   <p>
    Cluster 18:
    <strong>
     poets
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 19:
    <strong>
     film directors, playwrights
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 20:
    <strong>
     songwriters, singers, music producers
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 21:
    <strong>
     generals of U.S. Air Force
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 22:
    <strong>
     music directors, conductors
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 23:
    <strong>
     basketball players
    </strong>
   </p>
  </li>
  <li>
   <p>
    Cluster 24:
    <strong>
     golf players
    </strong>
   </p>
  </li>
 </ul>
 <p>
  Indeed, increasing K achieved the desired effect of breaking up large clusters. Depending on the application, this may or may not be preferable to the K=10 analysis.
 </p>
 <p>
  Let's take it to the extreme and set K=100. We have a suspicion that this value is too large. Let us look at the top words from each cluster:
 </p>
 <pre language="python">k=100
visualize_document_clusters(wiki, tf_idf, centroids[k], cluster_assignment[k], k,
                            map_index_to_word, display_content=False)
# turn off text for brevity -- turn it on if you are curious ;)</pre>
 <p>
  The class of soccer (association football) players has been broken into two clusters (44 and 45). Same goes for Austrialian rules football players (clusters 26 and 48). The class of baseball players have been also broken into two clusters (16 and 91).
 </p>
 <p>
  <strong>
   A high value of K encourages pure clusters, but we cannot keep increasing K. For large enough K, related documents end up going to different clusters.
  </strong>
 </p>
 <p>
  That said, the result for K=100 is not entirely bad. After all, it gives us separate clusters for such categories as Brazil, wrestling, computer science and the Mormon Church. If we set K somewhere between 25 and 100, we should be able to avoid breaking up clusters while discovering new ones.
 </p>
 <p>
  Also, we should ask ourselves how much
  <strong>
   granularity
  </strong>
  we want in our clustering. If we wanted a rough sketch of Wikipedia, we don't want too detailed clusters. On the other hand, having many clusters can be valuable when we are zooming into a certain part of Wikipedia.
 </p>
 <p>
  <strong>
   There is no golden rule for choosing K. It all depends on the particular application and domain we are in.
  </strong>
 </p>
 <p>
  Another heuristic people use that does not rely on so much visualization, which can be hard in many applications (including here!) is as follows. Track heterogeneity versus K and look for the "elbow" of the curve where the heterogeneity decrease rapidly before this value of K, but then only gradually for larger values of K. This naturally trades off between trying to minimize heterogeneity, but reduce model complexity. In the heterogeneity versus K plot made above, we did not yet really see a flattening out of the heterogeneity, which might indicate that indeed K=100 is "reasonable" and we only see real overfitting for larger values of K (which are even harder to visualize using the methods we attempted above.)
 </p>
 <p>
  <strong>
   Quiz Question.
  </strong>
  Another sign of too large K is having lots of small clusters. Look at the distribution of cluster sizes (by number of member data points). How many of the 100 clusters have fewer than 236 articles, i.e. 0.4% of the dataset?
 </p>
 <h3 level="3">
  Takeaway
  <a href="http://localhost:8888/notebooks/machine-learning-specialization-private/course-4/2_kmeans-with-text-data_sklearn.ipynb#Takeaway">
  </a>
 </h3>
 <p>
  Keep in mind though that tiny clusters aren't necessarily bad. A tiny cluster of documents that really look like each others is definitely preferable to a medium-sized cluster of documents with mixed content. However, having too few articles in a cluster may cause overfitting by reading too much into limited pool of training data.
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
