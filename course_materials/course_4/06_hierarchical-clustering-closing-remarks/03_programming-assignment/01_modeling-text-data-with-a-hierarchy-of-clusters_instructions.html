<meta charset="utf-8"/>
<co-content>
 <h1 level="1">
  Modeling text data with a hierarchy of clusters
 </h1>
 <p>
  <strong>
   Hierarchical clustering
  </strong>
  refers to a class of clustering methods that seek to build a
  <strong>
   hierarchy
  </strong>
  of clusters, in which some clusters contain others. In this assignment, we will explore a top-down approach, recursively bipartitioning the data using k-means.
 </p>
 <h2 level="2">
  If you are using GraphLab Create
 </h2>
 <p>
  An IPython Notebook has been provided below to you for this assignment. This notebook contains the instructions, quiz questions and partially-completed code for you to use as well as some cells to test your code.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the Wikipedia people dataset in SFrame format:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wbzPOZcWEeaXRw6bxrYUfA" name="people_wiki.gl">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the companion IPython notebook:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wb-18ZcWEea_cQqzDLeQwg" name="6_hierarchical_clustering_blank.ipynb">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download a collection of helper functions:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wb-2TJcWEeaK1Q4gRyvE8A" name="em_utilities.py">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Save the files in the same directory (where you are calling IPython notebook from) and unzip the data file and model file.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Open the companion IPython notebook and follow the instructions in the notebook. The instructions below do not apply to users of GraphLab Create.
  </strong>
 </p>
 <h2 level="2">
  If you are not using GraphLab Create
 </h2>
 <p>
  It is possible to complete this assignment without using GraphLab Create. The instructions below are geared towards Python users, but you are free to adapt them to your specific environment.
 </p>
 <p>
  <strong>
   Disclaimer
  </strong>
  . We have tested the assessment using the standard Python installation (with access to scikit-learn). However, the assessment may not be compatible with other tools (e.g. Matlab, R).
 </p>
 <h3 level="3">
  Download the dataset
 </h3>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the Wikipedia people dataset in SFrame format
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="wbzPOZcWEeaXRw6bxrYUfA" name="people_wiki.gl">
 </asset>
 <p>
  For those experimenting with other tools:
 </p>
 <asset assettype="generic" extension="zip" id="Yumlh5cXEeaR7Qr3yid5Zg" name="people_wiki.csv">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the mapping between words and integer indices:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="i6sln5cXEeaGLgqdPwfqeA" name="people_wiki_map_index_to_word.gl">
 </asset>
 <p>
  For those experimenting with other tools:
 </p>
 <asset assettype="generic" extension="zip" id="i6oUa5cXEeahEhKiddgzxA" name="people_wiki_map_index_to_word.json">
 </asset>
 <ul bullettype="bullets">
  <li>
   <p>
    Download the pre-processed set of TF-IDF scores:
   </p>
  </li>
 </ul>
 <asset assettype="generic" extension="zip" id="5iOzHJcXEeaMvRLid-X_BA" name="people_wiki_tf_idf.npz">
 </asset>
 <h3 level="3">
  Import packages
 </h3>
 <pre language="python">import sframe                                     # see below for install instruction
import matplotlib.pyplot as plt
import numpy as np
from scipy.sparse import csr_matrix
from sklearn.cluster import KMeans                # we'll be using scikit-learn's KMeans for this assignment
from sklearn.metrics import pairwise_distances
from sklearn.preprocessing import normalize
%matplotlib inline</pre>
 <p>
  <strong>
   About SFrame
  </strong>
  . SFrame is a dataframe library Turi has released free-of-charge. Its source code is available
  <a href="https://github.com/turi-code/SFrame">
   here
  </a>
  . You may install SFrame via pip:
 </p>
 <pre language="sh">pip install --upgrade sframe</pre>
 <h3 level="3">
  Load in the dataset
 </h3>
 <pre language="python">wiki = sframe.SFrame('people_wiki.gl/')</pre>
 <p>
  As in the previous assignment, we extract the TF-IDF vector of each document.
 </p>
 <p>
  For your convenience, we extracted the TF-IDF vectors from the dataset. The vectors are packaged in a sparse matrix, where the i-th row gives the TF-IDF vectors for the i-th document. Each column corresponds to a unique word appearing in the dataset.
 </p>
 <p>
  To load in the TF-IDF vectors, run
 </p>
 <pre language="python">def load_sparse_csr(filename):
    loader = np.load(filename)
    data = loader['data']
    indices = loader['indices']
    indptr = loader['indptr']
    shape = loader['shape']
    
    return csr_matrix( (data, indices, indptr), shape)

tf_idf = load_sparse_csr('people_wiki_tf_idf.npz')
map_index_to_word = sframe.SFrame('people_wiki_map_index_to_word.gl/')</pre>
 <p>
  To be consistent with the k-means assignment, let's normalize all vectors to have unit norm.
 </p>
 <pre language="python">tf_idf = normalize(tf_idf)</pre>
 <p>
  <em>
   (Optional) Extracting TF-IDF vectors yourself
  </em>
  . We provide the pre-computed TF-IDF vectors to minimize potential compatibility issues. You are free to experiment with other tools to compute the TF-IDF vectors yourself. A good place to start is
  <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">
   sklearn.TfidfVectorizer
  </a>
  . Note. Due to variations in
  <a href="https://en.wikipedia.org/wiki/Tokenization_(lexical_analysis)">
   tokenization
  </a>
  and other factors, your TF-IDF vectors may differ from the ones we provide. For the purpose the assessment, we ask you to use the vectors from people_wiki_tf_idf.npz.
 </p>
 <h3 level="3">
  Bipartition the Wikipedia dataset using k-means
 </h3>
 <p>
  Recall our workflow for clustering text data with k-means:
 </p>
 <ol bullettype="numbers">
  <li>
   <p>
    Load the dataframe containing a dataset, such as the Wikipedia text dataset.
   </p>
  </li>
  <li>
   <p>
    Extract the data matrix from the dataframe.
   </p>
  </li>
  <li>
   <p>
    Run k-means on the data matrix with some value of k.
   </p>
  </li>
  <li>
   <p>
    Visualize the clustering results using the centroids, cluster assignments, and the original dataframe. We keep the original dataframe around because the data matrix does not keep auxiliary information (in the case of the text dataset, the title of each article).
   </p>
  </li>
 </ol>
 <p>
  Let us modify the workflow to perform bipartitioning:
 </p>
 <ol bullettype="numbers">
  <li>
   <p>
    Load the dataframe containing a dataset, such as the Wikipedia text dataset.
   </p>
  </li>
  <li>
   <p>
    Extract the data matrix from the dataframe.
   </p>
  </li>
  <li>
   <p>
    Run k-means on the data matrix with k=2.
   </p>
  </li>
  <li>
   <p>
    Divide the data matrix into two parts using the cluster assignments.
   </p>
  </li>
  <li>
   <p>
    Divide the dataframe into two parts, again using the cluster assignments. This step is necessary to allow for visualization.
   </p>
  </li>
  <li>
   <p>
    Visualize the bipartition of data.
   </p>
  </li>
 </ol>
 <p>
  We'd like to be able to repeat Steps 3-6 multiple times to produce a
  <strong>
   hierarchy
  </strong>
  of clusters such as the following:
 </p>
 <pre language="plain_text">                      (root)
                         |
            +------------+-------------+
            |                          |
         Cluster                    Cluster
     +------+-----+             +------+-----+
     |            |             |            |
   Cluster     Cluster       Cluster      Cluster</pre>
 <p>
  Each
  <strong>
   parent cluster
  </strong>
  is bipartitioned to produce two
  <strong>
   child clusters
  </strong>
  . At the very top is the
  <strong>
   root cluster
  </strong>
  , which consists of the entire dataset.
 </p>
 <p>
  Now we write a wrapper function to bipartition a given cluster using k-means. There are three variables that together comprise the cluster:
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    dataframe: a subset of the original dataframe that correspond to member rows of the cluster
   </p>
  </li>
  <li>
   <p>
    matrix: same set of rows, stored in sparse matrix format
   </p>
  </li>
  <li>
   <p>
    centroid: the centroid of the cluster (not applicable for the root cluster)
   </p>
  </li>
 </ul>
 <p>
  Rather than passing around the three variables separately, we package them into a Python dictionary. The wrapper function takes a single dictionary (representing a parent cluster) and returns two dictionaries (representing the child clusters).
 </p>
 <pre language="python">def bipartition(cluster, maxiter=400, num_runs=4, seed=None):
    '''cluster: should be a dictionary containing the following keys
                * dataframe: original dataframe
                * matrix:    same data, in matrix format
                * centroid:  centroid for this particular cluster'''
    
    data_matrix = cluster['matrix']
    dataframe   = cluster['dataframe']
    
    # Run k-means on the data matrix with k=2. We use scikit-learn here to simplify workflow.
    kmeans_model = KMeans(n_clusters=2, max_iter=maxiter, n_init=num_runs, random_state=seed, n_jobs=1)    
    kmeans_model.fit(data_matrix)
    centroids, cluster_assignment = kmeans_model.cluster_centers_, kmeans_model.labels_
    
    # Divide the data matrix into two parts using the cluster assignments.
    data_matrix_left_child, data_matrix_right_child = data_matrix[cluster_assignment==0], \
                                                      data_matrix[cluster_assignment==1]
    
    # Divide the dataframe into two parts, again using the cluster assignments.
    cluster_assignment_sa = sframe.SArray(cluster_assignment) # minor format conversion
    dataframe_left_child, dataframe_right_child     = dataframe[cluster_assignment_sa==0], \
                                                      dataframe[cluster_assignment_sa==1]
        
    
    # Package relevant variables for the child clusters
    cluster_left_child  = {'matrix': data_matrix_left_child,
                           'dataframe': dataframe_left_child,
                           'centroid': centroids[0]}
    cluster_right_child = {'matrix': data_matrix_right_child,
                           'dataframe': dataframe_right_child,
                           'centroid': centroids[1]}
    
    return (cluster_left_child, cluster_right_child)</pre>
 <p>
  The following cell performs bipartitioning of the Wikipedia dataset. Allow 20-60 seconds to finish.
 </p>
 <p>
  Note. For the purpose of the assignment, we set an explicit seed (seed=1) to produce identical outputs for every run. In pratical applications, you might want to use different random seeds for all runs.
 </p>
 <pre language="python">wiki_data = {'matrix': tf_idf, 'dataframe': wiki} # no 'centroid' for the root cluster
left_child, right_child = bipartition(wiki_data, maxiter=100, num_runs=6, seed=1)</pre>
 <p>
  Let's examine the contents of one of the two clusters, which we call the left_child, referring to the tree visualization above.
 </p>
 <pre language="python">print left_child
print right_child</pre>
 <h3 level="3">
  Visualize the bipartition
 </h3>
 <p>
  We provide you with a modified version of the visualization function from the k-means assignment. For each cluster, we print the top 5 words with highest TF-IDF weights in the centroid and display excerpts for the 8 nearest neighbors of the centroid.
 </p>
 <pre language="python">def display_single_tf_idf_cluster(cluster, map_index_to_word):
    '''map_index_to_word: SFrame specifying the mapping betweeen words and column indices'''
    
    wiki_subset   = cluster['dataframe']
    tf_idf_subset = cluster['matrix']
    centroid      = cluster['centroid']
    
    # Print top 5 words with largest TF-IDF weights in the cluster
    idx = centroid.argsort()[::-1]
    for i in xrange(5):
        print('{0:s}:{1:.3f}'.format(map_index_to_word['category'][idx[i]], centroid[idx[i]])),
    print('')
    
    # Compute distances from the centroid to all data points in the cluster.
    distances = pairwise_distances(tf_idf_subset, [centroid], metric='euclidean').flatten()
    # compute nearest neighbors of the centroid within the cluster.
    nearest_neighbors = distances.argsort()
    # For 8 nearest neighbors, print the title as well as first 180 characters of text.
    # Wrap the text at 80-character mark.
    for i in xrange(8):
        text = ' '.join(wiki_subset[nearest_neighbors[i]]['text'].split(None, 25)[0:25])
        print('* {0:50s} {1:.5f}\n  {2:s}\n  {3:s}'.format(wiki_subset[nearest_neighbors[i]]['name'],
              distances[nearest_neighbors[i]], text[:90], text[90:180] if len(text) &gt; 90 else ''))
    print('')</pre>
 <p>
  Let's visualize the two child clusters:
 </p>
 <pre language="python">display_single_tf_idf_cluster(left_child, map_index_to_word)
display_single_tf_idf_cluster(right_child, map_index_to_word)</pre>
 <p>
  The left cluster consists of athletes, whereas the right cluster consists of non-athletes. So far, we have a single-level hierarchy consisting of two clusters, as follows:
 </p>
 <pre language="plain_text">                                           Wikipedia
                                               +
                                               |
                    +--------------------------+--------------------+
                    |                                               |
                    +                                               +
                 Athletes                                      Non-athletes</pre>
 <p>
  Is this hierarchy good enough?
  <strong>
   When building a hierarchy of clusters, we must keep our particular application in mind.
  </strong>
  For instance, we might want to build a
  <strong>
   directory
  </strong>
  for Wikipedia articles. A good directory would let you quickly narrow down your search to a small set of related articles. The categories of athletes and non-athletes are too general to facilitate efficient search. For this reason, we decide to build another level into our hierarchy of clusters with the goal of getting more specific cluster structure at the lower level. To that end, we subdivide both the athletes and non-athletes clusters.
 </p>
 <h3 level="3">
  Perform recursive bipartitioning
 </h3>
 <p>
  <strong>
   Cluster of athletes.
  </strong>
  To help identify the clusters we've built so far, let's give them easy-to-read aliases:
 </p>
 <pre language="python">athletes = left_child
non_athletes = right_child</pre>
 <p>
  Using the bipartition function, we produce two child clusters of the athlete cluster:
 </p>
 <pre language="python"># Bipartition the cluster of athletes
left_child_athletes, right_child_athletes = bipartition(athletes, maxiter=100, num_runs=6, seed=1)</pre>
 <p>
  The left child cluster mainly consists of baseball players:
 </p>
 <pre language="python">display_single_tf_idf_cluster(left_child_athletes, map_index_to_word)</pre>
 <p>
  On the other hand, the right child cluster is a mix of players in association football, Austrailian rules football and ice hockey:
 </p>
 <pre language="python">display_single_tf_idf_cluster(right_child_athletes, map_index_to_word)</pre>
 <p>
  Our hierarchy of clusters now looks like this:
 </p>
 <pre language="plain_text">                                           Wikipedia
                                               +
                                               |
                    +--------------------------+--------------------+
                    |                                               |
                    +                                               +
                 Athletes                                      Non-athletes
                    +
                    |
        +-----------+--------+
        |                    |
        |            association football/
        +          Austrailian rules football/
     baseball             ice hockey</pre>
 <p>
  Should we keep subdividing the clusters? If so, which cluster should we subdivide? To answer this question, we again think about our application. Since we organize our directory by topics, it would be nice to have topics that are about as coarse as each other. For instance, if one cluster is about baseball, we expect some other clusters about football, basketball, volleyball, and so forth. That is,
  <strong>
   we would like to achieve similar level of granularity for all clusters.
  </strong>
 </p>
 <p>
  Notice that the right child cluster is more coarse than the left child cluster. The right cluster posseses a greater variety of topics than the left (ice hockey/association football/Australian football vs. baseball). So the right child cluster should be subdivided further to produce finer child clusters.
 </p>
 <p>
  Let's give the clusters aliases as well:
 </p>
 <pre language="python">baseball            = left_child_athletes
ice_hockey_football = right_child_athletes</pre>
 <p>
  <strong>
   Cluster of ice hockey players and football players.
  </strong>
  In answering the following quiz question, take a look at the topics represented in the top documents (those closest to the centroid), as well as the list of words with highest TF-IDF weights.
 </p>
 <p>
  Let us bipartition the cluster of ice hockey and football players.
 </p>
 <pre language="python">left_child_ihs, right_child_ihs = bipartition(ice_hockey_football, maxiter=100, num_runs=6, seed=1)
display_single_tf_idf_cluster(left_child_ihs, map_index_to_word)
display_single_tf_idf_cluster(right_child_ihs, map_index_to_word)</pre>
 <p>
  <strong>
   Quiz Question
  </strong>
  . Which diagram best describes the hierarchy right after splitting the
  <strong>
   ice_hockey_football
  </strong>
  cluster? Refer to the quiz form for the diagrams.
 </p>
 <p>
  <strong>
   Caution
  </strong>
  . The granularity criteria is an imperfect heuristic and must be taken with a grain of salt. It takes a lot of manual intervention to obtain a good hierarchy of clusters.
 </p>
 <ul bullettype="bullets">
  <li>
   <p>
    <strong>
     If a cluster is highly mixed, the top articles and words may not convey the full picture of the cluster.
    </strong>
    Thus, we may be misled if we judge the purity of clusters solely by their top documents and words.
   </p>
  </li>
  <li>
   <p>
    <strong>
     Many interesting topics are hidden somewhere inside the clusters but do not appear in the visualization.
    </strong>
    We may need to subdivide further to discover new topics. For instance, subdividing the ice_hockey_football cluster led to the appearance of golf.
   </p>
  </li>
 </ul>
 <p>
  <strong>
   Cluster of non-athletes
  </strong>
  . Now let us subdivide the cluster of non-athletes.
 </p>
 <pre language="python"># Bipartition the cluster of non-athletes
left_child_non_athletes, right_child_non_athletes = bipartition(non_athletes, maxiter=100, num_runs=6, seed=1)

display_single_tf_idf_cluster(left_child_non_athletes, map_index_to_word)
display_single_tf_idf_cluster(right_child_non_athletes, map_index_to_word)</pre>
 <p>
  Neither of the clusters show clear topics, apart from the genders. Let us divide them further.
 </p>
 <pre language="python">male_non_athletes = left_child_non_athletes
female_non_athletes = right_child_non_athletes</pre>
 <p>
  <strong>
   Quiz Question
  </strong>
  . Let us bipartition the clusters
  <strong>
   male_non_athletes
  </strong>
  and
  <strong>
   female_non_athletes
  </strong>
  . Which diagram best describes the resulting hierarchy of clusters for the non-athletes? Refer to the quiz for the diagrams.
 </p>
 <p>
  <strong>
   Note
  </strong>
  . Use maxiter=100, num_runs=6, seed=1 for consistency of output.
 </p>
</co-content>
<style>
 body {
    padding: 50px 85px 50px 85px;
}

table th, table td {
    border: 1px solid #e0e0e0;
    padding: 5px 20px;
    text-align: left;
}
input {
    margin: 10px;
}
}
th {
    font-weight: bold;
}
td, th {
    display: table-cell;
    vertical-align: inherit;
}
img {
    height: auto;
    max-width: 100%;
}
pre {
    display: block;
    margin: 20px;
    background: #424242;
    color: #fff;
    font-size: 13px;
    white-space: pre-wrap;
    padding: 9.5px;
    margin: 0 0 10px;
    border: 1px solid #ccc;
}
</style>
<script async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$$','$$'], ['$','$'] ],
      displayMath: [ ["\\[","\\]"] ],
      processEscapes: true
    }
  });
</script>
