[MUSIC] So finally, I just wanted to present
the coordinate descent algorithm for lasso if you don't
normalize your features. So this is the most generic
form of the algorithm, because of course it applies to
normalized features as well. But let's just remember our algorithm for
our normalized features. So, here it is now. And relative to this, the only changes we need to make are
what's highlighted in these green boxes. And what we see is that we need to
precompute for each one of our features. This term is Zj, and that's exactly
equivalent to the normalizer that we described when we normalized our features. So if you don't normalize,
you still have to compute this normalizer. But we're gonna use it in a different way
as we're going through this algorithm. Where, when we go to compute roh j, we're
looking at our unnormalized features. And when we're forming our predictions,
y hat sub i, so our prediction for the ith observation, again, that
prediction is using unnormalized features. So there are two places in the rho j
compuation where you would need to change things for unnormalized features. And then finally when we're setting w hat
j according to the soft thresholding rule, instead of just looking at
roh j plus lambda over two, or roh j minus lambda over two, or zero. We're gonna divide each of these
terms by z j, this normalizer. Okay, so you see that it's fairly
straight forward to implement this for unnormalized features, but the intuition
we provided was much clearer for the case of normalized features. [MUSIC]