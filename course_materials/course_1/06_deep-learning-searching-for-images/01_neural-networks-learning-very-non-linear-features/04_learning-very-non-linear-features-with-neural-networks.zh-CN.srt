1
00:00:00,000 --> 00:00:04,692
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community

2
00:00:04,692 --> 00:00:07,416
好 我们讲了一个现实生活中的应用

3
00:00:07,416 --> 00:00:10,850
基于图像特征来寻找鞋子和裙子

4
00:00:10,850 --> 00:00:14,080
今天我们要用到的技术叫做深度学习

5
00:00:14,080 --> 00:00:17,510
特别要提到的是这个技术是建立在神经网络的基础之上

6
00:00:17,510 --> 00:00:21,260
但在我们开始前 让我们来谈谈数据表现形式

7
00:00:21,260 --> 00:00:22,806
我们讨论了像TFIDF这样的文字数据的表现形式

8
00:00:22,806 --> 00:00:27,650
还有词袋模型 但是你要怎么表示图片数据呢？

9
00:00:29,090 --> 00:00:31,910
这些数据被称为特征 是机器学习里的重要部分

10
00:00:33,050 --> 00:00:37,150
那么一般来讲 在谈到机器学习时 
我们会有一些输入值

11
00:00:37,150 --> 00:00:40,200
比如我们在用分类算法来做情绪分析

12
00:00:40,200 --> 00:00:43,160
你给出一个句子 这个句子通过分类器模型

13
00:00:43,160 --> 00:00:46,780
由此我们来决定这个句子有积极情绪还是消极情绪

14
00:00:46,780 --> 00:00:49,300
在图像分类中 我们的目标是从一张图片出发

15
00:00:49,300 --> 00:00:52,990
这个图像的像素 也就是模型的输入值
将通过分类器模型

16
00:00:52,990 --> 00:00:55,920
我们来举个例子 这是我的狗狗 我想区分它

17
00:00:55,920 --> 00:00:59,660
是拉布拉多寻回犬 还是其他品种的狗

18
00:00:59,660 --> 00:01:03,070
就像我们前面说的 特征指的就是我们的数据的表现形式

19
00:01:03,070 --> 00:01:05,000
我们把它们当作输入送入分类器

20
00:01:06,060 --> 00:01:08,060
数据有很多种表现形式 比如说文字

21
00:01:08,060 --> 00:01:11,050
可以用词袋模型和TFIDF的形式

22
00:01:11,050 --> 00:01:13,400
对图片而言 有很多其他的表现形式

23
00:01:13,400 --> 00:01:15,270
在这个单元的学习中 我们会说到其中的一部分

24
00:01:16,330 --> 00:01:18,900
但今天我们来集中讲一讲神经网络

25
00:01:18,900 --> 00:01:21,820
神经网络提供了一种非线性的数据表现形式

26
00:01:23,195 --> 00:01:25,080
现在让我们回到分类算法

27
00:01:25,080 --> 00:01:26,850
我们来复习一下

28
00:01:26,850 --> 00:01:30,710
我们之前讲线性分类器会创造一条线

29
00:01:30,710 --> 00:01:35,630
或者是一个线性决策边界来区分正向类和负向类

30
00:01:35,630 --> 00:01:39,921
这个边界是用 w0 这个数 加上

31
00:01:39,921 --> 00:01:45,730
w1 乘以第一个特征变量x1 
加上w2乘以第二个特征变量x2 等等

32
00:01:45,730 --> 00:01:50,160
在正的一边 分值都大于零

33
00:01:50,160 --> 00:01:52,690
在负的一边 分值都小于零

34
00:01:52,690 --> 00:01:54,920
我要是有一个这么好的分值函数

35
00:01:54,920 --> 00:01:58,180
我就可以把正向类和负向类分开

36
00:01:58,180 --> 00:02:02,410
在神经网络中 我们将用图形来代表这样的分类器

37
00:02:03,530 --> 00:02:08,070
对每一个特征变量 我们都有一个与之对应的节点 x1 x2

38
00:02:08,070 --> 00:02:13,730
这样一直下去 到第d个特征变量xd
对我们要预测的输出值y 也有一个节点对应它

39
00:02:13,730 --> 00:02:16,950
那么第一个特征变量x1乘以权重值w1

40
00:02:16,950 --> 00:02:18,700
我把这个权重值标在了x1和y的连线上

41
00:02:18,700 --> 00:02:21,870
x2乘以权重值w2 我把这个权重值放到第三条线上

42
00:02:21,870 --> 00:02:26,850
就这样一直到特征变量xd 用它乘以wd，
我反这个权重值放到了最后一条线上

43
00:02:26,850 --> 00:02:30,970
至于最后剩下的这个权重值w0 它不和任何特征变量相乘

44
00:02:30,970 --> 00:02:34,900
但它会乘以常数1 所以我们把w0放到最上面

45
00:02:34,900 --> 00:02:40,340
现在想象一下所有权重值 从w0到wd

46
00:02:40,340 --> 00:02:44,850
与各自的常数项或者特征变量 x1到xd以及常数1 相乘
你就可以计算出最后的分值

47
00:02:44,850 --> 00:02:48,400
当这个分值大于零时 我们就让输出值等于一

48
00:02:48,400 --> 00:02:51,550
当这个分值小于零时 我们就让输出值等于零

49
00:02:51,550 --> 00:02:56,650
这就是一个只有一层的小型神经网络的例子

50
00:02:56,650 --> 00:03:01,470
我们把一个小型线性分类器称为一个神经网络

51
00:03:01,470 --> 00:03:02,940
或者称为一个单层神经网络

52
00:03:02,940 --> 00:03:05,770
什么东西可以用单层神经网络来表示呢？

53
00:03:05,770 --> 00:03:10,620
让我们来看看x1 or x2的逻辑或运算

54
00:03:10,620 --> 00:03:14,240
我们能用像刚才说的小型神经网络来表示这些函数吗?

55
00:03:14,240 --> 00:03:17,140
好 让我们先对这些函数下好清楚的定义

56
00:03:17,140 --> 00:03:22,891
我们所拥有的就是变量x1

57
00:03:22,891 --> 00:03:26,910
变量x2 以及输出值y

58
00:03:26,910 --> 00:03:28,680
对于这些变量有以下几种可能性

59
00:03:28,680 --> 00:03:32,998
x1可以是0 x2可以是0
那么因为是逻辑或运算

60
00:03:32,998 --> 00:03:36,340
在这种情况下 输出值y也等于0

61
00:03:36,340 --> 00:03:40,272
当x1是1 x2是0时 结果会是1

62
00:03:40,272 --> 00:03:43,360
当x1是0，x2是1时，结果会是1

63
00:03:43,360 --> 00:03:47,300
类似的 当x1和x2都是1时 结果还是1

64
00:03:47,300 --> 00:03:54,080
我们需要定义一个打分函数
使它在后三行的情况下输出正值

65
00:03:54,080 --> 00:03:59,890
当 在表格最后三行的时候 值大于0
但是对于第一行 小于0

66
00:03:59,890 --> 00:04:01,390
我们怎么做呢？我们该怎么做呢？

67
00:04:01,390 --> 00:04:03,870
其实有很多办法来实现 比如

68
00:04:03,870 --> 00:04:06,020
我分别加一个权重值 1

69
00:04:06,020 --> 00:04:11,620
在x1和x2的边上 然后我们来想一下分值

70
00:04:11,620 --> 00:04:16,830
第一行的分值等于零 而其他行的分值都大于零

71
00:04:16,830 --> 00:04:18,890
看来我们还是要对这些分值做进一步区分

72
00:04:18,890 --> 00:04:21,190
我们可以在第一条边上加一个负的权重值

73
00:04:21,190 --> 00:04:26,200
我们把它设为-0.5 然后我们来看分值会有什么变化

74
00:04:26,200 --> 00:04:33,366
当x1是0 x2是0时 最后分值就变成了-0.5

75
00:04:33,366 --> 00:04:38,460
这样我就得到了一个小于零的分值
而且分值正确 输出值也正确

76
00:04:38,460 --> 00:04:44,610
当x1是1 x2是1时 最后分值是0.5

77
00:04:44,610 --> 00:04:48,220
同样地 当x1是0 x2是1时 分值也是0.5

78
00:04:48,220 --> 00:04:52,680
最后 当x1和x2都是1时 最后的分值等于1.5

79
00:04:52,680 --> 00:04:58,300
就这样 在各条边上加上简单的权重值 
我们就可以把x1 or x2表示出来

80
00:04:59,470 --> 00:05:02,280
现在 我们能把x1与x2的逻辑与运算表示出来吗？

81
00:05:04,040 --> 00:05:09,560
类似地 我们可以把权重值1分别放在x1和

82
00:05:09,560 --> 00:05:16,810
x2的边上 但这次只有当x1和x2都是1的时候
我们才让最后得分大于零

83
00:05:16,810 --> 00:05:21,860
所以常数项即第一条边的权重值
就不能是-0.5 我们把它换成-1.5

84
00:05:21,860 --> 00:05:27,830
如果你像我们在第一个例子里那样把真值表填好

85
00:05:27,830 --> 00:05:31,320
你会注意到 我们正在用单层神经网络来表示x1

86
00:05:31,320 --> 00:05:34,190
和x2的逻辑与运算

87
00:05:34,190 --> 00:05:38,180
所以单层神经网络和

88
00:05:38,180 --> 00:05:43,350
和我们学过的标准线性分类器基本是一样的

89
00:05:43,350 --> 00:05:46,280
那么什么东西是线性分类器无法表示的呢？

90
00:05:46,280 --> 00:05:49,200
它可以表示x1 or x2的逻辑或运算函数

91
00:05:49,200 --> 00:05:52,020
它可以表示x1 and x2的逻辑与运算函数
但是什么样的函数

92
00:05:52,020 --> 00:05:53,700
还是个特别简单的函数
是它没有办法表示的呢？

93
00:05:54,710 --> 00:05:56,350
这里有一个例子这里有一个例子

94
00:05:57,350 --> 00:06:00,760
在这个例子里 没有哪条线可以把加号和减号区分开来

95
00:06:00,760 --> 00:06:02,730
而这个函数叫做XOR 即逻辑异或运算

96
00:06:02,730 --> 00:06:05,830
我喜欢把它称作任何事情的反例

97
00:06:05,830 --> 00:06:07,560
所以任何时候你想找反例的话

98
00:06:07,560 --> 00:06:10,220
第一个值得尝试的例子就是XOR

99
00:06:10,220 --> 00:06:14,940
现在对于这个例子 
我们之前讲过的线性特征就不够用了

100
00:06:14,940 --> 00:06:18,380
我们需要一些非线性特征 这就到了

101
00:06:18,380 --> 00:06:22,110
神经网络模型发挥用处的时候了
我们先来看个例子

102
00:06:22,110 --> 00:06:25,040
让我们来回顾一下XOR异或运算

103
00:06:25,040 --> 00:06:30,680
XOR在这些情况下结果等于一 当x1为真

104
00:06:30,680 --> 00:06:35,300
x2为假 即非x2和非x1的逻辑或运算

105
00:06:35,300 --> 00:06:39,000
当x1为假或者说x1是0 x2是1的时候

106
00:06:39,000 --> 00:06:43,350
那么我们该怎么用神经网络来表示这个函数呢？

107
00:06:43,350 --> 00:06:47,650
我们把第一项叫做z1 第二项叫做z2

108
00:06:49,500 --> 00:06:52,310
我们现在要做的是建一个神经网络

109
00:06:52,310 --> 00:06:56,512
它不能直接通过输入x1和x2来预测y 

110
00:06:56,512 --> 00:07:01,660
但它能预测中间值z1和z2
然后用这些中间值来预测y

111
00:07:03,040 --> 00:07:03,920
现在让我们来看一下z1

112
00:07:03,920 --> 00:07:09,280
我们该怎么只用一个神经网络来预测z1呢？

113
00:07:10,560 --> 00:07:14,100
我们之前已经讨论过这个了 让我们直接来做吧

114
00:07:14,100 --> 00:07:17,820
这次和之前讲的逻辑与运算有一点不同

115
00:07:17,820 --> 00:07:23,480
因为是not x2 我们需要把-1放在x2的边上

116
00:07:23,480 --> 00:07:26,690
然后我们在x1的边上放上1 
在常数项的边上放-0.5

117
00:07:26,690 --> 00:07:28,510
这就是我们对z1的表示方式了

118
00:07:30,050 --> 00:07:35,663
对z2也用相似的表示方法
我们在x1的边上放上-1

119
00:07:35,663 --> 00:07:42,617
就是这条x1到z2的边
我们在这条x2到z2的边上放1

120
00:07:42,617 --> 00:07:49,230
然后在常数项的边上放-0.5，这样就可以来表示z2了

121
00:07:50,430 --> 00:07:55,450
最后一步 如果z1和z2都存在的话
我们剩下要做的就是逻辑或运算

122
00:07:55,450 --> 00:07:59,810
我们已经知道如何对布尔变量做或运算了

123
00:07:59,810 --> 00:08:03,360
也就是1 1  -0.5

124
00:08:03,360 --> 00:08:08,280
现在到了我们的精彩时刻

125
00:08:08,280 --> 00:08:13,160
我们现在已经造出了我们的第一个深度神经网络

126
00:08:13,160 --> 00:08:17,430
虽然不是超深度 仅仅只有两层 但还是有点小兴奋

127
00:08:19,370 --> 00:08:21,360
我们刚造完我们的第一个神经网络

128
00:08:21,360 --> 00:08:24,880
这是一个双层神经网络 总的来说

129
00:08:24,880 --> 00:08:28,050
神经网络指的就是对你的数据进行多层次的变换

130
00:08:28,050 --> 00:08:30,820
然后我们用这些变换后的数据来建立非线性特征集

131
00:08:30,820 --> 00:08:34,200
我们会在计算机视觉领域看到一些例子

132
00:08:34,200 --> 00:08:36,100
如今 神经网络已经发展了

133
00:08:36,100 --> 00:08:40,200
大概50年了 和机器学习存在的时间差不多

134
00:08:40,200 --> 00:08:44,030
然而大约在90年代 神经网络算法失宠了

135
00:08:44,030 --> 00:08:46,890
当时的科学家发现要从神经网络中
获取较高的准确率非常困难

136
00:08:46,890 --> 00:08:49,740
但是大约在十年前 一切都改变了

137
00:08:49,740 --> 00:08:51,910
因为有两个事情发生了

138
00:08:51,910 --> 00:08:56,650
一个是数据大量地增长了 因为神经网络有很多很多

139
00:08:56,650 --> 00:08:58,150
很多很多层

140
00:08:58,150 --> 00:09:01,830
你需要大量的数据 来训练每层神经网络

141
00:09:01,830 --> 00:09:02,840
它们有很多很多的参数

142
00:09:04,030 --> 00:09:07,050
我们会看一个超级让人兴奋的神经网络
它有6000万个参数

143
00:09:07,050 --> 00:09:09,370
所以我们需要大量数据来训练它们

144
00:09:09,370 --> 00:09:12,310
近几年我们从不同的渠道获得了如此高量级的数据

145
00:09:12,310 --> 00:09:14,830
特别是通过网络

146
00:09:15,860 --> 00:09:20,840
而第二件让神经网络成为可能的事件

147
00:09:20,840 --> 00:09:22,940
是计算能力的增强

148
00:09:22,940 --> 00:09:25,760
因为我们需要处理更大的神经网络和更多的数据

149
00:09:25,760 --> 00:09:30,550
我们需要快速的电脑和GPU
GPU原本是设计用来

150
00:09:30,550 --> 00:09:33,070
加速电脑游戏的图像运算的

151
00:09:33,070 --> 00:09:37,150
结果人们发现GPU非常适合用于创建和

152
00:09:37,150 --> 00:09:39,920
使用有着大量数据的神经网络

153
00:09:39,920 --> 00:09:41,570
因为有了GPU

154
00:09:41,570 --> 00:09:44,800
因为这些深度的神经网络 一切都改变了

155
00:09:44,800 --> 00:09:48,338
对我们现实世界已经产生了许多影响

156
00:09:48,338 --> 00:09:52,359
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community