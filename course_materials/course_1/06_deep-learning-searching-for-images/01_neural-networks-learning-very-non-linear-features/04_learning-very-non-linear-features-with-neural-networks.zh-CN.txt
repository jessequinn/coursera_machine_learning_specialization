[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community 好 我们讲了一个现实生活中的应用 基于图像特征来寻找鞋子和裙子 今天我们要用到的技术叫做深度学习 特别要提到的是这个技术是建立在神经网络的基础之上 但在我们开始前 让我们来谈谈数据表现形式 我们讨论了像TFIDF这样的文字数据的表现形式 还有词袋模型 但是你要怎么表示图片数据呢？ 这些数据被称为特征 是机器学习里的重要部分 那么一般来讲 在谈到机器学习时 
我们会有一些输入值 比如我们在用分类算法来做情绪分析 你给出一个句子 这个句子通过分类器模型 由此我们来决定这个句子有积极情绪还是消极情绪 在图像分类中 我们的目标是从一张图片出发 这个图像的像素 也就是模型的输入值
将通过分类器模型 我们来举个例子 这是我的狗狗 我想区分它 是拉布拉多寻回犬 还是其他品种的狗 就像我们前面说的 特征指的就是我们的数据的表现形式 我们把它们当作输入送入分类器 数据有很多种表现形式 比如说文字 可以用词袋模型和TFIDF的形式 对图片而言 有很多其他的表现形式 在这个单元的学习中 我们会说到其中的一部分 但今天我们来集中讲一讲神经网络 神经网络提供了一种非线性的数据表现形式 现在让我们回到分类算法 我们来复习一下 我们之前讲线性分类器会创造一条线 或者是一个线性决策边界来区分正向类和负向类 这个边界是用 w0 这个数 加上 w1 乘以第一个特征变量x1 
加上w2乘以第二个特征变量x2 等等 在正的一边 分值都大于零 在负的一边 分值都小于零 我要是有一个这么好的分值函数 我就可以把正向类和负向类分开 在神经网络中 我们将用图形来代表这样的分类器 对每一个特征变量 我们都有一个与之对应的节点 x1 x2 这样一直下去 到第d个特征变量xd
对我们要预测的输出值y 也有一个节点对应它 那么第一个特征变量x1乘以权重值w1 我把这个权重值标在了x1和y的连线上 x2乘以权重值w2 我把这个权重值放到第三条线上 就这样一直到特征变量xd 用它乘以wd，
我反这个权重值放到了最后一条线上 至于最后剩下的这个权重值w0 它不和任何特征变量相乘 但它会乘以常数1 所以我们把w0放到最上面 现在想象一下所有权重值 从w0到wd 与各自的常数项或者特征变量 x1到xd以及常数1 相乘
你就可以计算出最后的分值 当这个分值大于零时 我们就让输出值等于一 当这个分值小于零时 我们就让输出值等于零 这就是一个只有一层的小型神经网络的例子 我们把一个小型线性分类器称为一个神经网络 或者称为一个单层神经网络 什么东西可以用单层神经网络来表示呢？ 让我们来看看x1 or x2的逻辑或运算 我们能用像刚才说的小型神经网络来表示这些函数吗? 好 让我们先对这些函数下好清楚的定义 我们所拥有的就是变量x1 变量x2 以及输出值y 对于这些变量有以下几种可能性 x1可以是0 x2可以是0
那么因为是逻辑或运算 在这种情况下 输出值y也等于0 当x1是1 x2是0时 结果会是1 当x1是0，x2是1时，结果会是1 类似的 当x1和x2都是1时 结果还是1 我们需要定义一个打分函数
使它在后三行的情况下输出正值 当 在表格最后三行的时候 值大于0
但是对于第一行 小于0 我们怎么做呢？我们该怎么做呢？ 其实有很多办法来实现 比如 我分别加一个权重值 1 在x1和x2的边上 然后我们来想一下分值 第一行的分值等于零 而其他行的分值都大于零 看来我们还是要对这些分值做进一步区分 我们可以在第一条边上加一个负的权重值 我们把它设为-0.5 然后我们来看分值会有什么变化 当x1是0 x2是0时 最后分值就变成了-0.5 这样我就得到了一个小于零的分值
而且分值正确 输出值也正确 当x1是1 x2是1时 最后分值是0.5 同样地 当x1是0 x2是1时 分值也是0.5 最后 当x1和x2都是1时 最后的分值等于1.5 就这样 在各条边上加上简单的权重值 
我们就可以把x1 or x2表示出来 现在 我们能把x1与x2的逻辑与运算表示出来吗？ 类似地 我们可以把权重值1分别放在x1和 x2的边上 但这次只有当x1和x2都是1的时候
我们才让最后得分大于零 所以常数项即第一条边的权重值
就不能是-0.5 我们把它换成-1.5 如果你像我们在第一个例子里那样把真值表填好 你会注意到 我们正在用单层神经网络来表示x1 和x2的逻辑与运算 所以单层神经网络和 和我们学过的标准线性分类器基本是一样的 那么什么东西是线性分类器无法表示的呢？ 它可以表示x1 or x2的逻辑或运算函数 它可以表示x1 and x2的逻辑与运算函数
但是什么样的函数 还是个特别简单的函数
是它没有办法表示的呢？ 这里有一个例子这里有一个例子 在这个例子里 没有哪条线可以把加号和减号区分开来 而这个函数叫做XOR 即逻辑异或运算 我喜欢把它称作任何事情的反例 所以任何时候你想找反例的话 第一个值得尝试的例子就是XOR 现在对于这个例子 
我们之前讲过的线性特征就不够用了 我们需要一些非线性特征 这就到了 神经网络模型发挥用处的时候了
我们先来看个例子 让我们来回顾一下XOR异或运算 XOR在这些情况下结果等于一 当x1为真 x2为假 即非x2和非x1的逻辑或运算 当x1为假或者说x1是0 x2是1的时候 那么我们该怎么用神经网络来表示这个函数呢？ 我们把第一项叫做z1 第二项叫做z2 我们现在要做的是建一个神经网络 它不能直接通过输入x1和x2来预测y 但它能预测中间值z1和z2
然后用这些中间值来预测y 现在让我们来看一下z1 我们该怎么只用一个神经网络来预测z1呢？ 我们之前已经讨论过这个了 让我们直接来做吧 这次和之前讲的逻辑与运算有一点不同 因为是not x2 我们需要把-1放在x2的边上 然后我们在x1的边上放上1 
在常数项的边上放-0.5 这就是我们对z1的表示方式了 对z2也用相似的表示方法
我们在x1的边上放上-1 就是这条x1到z2的边
我们在这条x2到z2的边上放1 然后在常数项的边上放-0.5，这样就可以来表示z2了 最后一步 如果z1和z2都存在的话
我们剩下要做的就是逻辑或运算 我们已经知道如何对布尔变量做或运算了 也就是1 1  -0.5 现在到了我们的精彩时刻 我们现在已经造出了我们的第一个深度神经网络 虽然不是超深度 仅仅只有两层 但还是有点小兴奋 我们刚造完我们的第一个神经网络 这是一个双层神经网络 总的来说 神经网络指的就是对你的数据进行多层次的变换 然后我们用这些变换后的数据来建立非线性特征集 我们会在计算机视觉领域看到一些例子 如今 神经网络已经发展了 大概50年了 和机器学习存在的时间差不多 然而大约在90年代 神经网络算法失宠了 当时的科学家发现要从神经网络中
获取较高的准确率非常困难 但是大约在十年前 一切都改变了 因为有两个事情发生了 一个是数据大量地增长了 因为神经网络有很多很多 很多很多层 你需要大量的数据 来训练每层神经网络 它们有很多很多的参数 我们会看一个超级让人兴奋的神经网络
它有6000万个参数 所以我们需要大量数据来训练它们 近几年我们从不同的渠道获得了如此高量级的数据 特别是通过网络 而第二件让神经网络成为可能的事件 是计算能力的增强 因为我们需要处理更大的神经网络和更多的数据 我们需要快速的电脑和GPU
GPU原本是设计用来 加速电脑游戏的图像运算的 结果人们发现GPU非常适合用于创建和 使用有着大量数据的神经网络 因为有了GPU 因为这些深度的神经网络 一切都改变了 对我们现实世界已经产生了许多影响 [背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community