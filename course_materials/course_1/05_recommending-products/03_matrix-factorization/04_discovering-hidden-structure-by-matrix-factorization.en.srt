1
00:00:00,037 --> 00:00:03,938
[MUSIC]

2
00:00:03,938 --> 00:00:06,228
So to this point though,

3
00:00:06,228 --> 00:00:12,210
we've assumed that we know all
these Lu user topic vectors.

4
00:00:12,210 --> 00:00:15,690
So we can stack them
up into this L matrix.

5
00:00:15,690 --> 00:00:17,810
And we know all the movie topic factors.

6
00:00:17,810 --> 00:00:21,340
So we put them altogether
in that R matrix.

7
00:00:21,340 --> 00:00:24,490
And then, we multiply the two
together to get this big prediction

8
00:00:24,490 --> 00:00:27,960
of readings that every user
would give to every movie.

9
00:00:29,140 --> 00:00:34,300
But important thing here is the fact that
we don't actually have this information.

10
00:00:34,300 --> 00:00:39,800
We don't have these features about
the users, or about the movies.

11
00:00:39,800 --> 00:00:44,080
So instead, what we're gonna do is we're
gonna flip this problem on its head.

12
00:00:44,080 --> 00:00:48,830
And we're gonna try and
estimate these matrices, this L and

13
00:00:48,830 --> 00:00:53,110
R matrix, which is equivalent to
estimating these topic vectors, or

14
00:00:53,110 --> 00:00:58,870
feature vectors, for every user and
every movie based on our observed ratings.

15
00:00:58,870 --> 00:01:03,690
So these matrices, or
these collections of topic vectors,

16
00:01:03,690 --> 00:01:05,900
are the parameters of our model.

17
00:01:05,900 --> 00:01:08,890
So going back to the regression

18
00:01:10,700 --> 00:01:15,620
module that we had, we talked about
models and their associated parameters in

19
00:01:15,620 --> 00:01:18,360
thinking about estimating
those parameters from data.

20
00:01:20,050 --> 00:01:22,640
So this is a very similar notion.

21
00:01:22,640 --> 00:01:23,720
So we have data.

22
00:01:23,720 --> 00:01:24,610
What are our data?

23
00:01:24,610 --> 00:01:28,090
In this case,
our data are our observed ratings.

24
00:01:28,090 --> 00:01:29,900
So those are the black squares.

25
00:01:31,320 --> 00:01:38,930
And our parameters are these user and
movie topic factors.

26
00:01:38,930 --> 00:01:43,370
Okay, so what we're gonna do is we're
gonna estimate each of these from

27
00:01:43,370 --> 00:01:44,180
our observe reading.

28
00:01:44,180 --> 00:01:47,740
So only using these black cells,
we're gonna try and

29
00:01:47,740 --> 00:01:53,380
estimate these L and
R vectors and resulting matrices.

30
00:01:53,380 --> 00:01:55,499
So how are we gonna do this?

31
00:01:57,060 --> 00:01:58,630
Well, let's think of a metric for

32
00:01:58,630 --> 00:02:02,300
fit just like we talked about
in the regression module.

33
00:02:02,300 --> 00:02:03,110
If you remember,

34
00:02:03,110 --> 00:02:06,860
going back to that module we talked about
something called residual sum of squares.

35
00:02:07,910 --> 00:02:09,520
There we were talking about a house.

36
00:02:09,520 --> 00:02:12,668
It had some set of features.

37
00:02:12,668 --> 00:02:15,020
And then,
[COUGH] we had weights on those features,

38
00:02:15,020 --> 00:02:17,220
those weights were our parameters.

39
00:02:17,220 --> 00:02:20,810
And we were predicting
somehow sales price.

40
00:02:20,810 --> 00:02:25,420
And then, we compared with the actual
sales price, and we looked at

41
00:02:25,420 --> 00:02:31,760
the square of that difference, and we
summed over every house in our data set.

42
00:02:31,760 --> 00:02:36,060
Well, in this case,
the parameters of our model are L and R.

43
00:02:36,060 --> 00:02:42,605
And our prediction,
our ratings hat is gonna be <Lu,

44
00:02:42,605 --> 00:02:50,870
Rv>, this notation of doing this
element wise product in summing.

45
00:02:52,190 --> 00:02:53,795
And that's our predicted rating.

46
00:03:01,283 --> 00:03:05,562
And then,
our observed rating is Rating(u,v),

47
00:03:05,562 --> 00:03:09,840
and what we're gonna do is
look at that difference,

48
00:03:09,840 --> 00:03:13,732
the difference between
our observed rating and

49
00:03:13,732 --> 00:03:20,753
what we're predicting with our parameters
Lu and Rv, and we're gonna square them.

50
00:03:20,753 --> 00:03:25,912
And we're gonna say our residual sum

51
00:03:25,912 --> 00:03:30,572
of squares of our parameters L and

52
00:03:30,572 --> 00:03:36,231
R is equal to this, and then we're gonna

53
00:03:36,231 --> 00:03:41,740
sum over all movies that have ratings.

54
00:03:41,740 --> 00:03:46,093
So we're gonna include all u,

55
00:03:46,093 --> 00:03:51,953
v pairs where [BLANK_ AUDIO] rating,

56
00:03:51,953 --> 00:03:57,143
let me use u' to v' to distinguish it

57
00:03:57,143 --> 00:04:03,520
from the specific u and
v example I gave here.

58
00:04:03,520 --> 00:04:07,540
So we're gonna add in all u' v' pairs.

59
00:04:07,540 --> 00:04:10,794
Where rating u' and

60
00:04:10,794 --> 00:04:15,202
rating v' are available.

61
00:04:18,999 --> 00:04:20,520
And where are these available?

62
00:04:20,520 --> 00:04:21,710
They are the black squares.

63
00:04:21,710 --> 00:04:24,670
Just remember this picture here.

64
00:04:24,670 --> 00:04:30,620
Okay, so what I'm doing is I'm
taking a given L matrix and

65
00:04:30,620 --> 00:04:33,760
R matrix, I'm looking at my predictions.

66
00:04:33,760 --> 00:04:41,040
So I'm looking at, and I'm gonna evaluate
how well I did on all these black squares.

67
00:04:42,330 --> 00:04:49,130
So I'm looking at how well the L and
U that I'm using fit my observed ratings.

68
00:04:51,160 --> 00:04:55,320
And then,
when I want to go to estimate L and R,

69
00:04:55,320 --> 00:05:01,250
just like when I wanted to estimate the
weights on the regression coefficients in

70
00:05:01,250 --> 00:05:06,930
that housing value prediction problem,
I search over, in this case, all

71
00:05:06,930 --> 00:05:13,010
the user topic vectors and all the movie
topic vectors, and find the combination of

72
00:05:13,010 --> 00:05:17,570
this huge space of parameters that
best fits my observed ratings.

73
00:05:19,480 --> 00:05:25,200
And so, the reason this is called a Matrix
Factorization model, so that's really key.

74
00:05:25,200 --> 00:05:30,610
This is called a matrix factorization
model, because I'm taking this matrix,

75
00:05:30,610 --> 00:05:34,150
and approximating it with
this factorization here.

76
00:05:37,060 --> 00:05:43,365
But the key thing is the output of this,
is a set of estimated parameters here.

77
00:05:43,365 --> 00:05:49,180
And, unfortunately, I've just written
over this very, very, very key point.

78
00:05:49,180 --> 00:05:50,440
So I apologize for that.

79
00:05:50,440 --> 00:05:52,720
Let's pause for a second, so
that you get the writing.

80
00:05:53,950 --> 00:05:58,750
And now, I'll just say in words this next
animation is saying that there are a lot

81
00:05:58,750 --> 00:06:03,440
of very efficient algorithms for
doing this factorization.

82
00:06:03,440 --> 00:06:09,140
And we're gonna talk about them in great
extent in the recommender systems,

83
00:06:09,140 --> 00:06:13,660
or matrix factorization course later on.

84
00:06:15,000 --> 00:06:18,070
But then, okay, so
very efficient algorithms for

85
00:06:18,070 --> 00:06:21,770
computing these estimates of L and R.

86
00:06:21,770 --> 00:06:23,960
How do I form my predictions?

87
00:06:23,960 --> 00:06:30,340
How do I fill in all these white squares,
which was my goal to start with.

88
00:06:30,340 --> 00:06:35,370
Well, I just use my estimated L hat u.

89
00:06:35,370 --> 00:06:37,520
And R hat v.

90
00:06:37,520 --> 00:06:41,500
And I form my prediction
just as we described

91
00:06:41,500 --> 00:06:43,670
when we assumed that we
actually knew these vectors.

92
00:06:45,220 --> 00:06:49,350
Okay, well, matrix factorization
is a really, really powerful tool.

93
00:06:49,350 --> 00:06:53,414
And it's been proven useful on
lots of different applications.

94
00:06:53,414 --> 00:06:58,246
But there's one limitation, and that's
a problem that we talked about a little

95
00:06:58,246 --> 00:07:02,367
bit earlier of the cold-start
problem where this model still can't

96
00:07:02,367 --> 00:07:06,120
handle this problem of what if we
get a new user or a new movie.

97
00:07:06,120 --> 00:07:11,120
So that's the case where we
have no ratings either for

98
00:07:11,120 --> 00:07:15,030
a specific user or a specific movie.

99
00:07:15,030 --> 00:07:18,480
So that might be a new movie that
arrives or a new user arrives.

100
00:07:18,480 --> 00:07:20,960
So this is a really important problem.

101
00:07:20,960 --> 00:07:24,280
And one that, for example,
Netflix faces all the time.

102
00:07:24,280 --> 00:07:28,050
How do we make predictions for
these users or movies?

103
00:07:28,050 --> 00:07:32,159
[MUSIC]