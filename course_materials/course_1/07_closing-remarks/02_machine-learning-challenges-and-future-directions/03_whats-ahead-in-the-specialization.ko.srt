1
00:00:00,000 --> 00:00:04,249
[음악]

2
00:00:04,249 --> 00:00:07,385
이제 여러분은 미래의 기계 실행에 대해 들떠 있을 것입니다.

3
00:00:07,385 --> 00:00:10,770
이제 우리가 specialization에서 배울 것들에 대해 이야기해보죠.

4
00:00:10,770 --> 00:00:14,701
regression 코스에서는 여러분은 regression이 무엇인지 알고 있습니다.

5
00:00:14,701 --> 00:00:18,397
우리는 모델들에 대한 다른 공식들에 대해 좀 더 자세하게

6
00:00:18,397 --> 00:00:19,055
알아볼 것입니다.

7
00:00:19,055 --> 00:00:21,463
예를 들어, 많은 특성들에 대해 어떻게 대처하는지에 대한 방법,

8
00:00:21,463 --> 00:00:24,370
즉 우리가 이 코스에서 배우지 않았던 것에 대해 말이죠.

9
00:00:24,370 --> 00:00:29,210
그리고 우리는 이런 모델들에 적용되는 알고리즘에 대해 매우 구체적으로

10
00:00:29,210 --> 00:00:30,090
이야기해볼 것입니다.

11
00:00:30,090 --> 00:00:33,780
따라서 서로 다른 최적화 알고리즘, 우리는

12
00:00:33,780 --> 00:00:38,000
거기에 우리가 이야기할 수 있는 코스트, 사각형들의 잔여 값의 총 합을 최소화 하는 것에 대해 기억하세요.

13
00:00:38,000 --> 00:00:40,670
우리는 gradient descent 그리고 coordinate desent와 같은 서로 다른 

14
00:00:40,670 --> 00:00:43,930
알고리즘에 대해 이야기할 것이며 이를 통해 실제로 최적화를 해볼 것입니다.

15
00:00:45,260 --> 00:00:50,200
그런 다음 이 집의 판매가를 예측하는 케이스 학습을 통해서

16
00:00:50,200 --> 00:00:53,470
많은 여러 분야에서의 기계 학습에 대한 기초에 대해

17
00:00:53,470 --> 00:00:56,210
많은 컨셉들을 배울 것입니다.

18
00:00:56,210 --> 00:00:59,300
그리고 이 중의 어떤 것들은 우리가 코스트를 측정하는 방법에 대해서 다룰 것입니다.

19
00:00:59,300 --> 00:01:02,010
우리는 모델을 선택하는 것과 우리의 모델의 오버피팅이 대해 대처하는 것에 대해

20
00:01:02,010 --> 00:01:04,670
어떻게 생각하는지에 대해 말이죠.

21
00:01:04,670 --> 00:01:08,630
우리는 이 문맥에서 이것에 대해 좀 더 탐구할 것입니다. 하지만 이런 아이디어들은

22
00:01:08,630 --> 00:01:14,360
regression과 집의 판매가를 예측하는 범위를 훨씬 넘어섭니다.

23
00:01:14,360 --> 00:01:16,290
classificaiton 코스에서 우리는

24
00:01:16,290 --> 00:01:20,350
liniear 분류법들의 특정 예제들에 대해 이야기할 것입니다.

25
00:01:20,350 --> 00:01:24,878
우리는 또한 무수히 많은 특성들을 스케일업 하는 메소드에 대해

26
00:01:24,878 --> 00:01:26,182
이야기할 것이고

27
00:01:26,182 --> 00:01:31,343
이 고차원적인 특성을 묘사하는 분류법들을 생성해볼 것입니다.

28
00:01:31,343 --> 00:01:34,611
그리고 우리는 이런 유형의 classification들의 성과를 위한

29
00:01:34,611 --> 00:01:38,150
알고리즘에 대해서도 이야기할 것입니다.

30
00:01:38,150 --> 00:01:42,120
특히, 우리가 매우 방대한 데이터 세트를 스케일업 하게 도와주는 

31
00:01:42,120 --> 00:01:44,610
최적화 알고리즘에 대해서 살펴볼 것입니다.

32
00:01:44,610 --> 00:01:47,270
그리고 우리는 또한 우리가 어떻게 

33
00:01:47,270 --> 00:01:51,050
boosting이라는 것을 사용해서 서로 다른 모델들을 융합할 수 있는지에 대해 알아볼 것입니다.

34
00:01:52,550 --> 00:01:55,620
그리고 우리는 여러가지의 다른 컨셉들에 대해서도 살펴볼 것입니다.

35
00:01:55,620 --> 00:01:59,590
제가 흥미롭다고 생각하는 한 가지는 온라인 학습이라고 불리는 것인데,

36
00:01:59,590 --> 00:02:02,830
여기서 우리는 지속적으로 데이터를 얻게 되고

37
00:02:02,830 --> 00:02:08,300
우리는 데이터를 얻음과 동시에 지속적으로 추론들을 만들어냅니다.

38
00:02:08,300 --> 00:02:12,320
그런 다음 우리가 클러스팅과 검색 등을 하게 될 때

39
00:02:12,320 --> 00:02:16,580
우리는 클러스팅 작업이 의미하는 것이 무엇인지 그리고 우리의 문서 검색 업무가 무엇인지에

40
00:02:16,580 --> 00:02:18,490
대한 기초적인 아이디어들에 대해 배웁니다.

41
00:02:18,490 --> 00:02:22,190
하지만 우리는 지금보다 더 올라가서, 예를 들어,

42
00:02:22,190 --> 00:02:27,730
우리가 클러스트링에 대해 생각할 때, 어쩌면 문서는 그저 스포트나 세계 뉴스,

43
00:02:27,730 --> 00:02:30,690
과학 또는 엔터테인먼트에 대한 것이 아닐 수 있습니다.

44
00:02:30,690 --> 00:02:34,945
어쩌면 이 문서는 여러 다른 주제들이 혼합된 것으로 되어 있을 수도 있죠.

45
00:02:34,945 --> 00:02:40,725
쉬운 예로 금융과 세계 뉴스에 대한 내용이 모두 담긴 문서를 생각해 볼 수 있습니다.

46
00:02:40,725 --> 00:02:43,595
따라서 우리는 이것을 현재의 우리 데이터에 존재할 지도 모르는 더 복잡한 구조의

47
00:02:43,595 --> 00:02:46,725
모델로 만들 수 있을 지 생각해 볼 것입니다.

48
00:02:48,155 --> 00:02:53,307
그리고 이것들의 알고리즘적인 면에서 우리는 우리가 검색 업무를 할 때

49
00:02:53,307 --> 00:02:57,991
매우 효과적으로 데이터를 검색할 수 있는 방법들에 대해 살펴볼 것입니다.

50
00:02:57,991 --> 00:02:59,929
그리고 우리가 이야기하는 클러스트링 모델 작업을 위한 여러 다른

51
00:02:59,929 --> 00:03:03,380
알고리즘에 대해서도 말이죠.

52
00:03:05,210 --> 00:03:10,170
그리고 우리가 이 코스에서 배우게 될 매우 중요한 컨셉은,

53
00:03:10,170 --> 00:03:17,020
하나는 map-reduce라는 것을 사용하는 대량의 문서들의 콜렉션들을

54
00:03:17,020 --> 00:03:22,680
클러스트링 작업을 스케일업 하는 방법에 대해 생각해 보는 것입니다.

55
00:03:22,680 --> 00:03:25,600
다음으로 우리는 Recommender System과 

56
00:03:25,600 --> 00:03:27,690
Dimensionality Reduction 코스로 되돌아갈 것입니다.

57
00:03:27,690 --> 00:03:30,049
그리고 여기에서, 우리ㅏ가 이미 이 코스에서 언급했던 collaborative filtering과 

58
00:03:30,049 --> 00:03:32,754
matrix factorization에 대해서도 살펴볼 것입니다.

59
00:03:32,754 --> 00:03:37,000
우리는 또한 고차원의 데이터 세트를 가지고

60
00:03:37,000 --> 00:03:42,090
저차원적인 묘사의 개념의 모델링에 대해서도 생각해 볼 것입니다.

61
00:03:42,090 --> 00:03:45,100
그리고 우리는 이 dimentionality reduction을 위한

62
00:03:45,100 --> 00:03:46,966
알고리즘에 대해 생각해 볼 것입니다.

63
00:03:46,966 --> 00:03:50,330
그리고 우리는 또한 우리가 이 코스에서 설멍했던 matrix factorization 모델의 유형을

64
00:03:50,330 --> 00:03:54,740
규정하기 위한 알고리즘에 대해서도 이야기해볼 것입니다.

65
00:03:57,240 --> 00:04:01,230
이 경우, 우리가 살펴볼 중요한 컨셉은

66
00:04:01,230 --> 00:04:04,700
특히 우리가 matrix factorization에 대해 생각하는 것은

67
00:04:04,700 --> 00:04:08,140
우리가 matrix completion과 같은 것을 하는 것에 대해 생각하는 것과 같다는 것이죠.

68
00:04:08,140 --> 00:04:11,970
그리고 그것이 우리가 모든 알수없는 사각형들을 채우는 장소입니다.

69
00:04:11,970 --> 00:04:13,800
여러분이 만약 이 코스에서 그것을 기억한다면 말이죠.

70
00:04:13,800 --> 00:04:18,360
그리고 더 일반적인, 이 cold-start와 같은 문제는

71
00:04:18,360 --> 00:04:22,540
우리의 recommender system과 같은 경우인데, 이것은 사용자 또는 제품에 대한

72
00:04:22,540 --> 00:04:24,500
정보가 없을 수도 있고 이러한 추천목록을 만들기 원합니다.

73
00:04:25,570 --> 00:04:29,370
그리고 마침내, 우리는 Capstone에 다다를 것입니다.

74
00:04:29,370 --> 00:04:33,260
저는 여려분이 여기에 대한 센스를 가지기를 바라며, 이것은 매우 멋집니다.

75
00:04:33,260 --> 00:04:36,540
이제 여러분은 이 코스를 살펴보았습니다. 여러분은 이 Capstone을 만드는 용어,

76
00:04:36,540 --> 00:04:40,620
우리가 이야기해왔던 컨셉에 대해 이해하고 있을 것입니다.

77
00:04:40,620 --> 00:04:44,300
특히, 우리는 각기 다른 이미지들을 검색하는 의미의 컴퓨터 버전의 중요한 아이디어를 이용해

78
00:04:44,300 --> 00:04:49,070
텍스트 감성 분석을 하기 위한 아이디어들을 통합하는

79
00:04:49,070 --> 00:04:54,280
추천 시스템에 대해서 살펴볼 것입니다.

80
00:04:54,280 --> 00:04:57,140
그리고 이것들을 하기 위해 우리는 딥러닝을 사용할 것입니다. 따라서

81
00:04:57,140 --> 00:05:00,250
거기에는 Capstone에 표현된 딥러닝에 관해 매우 중요하고 좀 더 구체적인

82
00:05:00,250 --> 00:05:03,420
정보들이 있을 것입니다.

83
00:05:03,420 --> 00:05:05,090
그러니 그 포인트에 다다를 수 있도록 하세요.

84
00:05:05,090 --> 00:05:09,300
이것은 매우 멋질 것이며 매우 중요합니다. 그리고 이 모든 것들은

85
00:05:09,300 --> 00:05:14,150
여러분이 지능적인 웹 응용 프로그램을 만들 수 있게 해주고

86
00:05:14,150 --> 00:05:18,080
모두를 놀라게 할 일을 할 수 있게 해줄 것입니다.

87
00:05:18,080 --> 00:05:22,830
단순히 여러분의 친구들이나 가족이 아닌 잠재적인 고용주들까지 말이죠.

88
00:05:22,830 --> 00:05:27,000
물론, 그것은 잠재적인 보너스일 수 있습니다.

89
00:05:27,000 --> 00:05:30,710
하지만 이것은 정말 재미있을 것입니다.

90
00:05:30,710 --> 00:05:33,936
우리는 여러분이 그 포인트를 이해했기를 바라고 Capstone 프로젝트를 즐기기를 바랍니다.

91
00:05:33,936 --> 00:05:38,939
[음악]