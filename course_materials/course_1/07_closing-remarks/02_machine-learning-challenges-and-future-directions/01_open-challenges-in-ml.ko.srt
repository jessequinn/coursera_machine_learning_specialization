1
00:00:00,015 --> 00:00:04,377
[음악]

2
00:00:04,377 --> 00:00:05,576
이 코스에서는

3
00:00:05,576 --> 00:00:08,685
우리는 다양한 기계학습 메소드와 이런 유형들의 메소드를 임팩트 있게 사용하는

4
00:00:08,685 --> 00:00:12,660
많은 응용 프로그램들에 대해서 이야기했습니다.

5
00:00:12,660 --> 00:00:15,560
하지만 물론, 기계학습에는 여전히

6
00:00:15,560 --> 00:00:16,640
남아있는 과제들이 있죠.

7
00:00:16,640 --> 00:00:17,710
그것들의 일부에 대해 한번 논의해보죠.

8
00:00:18,740 --> 00:00:23,080
그 중 하나는 우리가 종종 어떤 모델을 사용할 것인가에 대한 선택을 해야 한다는 것입니다.

9
00:00:23,080 --> 00:00:24,400
예를 들어,

10
00:00:24,400 --> 00:00:28,370
우리가 제품 추천에 대해 이야기했을 때, 우리는 classification 모델을 사용할 수 있다고 말합니다.

11
00:00:28,370 --> 00:00:32,740
우리가 사용자와 제품의 특성들을 가지고 이것을 이 분류법에 패스시키면

12
00:00:32,740 --> 00:00:36,990
이것은 이 사람이 이 제품을 좋아할 것인지 싫어할 것인지에 대해 네 또는 아니오로 대답합니다.

13
00:00:36,990 --> 00:00:41,020
하지만 우리는 또한 우리가 사용자들과 제품들의 특성들에 대해 배우고

14
00:00:41,020 --> 00:00:46,380
그것을 사용해서 사용자들에게 제품을 추천하는 matrix factorization에 대해 이야기했습니다.

15
00:00:46,380 --> 00:00:50,170
그리고 우리는 또한 featurized matrix factorization을 통해, 이 두 개의

16
00:00:50,170 --> 00:00:57,580
아이디어들, 가능한 모델들의 흔한 리스트를 우리는 업무가 매우 큰 것을 고려할 수 있습니다. 

17
00:00:57,580 --> 00:01:01,360
따라서 일반적으로 이것은 전문직 종사자들을 당혹스럽게 만들었습니다.

18
00:01:01,360 --> 00:01:04,710
어떤 모델을 사용할 것이고 가능한 선택들에 대한 검색은 여전히

19
00:01:04,710 --> 00:01:08,889
기계학습에서 남아있는 과제입니다.

20
00:01:10,610 --> 00:01:13,980
우리가 직면하게 되는 또 다른 중요한 과제는 

21
00:01:13,980 --> 00:01:15,900
우리의 데이터를 어떻게 표현할 것인가 입니다.

22
00:01:15,900 --> 00:01:18,760
예를 들어, 우리가 우리의 문서 모델링에 대해 이야기했을 때,

23
00:01:18,760 --> 00:01:24,230
우리의 문서 검색 업무에서 우리는 raw word counts를 사용할 수 있다고 하거나

24
00:01:24,230 --> 00:01:28,300
벡터를 일반화할 수 있다고 이야기했죠.

25
00:01:28,300 --> 00:01:32,390
우리는 tf-idf와 같은 것을 사용해서 

26
00:01:32,390 --> 00:01:37,200
매우 인기있는 단어들을 골라내거나, 문서 내의 중요한 단어들을 강조할 수 있었습니다.

27
00:01:38,430 --> 00:01:41,580
하지만 솔직히 말해서, 거기에는 많은 다른 종류의 tf-idf가 있고,

28
00:01:41,580 --> 00:01:44,900
우리는 그 중 하나를 이것을 하기 위한 예시로 제공했습니다.

29
00:01:44,900 --> 00:01:49,220
여러분은 또한 BiTrams와 trigrams를 사용하는 것에 대해 생각할 수 있고, 거기에는

30
00:01:49,220 --> 00:01:53,190
우리가 문서에 등장하는 단어들을 표현할 수 있는 다양한 방법들이 있습니다.

31
00:01:53,190 --> 00:01:58,170
그것은 우리의 관심의 데이터 세트로, 우리는 이것을 표현하길 원하죠.

32
00:01:59,380 --> 00:02:01,150
하지만 그것은 그저 문서일 뿐입니다.

33
00:02:01,150 --> 00:02:03,770
그러면 우리는 어쩌면 이미지들을 가지고 있습니다.

34
00:02:03,770 --> 00:02:05,100
우리는 이미지를 어떻게 표현해야 할까요?

35
00:02:05,100 --> 00:02:06,290
우리는 이미 몇 가지 방법들에 대해 이야기했죠.

36
00:02:06,290 --> 00:02:10,640
우리는 다른 것들에 대해 이야기할 것이지만, 거기에는 많은 어려움이 있습니다.

37
00:02:10,640 --> 00:02:15,310
여러분이 네트워크에 기반하는 데이터를 가졌다면,

38
00:02:15,310 --> 00:02:17,820
페이스북과 같은 것 말이죠.

39
00:02:17,820 --> 00:02:22,440
따라서 여러분은 매우 복잡한 데이터 구조를 가질 수 있고,

40
00:02:22,440 --> 00:02:24,462
동시에 매우 다른 데이터 세트를 가질 수 있습니다.

41
00:02:24,462 --> 00:02:27,450
우리는 우리가 설명했던 유형들의 메소드를 사용할 수 있게 되기를 바랍니다.

42
00:02:28,610 --> 00:02:31,350
그럼 어떻게 우리의 데이터를 표현할 것인가, 물론 이것은

43
00:02:31,350 --> 00:02:36,010
우리가 데이터에 만드는 추론들의 유형에 대해 중요한 영향을 줄 수 있습니다.

44
00:02:36,010 --> 00:02:40,750
이것은 매우 중요한 문제이고 거기에는 여러분의 데이터에 대한

45
00:02:40,750 --> 00:02:42,690
올바른 표현법을 선택할 수 있게 해주는 메소드는 없습니다.

46
00:02:45,690 --> 00:02:49,900
오늘날 우리가 기계학습에서 마주하는 매우 중요한 과제 중 하나는

47
00:02:49,900 --> 00:02:54,660
multiple dimensions들을 어떻게 스케일업 할 것이냐 입니다.

48
00:02:54,660 --> 00:02:59,170
이것의 한 측면은 데이터는 점점 더 커지고 있다는 점입니다.

49
00:02:59,170 --> 00:03:03,440
이것은 미디어에서 광범위하게 거론되고 있는 부분이죠.

50
00:03:03,440 --> 00:03:07,170
그럼 우리가 점점 거대해지는 데이터에 의해 직면하게 되는

51
00:03:07,170 --> 00:03:09,400
일부 상황들에 대해 이야기해보죠.

52
00:03:09,400 --> 00:03:13,830
한 가지 사실은 거기에는 매우 다양한 플랫폼들이 존재하고,

53
00:03:13,830 --> 00:03:18,970
소셜 네트워킹과 같이, 그리고 이것은 크라우드소싱을 통해 데이터를 수집하고 있습니다.

54
00:03:18,970 --> 00:03:22,849
여러분의 사진들이나 영상들을 공유하게 되는 것과 같이 말이죠.

55
00:03:24,120 --> 00:03:29,640
그리고 레스토랑들을 리뷰하거나 여러분이 온라인에서 갈 수 있는 방법의 리스트 그리고

56
00:03:29,640 --> 00:03:35,420
계속해서 자라고 있는 세상에 데이터를 던져줍니다.

57
00:03:35,420 --> 00:03:36,870
그리고 이런 것들을 하는 사람들 그리고 이를 통헤 제공되는 데이터는

58
00:03:36,870 --> 00:03:39,500
점점 더 거대해지고 있습니다.

59
00:03:39,500 --> 00:03:43,159
우리는 우리에게 허용된 새로운 데이터 소스들을 매우 많이 가지고 있죠.

60
00:03:43,159 --> 00:03:48,562
게다가, 물건들을 구매하는 것에 대해 우리가 생각하는 방식은, 우리가 물건을 구매할 때,

61
00:03:48,562 --> 00:03:53,903
이제는 더 이상 상점으로 가서 그것들에 대한 것을 손으로 적는 것이 아닙니다.

62
00:03:53,903 --> 00:03:58,926
이제 우리는 아마존과 같은 거대한 온라인 시장을 통해

63
00:03:58,926 --> 00:04:04,050
각기 다른 제품들에 대한 정보를 수집하고

64
00:04:04,050 --> 00:04:08,974
각기 다른 방법으로 구매가 이루어지며,

65
00:04:08,974 --> 00:04:13,630
다양하고 방대한 데이터 소스가 교류합니다.

66
00:04:13,630 --> 00:04:18,110
그리고 이런 유형의 웹사이트들 외에도 거기에는

67
00:04:18,110 --> 00:04:20,270
우리가 작용할 수 있는 다양한 디바이스들이 있죠.

68
00:04:20,270 --> 00:04:22,840
따라서 거기에는 제가 착용할 수 있는 웨어러블 디바이스들,

69
00:04:22,840 --> 00:04:27,630
제가 하고 모든 행동들을 모니터하고 제가 밤에 어떻게 자는지 등을 모니터하는 시계가 있습니다.

70
00:04:27,630 --> 00:04:30,800
저는 제가 보는 모든 것들을 녹화하는 안경을 착용할 수 있죠.

71
00:04:32,590 --> 00:04:36,160
저는 또한 다양한 디바이스들이 연결되어

72
00:04:36,160 --> 00:04:39,960
다양한 정보들의 소스를 공유함으로써 서로 소통하는

73
00:04:39,960 --> 00:04:44,010
사물인터넷에 대해서도 이야기 할 수 있습니다.
다양한 정보들의 소스를 공유함으로써 서로 소통하는

74
00:04:44,010 --> 00:04:47,850
이것들은 우리가 자주 보고 다양한 새로운 데이터 소스들을 가지지만,

75
00:04:47,850 --> 00:04:52,420
물론 철저하게 완벽한 단계는 아닙니다.

76
00:04:52,420 --> 00:04:56,410
우리는 또한 병력과 같은 것에 대해서도 말할 수 있습니다.

77
00:04:56,410 --> 00:04:58,840
여러분은 이제 더 이상 의사의 사무실을 찾아가서

78
00:04:58,840 --> 00:05:02,740
그들이 손으로 노트를 적고 그것들을 파일에 넣게 할 필요가 없죠.

79
00:05:02,740 --> 00:05:05,240
종종 그들은 전자건강기록을 사용하고,

80
00:05:05,240 --> 00:05:08,400
이것들은 이제 시스템상에서 소통하며 우리는

81
00:05:08,400 --> 00:05:13,460
데이터 소스가 분석되고 이해된 후 약품조제로

82
00:05:13,460 --> 00:05:19,110
연결되는 것을 자주 보게 됩니다.

83
00:05:19,110 --> 00:05:22,100
방대한 새로운 데이터 세트는 매우 신이납니다..

84
00:05:22,100 --> 00:05:26,504
우리는 사람들이 어떻게 그들의 몸을 가동하고, 어떻게 구매하고,

85
00:05:26,504 --> 00:05:31,042
친구들을 만들고, 하루 하루 어떤 행동들을 하는지에 대해 배우죠. 하지만 물론

86
00:05:31,042 --> 00:05:35,514
이런 타입의 데이터들을 분석하는 메소드가 필요하고,

87
00:05:35,514 --> 00:05:39,985
또한 현재의 데이터 세트에 존재하는 고유한 데이터 구조도 필요합니다.

88
00:05:39,985 --> 00:05:44,090
그리고 noisy한 구조, 그리고 과제 리스트들은 아주 광범위하죠.

89
00:05:46,070 --> 00:05:48,790
이것은 기계학습에서 매우 큰 과제 중 하나인데,

90
00:05:48,790 --> 00:05:50,230
이 방대한 데이터에 대해 어떻게 대처하느냐 입니다.

91
00:05:51,450 --> 00:05:54,775
그리고 데이터가 방대해짐과 동시에,

92
00:05:54,775 --> 00:05:59,337
우리는 또한 우리가 이것들을 분석하는데 사용하는 모델들이

93
00:05:59,337 --> 00:06:03,539
점점 더 복잡한 데이터 세트를 분석해야 하는 과제에 직면하고 있습니다.

94
00:06:03,539 --> 00:06:08,429
따라서 모델들 역시 점점 방대해지고 있고

95
00:06:08,429 --> 00:06:14,115
이것들로부터 정보를 추출하기 위해 점점 복잡해지고 있습니다.

96
00:06:14,115 --> 00:06:15,915
저는 그것이 단어인지는 모르겠지만, 여러분은 제 포인트를 알겁니다.

97
00:06:15,915 --> 00:06:20,131
이런 배우 복잡한 데이터 소스들은 매우 방대한 데이터 소스들이죠.

98
00:06:20,131 --> 00:06:25,478
예를 들어, 우리가 클러스트링에 대해 이야기 했을 때,

99
00:06:25,478 --> 00:06:27,700
우리는 이것에 대해 이야기 했었죠, 응용 프로그램,

100
00:06:27,700 --> 00:06:32,554
뇌의 활동을 기록하는 것, 이것은 그저 한 파트입니다.

101
00:06:32,554 --> 00:06:37,571
이것은 이 유형의 데이터를 분석하기 위한 모델의 한 예이며,

102
00:06:37,571 --> 00:06:42,549
이 슬라이드에서 보여주는 구체적인 내용들은 포함하지 않습니다.

103
00:06:42,549 --> 00:06:46,133
그저 이것들은 많은 원들 그리고 화살들이라고 생각하세요.

104
00:06:46,133 --> 00:06:49,850
그리고 그것은 이것이 매우 복잡하고 큰 모델이라는 것을 의미합니다.

105
00:06:51,100 --> 00:06:55,380
여러분은 어쩌면, 데이터는 점점 커지고, 모델들도 점점 커지지만

106
00:06:55,380 --> 00:06:58,680
프로세서도 점점 빨라지기 때문에 괜찮다고 생각할 수 있습니다.

107
00:07:00,280 --> 00:07:02,740
그것은 조금 지난 이야기입니다.

108
00:07:02,740 --> 00:07:08,430
우리는 한동안 이런 프로세스들의 속도가

109
00:07:08,430 --> 00:07:10,550
기하급수적으로 증가해온 것을 보았습니다.

110
00:07:10,550 --> 00:07:12,920
하지만 그것은 약 10여년전 멈추었죠.

111
00:07:12,920 --> 00:07:14,650
그리고 이제 우리는

112
00:07:14,650 --> 00:07:20,520
개별의 프로세서들의 속도가 미미하게 증가하는 것을 보고 있습니다.

113
00:07:20,520 --> 00:07:24,350
따라서 대신에 우리는 스케일업을 위한 새로운 방법을 생각해야 합니다.

114
00:07:25,610 --> 00:07:28,670
그리고 우리가 오늘날 레버리징 하는 일반적인 것은 

115
00:07:28,670 --> 00:07:30,430
프로세서들의 콜렉션입니다.

116
00:07:30,430 --> 00:07:34,770
거기에는 다른 아키텍쳐가 있습니다.

117
00:07:34,770 --> 00:07:40,740
우리는 GPU, multicore, 클러스터 그리고 클라우드 컴퓨팅 리소스 등과 같은

118
00:07:40,740 --> 00:07:46,180
것들을 가졌고, 매우 세련되고 비싼 슈퍼 컴퓨터들을 가졌습니다.

119
00:07:46,180 --> 00:07:47,420
멋진 일이죠.

120
00:07:47,420 --> 00:07:49,070
이것들 매우 강력하거나

121
00:07:49,070 --> 00:07:52,920
잠재적으로 강력한 컴퓨팅 리소스를 가지고 있죠.

122
00:07:52,920 --> 00:07:55,530
하지만 여기서 질문은 우리가 이것을 기계학습에서 어떻게 사용할 수 있는가 입니다.

123
00:07:55,530 --> 00:07:58,325
그리고 기계학습에서 우리는 직면하고 있는 과제들이 있습니다.

124
00:07:58,325 --> 00:08:03,840
하나는 여기에서 기계학습을 빼고

125
00:08:03,840 --> 00:08:07,320
이것들을 어떻게 각기 다른 프로세서에 분배해서

126
00:08:07,320 --> 00:08:12,000
우리가 원하는 방향으로 모든 것들을 실행할 지 이며, 이것은 매우 어려운 일이죠.

127
00:08:12,000 --> 00:08:16,990
또 다른 과제는 우리가 어떻게 각기 다른 기계들에 데이터를 분배하고

128
00:08:16,990 --> 00:08:20,660
각각의 기계에서 우리가 가질 수 있는 실패 가능성을 줄이면서

129
00:08:20,660 --> 00:08:25,120
이런 작업들을 할 것인가 입니다.

130
00:08:26,760 --> 00:08:31,320
따라서 이것들은 우리가 기계학습에서 직면하고 있는 문제의 숫자를 표현합니다.

131
00:08:31,320 --> 00:08:35,590
 그리고 수 많은 신나는 리서치들이 이런 문제들을 해결하기 위해

132
00:08:35,590 --> 00:08:36,277
생겨나고 있습니다.

133
00:08:36,277 --> 00:08:40,149
[음악]