1
00:00:00,046 --> 00:00:03,485
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community

2
00:00:03,485 --> 00:00:06,110
分类器的确是用来做决策的

3
00:00:06,110 --> 00:00:09,360
这些决策包括 一个句子是积极的还是消极的

4
00:00:09,360 --> 00:00:13,621
是否一系列的实验测试

5
00:00:13,621 --> 00:00:18,060
把X光加到治疗当中 会导致一个像流感或者伤寒这样的疾病

6
00:00:18,060 --> 00:00:20,330
这就是一个我们需要做出的决策

7
00:00:20,330 --> 00:00:24,987
那么我们来讨论一下分类器是怎样通过特定的线性分类器来

8
00:00:24,987 --> 00:00:26,025
做决策的

9
00:00:26,025 --> 00:00:28,276
要理解决策边界

10
00:00:28,276 --> 00:00:32,000
假设你只有两个权值不为0的单词

11
00:00:32,000 --> 00:00:35,230
一个单词是“awesome” 权值为+1

12
00:00:35,230 --> 00:00:40,230
另一个单词是“awful” 权值为 -1.5

13
00:00:40,230 --> 00:00:44,450
在这种情况下，评分就由1乘以这个句子中“awesome"的数量

14
00:00:44,450 --> 00:00:47,930
减去1.5乘以“awful”的数量

15
00:00:49,260 --> 00:00:53,046
你可以画出坐标轴，这是“awesome”坐标轴

16
00:00:53,046 --> 00:00:55,260
这个是“awful”坐标轴

17
00:00:55,260 --> 00:00:59,200
例如 这个句子 寿司是非常棒的

18
00:00:59,200 --> 00:01:01,980
食物非常棒 但是服务很糟糕

19
00:01:03,100 --> 00:01:05,980
句子中有两个“awesome”和一个“awful”

20
00:01:05,980 --> 00:01:09,740
因此我们在图上画出 (2,1) 点

21
00:01:09,740 --> 00:01:14,410
同样的，如果有个句子含有3个“awful”和1个“awesome”

22
00:01:14,410 --> 00:01:19,240
如果都是“awesome” 3个“awesome”就画在 (3,0) 点

23
00:01:19,240 --> 00:01:22,950
其他的句子也是这样

24
00:01:22,950 --> 00:01:27,757
现在 让我们了解一下我们是怎样对句子进行评分的

25
00:01:27,757 --> 00:01:29,720
以及怎么推断结论

26
00:01:30,800 --> 00:01:37,130
例如，(3,0) 点代表的是有3个“awesome”，而没有“awful”出现

27
00:01:37,130 --> 00:01:40,180
3个“awesome”给你一个积极的预测

28
00:01:40,180 --> 00:01:43,950
因为这个评分是大于0的

29
00:01:43,950 --> 00:01:48,815
落在坐标右下侧的所有点都是符合这个规律的

30
00:01:48,815 --> 00:01:53,460
而落在坐标左上侧的点因为评分小于0

31
00:01:53,460 --> 00:01:58,840
例如这个有3个“awful” 1个“awesome”的点的评分小于0

32
00:01:58,840 --> 00:02:00,250
因此我们标记这些点是负类

33
00:02:00,250 --> 00:02:06,045
事实上 把正类和负类分开的

34
00:02:06,045 --> 00:02:10,978
是当我们不知道这个是正类还是负类的点的集合组成的一条连线

35
00:02:10,978 --> 00:02:17,320
这条线就是 “1.0 * #awesome - 1.5 * #awful = 0” 所表示的线

36
00:02:17,320 --> 00:02:21,690
这条线就是我们无法做出正类和负类判别决策的线

37
00:02:21,690 --> 00:02:24,280
我们把这条线叫做决策边界

38
00:02:24,280 --> 00:02:26,060
在其中一侧的所有点都判定为正类

39
00:02:26,060 --> 00:02:28,420
在另一侧的所有点都判定为负类

40
00:02:28,420 --> 00:02:30,963
这个例子中的决策边界

41
00:02:30,963 --> 00:02:35,310
“1.0 * #awesome - 1.5 * #awful = 0” 是一条直线

42
00:02:35,310 --> 00:02:38,230
这也是我们叫它线性分类器的原因

43
00:02:38,230 --> 00:02:40,430
这是一个线性决策边界

44
00:02:40,430 --> 00:02:42,960
决策边界将正类

45
00:02:42,960 --> 00:02:45,170
和负类的预测分开

46
00:02:45,170 --> 00:02:49,730
在这个只有两种特征的例子中 我把决策边界看作只是一条直线

47
00:02:49,730 --> 00:02:53,570
如果我们增加特征的数目 情况就会不同了

48
00:02:53,570 --> 00:02:58,050
在二维平面中 线性方程是一条直线

49
00:02:58,050 --> 00:03:03,120
在三维平面中 例如 我们有三个权值不为0的单词

50
00:03:03,120 --> 00:03:05,685
除了这三个单词为其他的权值为0 我们会得到一个平面

51
00:03:05,685 --> 00:03:10,680
画一个3D的图形有一点麻烦

52
00:03:10,680 --> 00:03:15,790
正类的预测平面的上面 而负类的预测

53
00:03:15,790 --> 00:03:20,500
在平面的下方，这个平面是与空间呈一定倾角的

54
00:03:21,660 --> 00:03:25,365
如果权值不为0的单词数目不只是3个

55
00:03:25,365 --> 00:03:30,390
可能有成百上千个单词的权值不为0

56
00:03:30,390 --> 00:03:32,980
在这种情况下 我们叫这个决策边界为超平面

57
00:03:32,980 --> 00:03:37,870
在非常高的纬度正负类的分离器 叫做超平面

58
00:03:37,870 --> 00:03:41,200
当然 这种情况下 你就不能使用线性分类器了

59
00:03:41,200 --> 00:03:43,200
你可以使用更加复杂的分类器

60
00:03:43,200 --> 00:03:44,870
在那些分类器之中 决策边界不是简单的直线

61
00:03:44,870 --> 00:03:48,900
而是超平面 他们有更复杂的形状 或者弯弯曲曲的形状

62
00:03:48,900 --> 00:03:55,013
我们在分类的课程中 会学到更多相关知识

63
00:03:55,013 --> 00:03:57,419
[背景音乐] 
翻译: dalongraymond | 审阅: 19waa
Coursera Global Translate Community