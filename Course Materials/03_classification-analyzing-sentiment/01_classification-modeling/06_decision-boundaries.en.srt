1
00:00:00,046 --> 00:00:03,485
[MUSIC]

2
00:00:03,485 --> 00:00:06,110
So classifiers are really
trying to make decisions.

3
00:00:06,110 --> 00:00:09,360
Decisions as to whether a sentence
is positive or negative, or

4
00:00:09,360 --> 00:00:13,621
whether a set of lab tests plus x-rays

5
00:00:13,621 --> 00:00:18,060
plus measurements lead to a certain
disease like flu or cold.

6
00:00:18,060 --> 00:00:20,330
That's a decision that needs to be made.

7
00:00:20,330 --> 00:00:24,987
So let's talk a little bit about how
classifiers especially linear classifiers

8
00:00:24,987 --> 00:00:26,025
make decisions.

9
00:00:26,025 --> 00:00:28,276
To understand decision boundaries,

10
00:00:28,276 --> 00:00:32,000
suppose you only had two
words with non-zero weight.

11
00:00:32,000 --> 00:00:35,230
You have awesome with
positive weight of one and

12
00:00:35,230 --> 00:00:40,230
awful which is just awful so
it has a negative weight of 1.5.

13
00:00:40,230 --> 00:00:44,450
If you have this situation then the score
is gonna be 1 times the number of

14
00:00:44,450 --> 00:00:47,930
awesomes in the sentence,
minus 1.5 times the number of awfuls.

15
00:00:49,260 --> 00:00:53,046
So we can plot this on an axis,
there's the awesome axis and

16
00:00:53,046 --> 00:00:55,260
then there's the awful axis.

17
00:00:55,260 --> 00:00:59,200
So for example, the sentence,
the sushi was awesome,

18
00:00:59,200 --> 00:01:01,980
the food was awesome, but
the service was awful.

19
00:01:03,100 --> 00:01:05,980
Then that has two awesomes and one awful.

20
00:01:05,980 --> 00:01:09,740
It's plotted in the point
(2,1) on the axis.

21
00:01:09,740 --> 00:01:14,410
And similarly for something that has say,
three awfuls and one awesome,

22
00:01:14,410 --> 00:01:19,240
and for something that is all awesome,
so three awesomes is at a point (3,0),

23
00:01:19,240 --> 00:01:22,950
and so on for other sentences.

24
00:01:22,950 --> 00:01:27,757
Now, let's understand a little better
how we scored the sentences and

25
00:01:27,757 --> 00:01:29,720
what does that imply about our decisions.

26
00:01:30,800 --> 00:01:37,130
So, for example, take the point
(3,0) as 3 awesomes, no awfuls.

27
00:01:37,130 --> 00:01:40,180
Three awesomes gives you a positive

28
00:01:40,180 --> 00:01:43,950
prediction because the score
is greater than zero.

29
00:01:43,950 --> 00:01:48,815
And that is true for every point really
on the bottom right of the axis.

30
00:01:48,815 --> 00:01:53,460
While the points on the top left all
have score less than zero so for

31
00:01:53,460 --> 00:01:58,840
example the point three awfuls,
one awesome got score less than zero.

32
00:01:58,840 --> 00:02:00,250
So, those get labeled negative.

33
00:02:00,250 --> 00:02:06,045
And, in fact, what separates the negative
predictions from the positive predictions

34
00:02:06,045 --> 00:02:10,978
is the line that defines the places
where I don't know what's positive and

35
00:02:10,978 --> 00:02:17,320
what's negative, and that's the line
where 1.0 #awesome- 1.5 #awful = 0.

36
00:02:17,320 --> 00:02:21,690
And that's the line when, I don't know,
the prediction is uncertain, and

37
00:02:21,690 --> 00:02:24,280
so we call that the decision boundary.

38
00:02:24,280 --> 00:02:26,060
Everything on one side
we predict is positive,

39
00:02:26,060 --> 00:02:28,420
everything on the other
we predict is negative.

40
00:02:28,420 --> 00:02:30,963
Now notice that the decision boundary,

41
00:02:30,963 --> 00:02:35,310
1.0 times awesome minus 1.5 times
awful equals to 0 is a line.

42
00:02:35,310 --> 00:02:38,230
And so that's why it's
called a linear classifier.

43
00:02:38,230 --> 00:02:40,430
It's a linear decision boundary.

44
00:02:40,430 --> 00:02:42,960
So decision boundaries
are what separate the positive

45
00:02:42,960 --> 00:02:45,170
predictions from the negative predictions.

46
00:02:45,170 --> 00:02:49,730
So, in the case of just two features,
we see this is just a line.

47
00:02:49,730 --> 00:02:53,570
But that situation will differ as
we increase the number of features.

48
00:02:53,570 --> 00:02:58,050
So in two dimensions,
a linear function is a line.

49
00:02:58,050 --> 00:03:03,120
In three dimensions, we have three, words
for example that have non-zero weight and

50
00:03:03,120 --> 00:03:05,685
everything else has zero weight and
we get to the plane.

51
00:03:05,685 --> 00:03:10,680
And so
it's a little hard to draw in 3D, but

52
00:03:10,680 --> 00:03:15,790
the positive predictions here are above
the plane and the negative predictions

53
00:03:15,790 --> 00:03:20,500
here are below the plane and the plane
is somehow inclined in the space.

54
00:03:21,660 --> 00:03:25,365
Now when you have not just
three non-zero words but in

55
00:03:25,365 --> 00:03:30,390
re-application you're gonna have tens of
thousands of words with non-zero weight.

56
00:03:30,390 --> 00:03:32,980
And in that case we'll
call those hyperplanes,

57
00:03:32,980 --> 00:03:37,870
really high dimensional
separators called hyperplanes.

58
00:03:37,870 --> 00:03:41,200
Now, of course you can use
more than linear classifiers.

59
00:03:41,200 --> 00:03:43,200
You can use more complex classifiers.

60
00:03:43,200 --> 00:03:44,870
And those, instead of having lines or

61
00:03:44,870 --> 00:03:48,900
hyperplanes, they have more complicated
shapes or squiggly separations.

62
00:03:48,900 --> 00:03:55,013
And we're gonna learn more about
that in the classification course.

63
00:03:55,013 --> 00:03:57,419
>> [MUSIC]