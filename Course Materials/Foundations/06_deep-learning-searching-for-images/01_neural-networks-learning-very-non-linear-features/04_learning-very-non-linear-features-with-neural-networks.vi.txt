[NHẠC] Vâng chúng ta đã nói về ứng dụng của việc tìm những đôi giày đẹp hoặc váy chỉ dựa trên các đặc điểm hình ảnh. Kỹ thuật mà chúng ta sẽ sử dụng hôm nay được gọi là nghiên cứu sâu. Cụ thể nó dựa trên một cái gọi là mạng thần kinh. Nhưng trước khi chúng ta hiểu điều này hãy nói về sự đại diện dữ liệu. Chúng ta đã thảo luận những thứ như là TFIDF và các mô hình từ, nhưng bạn biểu thị dữ liệu
 như thế nào khi mà nó là hình ảnh? Điều đó được gọi là các đặc trưng và là phần quan trọng của học máy. Do đó khi mà bạn nói về học máy, chúng ta sẽ đưa ra một số đầu vào. Chúng ta đang thực hiện phân loại, chúng ta đã nói về phân tích cảm tính. Bạn đưa ra một câu nó sẽ đi qua mô hình phân loại và chúng ta quyết định câu đó là tích cực hay tiêu cực. Trong phân loại hình ảnh, mục đích là để đi từ hình ảnh, đây là đầu vào của tôi, các điểm ảnh. Trong trường hợp này, đây là con chó của tôi
 và tôi muốn phân loại nó là chó săn Labrador khác với các loài khác. Chúng ta đã thảo luận, các đặc điểm là đại diện của dữ liệu mà sử dụng để đưa vào bộ phân loại. Có rất nhiều các đại diện, ví dụ văn bản chúng ta đã nói về các từ và TFIDF. Với những hình ảnh có rất nhiều các đại diện khác. Chúng ta sẽ thảo luận một vài điều trong bài học này. Nhưng hôm nay chúng ta sẽ tập trung vào mạng thần kinh, cái mà cung cấp sự đại diện phi tuyến cho dữ liệu. Bây giờ hãy quay lại với phân loại. Hãy cùng nhau xem lại. Chúng ta thảo luận về các bộ phân loại
 tuyến tính cái mà tạo ra đường này hoặc ranh giới quyết định tuyến tính giữa lớp tích cực và lớp tiêu cực. Và ranh giới được biểu thị bằng điểm w0+ w1 x1 x2 và vân vân. Một mặt , về mặt tích cực điểm lớn hơn 0 và về mặt tiêu cực thì điểm nhỏ hơn 0. Nếu tôi có chức năng điểm số tốt này, tôi có thể phân chia các kết quả tích cực từ các kết quả tiêu cực. Trong mạng noron chúng ta sẽ biểu thị các bộ phân loại sử dụng graph. Và ở đây chúng ta có một nút cho mỗi đặc trưng x1, x2, tất cả cách tới đặc trưng dth, xd và một nút cho đầu ra y, cái mà chúng ta dự đoán. Đặc điểm đầu tiên x1 được nhân với trọng số w1, vì thế tôi sẽ đặt trọng số đó bên cạnh. X2 được nhân với trọng số w2, tôi sẽ đặt nó bên cạnh, tới xd được nhân với wd đặt cuối cùng. Và trọng số cuối cùng w0 không được nhân với các đặc trưng nhưng được nhân với 1, vì thế chúng ta đặt ở trên đầu. Và nếu bạn giả sử việc nhân w0 qua wd với x1 qua xd và hệ số 1, bạn sẽ có được điểm. Khi điểm lớn hơn 0, chúng ta nói đầu ra là 1 và khi điểm nhỏ hơn 0 chúng ta nói đầu ra là 0. Đây là một ví dụ nhỏ, một mạng nhỏ. Chúng ta mô tả bộ phân loại tuyến tính nhỏ như một mạng thần kinh, như một mạng thần kinh một lớp. Điều này có thể biểu thị gì? Hãy dùng chức năng x1 hoặc x2. Chúng ta có thể biểu thị bằng việc sử dụng
 mạng thân kinh nhỏ này phải không? Hãy định nghĩa chức năng một chút. chúng ta có biến x1, x2 và đầu ra y. Có một số khả năng. X1 có thể bằng 0, x2 có thể bằng 0 và khi nó là x1 hoặc x2 thì đầu ra y sẽ là 0. Khi x1 bằng 1 và x2 bằng 0 thì đầu ra bằng 1. Khi x1 bằng 0, x2 bằng 1 thì đầu ra bằng 1, tương tự khi cả hai bằng 1 thì đầu ra bằng 1. Chúng ta muốn xác định điểm để giá trị lớn hơn 0 cho ba hàng cuối, nhưng nó nhỏ hơn 0 cho hàng đầu tiên. Vậy làm cách nào để chúng ta làm điều đó? Ví dụ có nhiều cách làm nhưng nếu tôi đặt trọng số 1. Khi mỗi một cạnh x1 và x2, và chúng ta nghĩ đến điểm, điểm của hàng đầu tiên bằng 0 và điểm của các hàng khác lớn hơn 0. Chúng ta muốn thêm sự phân tách, vì thế chúng ta có thể đặt giá trị âm lên cạnh đầu tiên. -0.5 và hãy xem điều gì xảy ra với điểm. Khi x1 bằng 0 và x2 bằng 0 thì điểm là -0.5 và tôi có điểm nhỏ hơn 0, điểm của tôi đúng, đầu ra của tôi là đúng. Khi x1 bằng 1 và x2 bằng 0, tôi có điểm là 0.5. Tương tự khi x1 bằng 0 và x2 bằng 1. Cuối cùng khi cả hai bằng 1 tôi có điểm là 1.5. Với các trọng số đơn giản trên các cạnh, tôi biểu thị x1 hoặc x2. Bây giờ chúng ta có thể biểu thị x1 hoặc x2 chứ? Tương tự chúng ta có thể đặt trọng số 1 trên cạnh x1 và x2 nhưng trong trường hợp này chúng ta chỉ muốn
 chuyển đổi khi cả x1 và x2 cùng bằng 1. Thay vì đặt -0.5 trên đỉnh, chúng ta đặt -1.5. Nếu bạn điền vào bảng như chúng ta đã làm với ví dụ đầu tiên, bạn sẽ thấy rằng chúng ta biểu thị x1 và x2 sử dụng mạng thần kinh đơn giản. Một mạng thân kinh một lớp về cơ bản giống với bộ phân loại tuyến tính cơ bản mà chúng ta đã học trong khóa học này. Một bộ phân loại tuyến tính đơn giản không biểu thị điều gì? Chúng ta đã nói nó có thể biểu thị x1 hoặc x2. Nó có thể đại diện x1 và x2 nhưng chức năng là gì, một chức năng đơn giản không thể biểu thị là gì? Vâng đây là một ví dụ. Không có phân cách cộng và trừ trong ví dụ. Chức năng này được gọi là XOR. Tôi thích gọi nó là bộ đếm. Bất cứ khi nào bạn tìm ví dụ bộ đếm, thứ đầu tiên phải thử là XOR. Trong trường hợp này các đặc trưng tuyến tính chúng ta đã mô tả không đủ và chúng ta cần một số các đặc trưng
phi tuyến tính, và đây là khi bạn dùng mạng và chúng ta sẽ thấy một ví dụ về điều đó. Hãy cùng xem lại XOR là gì. XOR có một giá trị khi X1 là đúng và X2 là sai vì thế không phải x2 hoặc không phải x1, x1 là sai hoặc 0 và x2 có giá trị 1. Chúng ta có thể đại diện điều này với mạng nơ ron như thế nào? Hãy gọi thuật ngữ đầu tiên là z1, thuật ngữ thứ 2 là z2. Cái mà chúng ta sẽ làm là xây dựng mạng nơ ron để đai diện không trực tiếp đầu vào x1 và x2 để dự đoán y, nhưng không dự đoán các giá trị trung gian giữa z1 và z2, sau đó những điều này sẽ dự đoán y. Hãy lấy z1. Chúng ta đại diện mạng nơ ron như thế nào, chỉ một mạng nơ ron mà có thể dự đoán z1. Chúng ta đã thảo luận một chút nhưng ở đây có. Nó hơi khác một chút so với các trường hợp trước đây. Bởi vì khi chúng ta phủ nhận, chúng ta nói x2, chúng ta đặt -1 vào cạnh và chúng ta đặt +1 trên x1 và -0,5 vào đó. Bây giờ chúng ta có sự đại diện cho z1. Tương tự với z2, chúng ta đặt kí hiệu trừ trên cạnh x1, cạnh này ở đây, x1 tới z2, chúng ta đặt +1 trên cạnh x2 tới z2 và -0,5 tiếp theo, bây giờ nó đại diện z2. Và bước cuối cùng, nếu z1 tồn tại, tất cả chúng ta phải làm là or chúng. Và chúng ta đã biết cách để or các biến. Nó chỉ là 1, -0,5. Và đây là một khoảng thời gian tuyệt với cho chúng ta. Bây giờ chúng ta đã xây dựng mạng nơ ron sâu đầu tiên, không phải siêu sâu, có hai tầng nhưng thú vị. Chúng ta chỉ xây dựng mạng nơ ron đầu tiên của chúng ta. Nó có hai tầng mạng nơ ron nhưng nói chung có các tầng chuyển đổi dữ liệu của chúng ta. Và chúng ta sử dụng các bộ chuyển đổi để tạo ra các tính năng phi tuyến tính và chúng ta sẽ thấy một số ví dụ về điều đó trong thao tác máy tính. Bây giờ mạng nơ ron đã có khoảng 50 năm trước, gần lâu bằng học máy. Tuy nhiên chúng không được ưu chuộng những năm 90 bởi vì sự cổ điển có một thời gian khó khăn để có độ chính xác cao trong các mạng nơ ron. Nhưng mọi thứ thay đổi khoảng 10 năm trước, bởi vì hai thứ sau. Đầu tiên nó là nhiều dữ liệu bởi vì mạng nơ ron có quá nhiều, nhiều, nhiều tầng. Nhiều tầng bạn cần nhiều dữ liệu có thể huấn luyện tất cả các tầng. Chúng có nhiều thông số. Chúng ta sẽ xem mạng nơ ron thú vị với 60 triệu thông số. Vì thế chúng ta cần nhiều dữ liệu để huấn luyện chúng. Gần đây chúng ta đã đưa ra nhiều dữ liệu từ các nguồn khác nhau đặc biệt là web. Điều thứ hai thay đổi lớn mà tạo ra mạng nơ ron sâu là sự cải tiến trong các nguồn tính. Bởi vì chúng ta phải xử lí các mạng nơ ron lớn hơn và nhiều dữ liệu hơn, chúng ta cần các máy tính nhanh hơn và GPU được thiết kế cho việc tăng tốc đồ họa cho các trò chơi. Hóa ra là công cụ chính xác để xây dựng và sử dụng mạng nơ ron với nhiều dữ liệu. Bởi vì GPU và bởi vì mạng nơ ron sâu, mọi thứ đã thay đổi. Bây giờ chúng ta có nhiều tác động trong thế giới. [NHẠC]