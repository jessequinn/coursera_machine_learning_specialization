1
00:00:00,037 --> 00:00:03,938
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community

2
00:00:03,938 --> 00:00:06,228
虽然这一点

3
00:00:06,228 --> 00:00:12,210
是在假设已经知道所有Lu用户主题向量的基础上计算的

4
00:00:12,210 --> 00:00:15,690
这样就可以将它们堆叠成L矩阵

5
00:00:15,690 --> 00:00:17,810
同样我们也知道所有的电影的主题因子

6
00:00:17,810 --> 00:00:21,340
可以把它们放在一起形成R矩阵

7
00:00:21,340 --> 00:00:24,490
然后 将两个矩阵相乘 就能得到 关于每个用户对每部电影的评价假设的大矩阵

8
00:00:24,490 --> 00:00:27,960
然后 将两个矩阵相乘 就能得到 关于每个用户对每部电影的评价假设的大矩阵

9
00:00:29,140 --> 00:00:34,300
但重要的是  事实上我们并不知道这些信息

10
00:00:34,300 --> 00:00:39,800
我们并不知道关于这些用户们 或关于电影们的特征是什么

11
00:00:39,800 --> 00:00:44,080
相反的 我们将要做的是逆向思考这个问题

12
00:00:44,080 --> 00:00:48,830
接下来 需要尝试估计这些矩阵 L和R矩阵的内容

13
00:00:48,830 --> 00:00:53,110
意思就是 需要在基于观测到的评价的基础上

14
00:00:53,110 --> 00:00:58,870
 为每个用户和每部电影估计这些主题向量 或者说 特征向量

15
00:00:58,870 --> 00:01:03,690
而这些矩阵 或者说关于主题向量的这些集合 就是我们模型的参数

16
00:01:03,690 --> 00:01:05,900
而这些矩阵 或者说关于主题向量的这些集合 就是我们模型的参数

17
00:01:05,900 --> 00:01:08,890
现在回到我们之前讲过的 回归 模块上

18
00:01:10,700 --> 00:01:15,620
即 对于已知模型和与其相关的参数

19
00:01:15,620 --> 00:01:18,360
如何有效地从数据中估计这些参数

20
00:01:20,050 --> 00:01:22,640
这里我们打算做的东西是相似的

21
00:01:22,640 --> 00:01:23,720
我们有数据

22
00:01:23,720 --> 00:01:24,610
数据是什么？

23
00:01:24,610 --> 00:01:28,090
这里 数据就是指我们观察到的评价

24
00:01:28,090 --> 00:01:29,900
就是那些黑色的方块们

25
00:01:31,320 --> 00:01:38,930
而参数就是指这些用户和电影的主题因子

26
00:01:38,930 --> 00:01:43,370
好的 接下来要做的就是 我们将从观察到的评价来估计每一个参数

27
00:01:43,370 --> 00:01:44,180
好的 接下来要做的就是 我们将从观察到的评价来估计每一个参数

28
00:01:44,180 --> 00:01:47,740
如果要估计L和R向量以及结果矩阵的话 可以使用信息只有这些黑格子

29
00:01:47,740 --> 00:01:53,380
如果要估计L和R向量以及结果矩阵的话 可以使用信息只有这些黑格子

30
00:01:53,380 --> 00:01:55,499
那将要如何做呢？

31
00:01:57,060 --> 00:01:58,630
好的 考虑下 求一个矩阵的合适值 是不是

32
00:01:58,630 --> 00:02:02,300
就像之前说过的回归模块那样

33
00:02:02,300 --> 00:02:03,110
如果你还记得

34
00:02:03,110 --> 00:02:06,860
在回归模块中 我们讨论过 残差平方和

35
00:02:07,910 --> 00:02:09,520
我们那时候讨论的是一个屋子

36
00:02:09,520 --> 00:02:12,668
它有一系列特征

37
00:02:12,668 --> 00:02:15,020
然后 [咳嗽声] 那些特征具有对应的权值

38
00:02:15,020 --> 00:02:17,220
而那些权值就是我们的参数

39
00:02:17,220 --> 00:02:20,810
这样我们就能够预测一些东西的销售价格

40
00:02:20,810 --> 00:02:25,420
将预测的价格和实际销售价格进行比较

41
00:02:25,420 --> 00:02:31,760
就能够求它们差的平方 然后将数据库中每个屋子对应的值相加

42
00:02:31,760 --> 00:02:36,060
好的 现在 模型的参数变成 L 和 R

43
00:02:36,060 --> 00:02:42,605
在我们的预测中 带帽的Rating将表示为 <Lu, 

44
00:02:42,605 --> 00:02:50,870
Rv> 这个符号表示元素按位相乘然后求和

45
00:02:52,190 --> 00:02:53,795
那就是我们预测出的评价

46
00:03:01,283 --> 00:03:05,562
然后 观察到的评价表示为Rating(u,v) 看看它们之间有何的不同

47
00:03:05,562 --> 00:03:09,840
然后 观察到的评价表示为Rating(u,v) 看看它们之间有何的不同

48
00:03:09,840 --> 00:03:13,732
求观察到评价和根据参数Lu以及Rv做出的预测评价间的差 然后将它们做平方操作

49
00:03:13,732 --> 00:03:20,753
求观察到评价和根据参数Lu以及Rv做出的预测评价间的差 然后将它们做平方操作

50
00:03:20,753 --> 00:03:25,912
接下来讨论的是 参数L和参数R的残差平方和

51
00:03:25,912 --> 00:03:30,572
接下来讨论的是 参数L和参数R的残差平方和

52
00:03:30,572 --> 00:03:36,231
并且将会对所有已经有评价的电影求和

53
00:03:36,231 --> 00:03:41,740
并且将会对所有已经有评价的电影求和

54
00:03:41,740 --> 00:03:46,093
下面将对所有的已经做出评价的 配对的u,v 对进行操作

55
00:03:46,093 --> 00:03:51,953
下面将对所有的已经做出评价的 u,v对进行操作

56
00:03:51,953 --> 00:03:57,143
这里用u'和v'来区别表示

57
00:03:57,143 --> 00:04:03,520
这里 u 和 v 表示的是给出的某个特例

58
00:04:03,520 --> 00:04:07,540
我们将添加所有的u'v'对

59
00:04:07,540 --> 00:04:10,794
这里加入的u‘的评价和v’的评价都是有效地

60
00:04:10,794 --> 00:04:15,202
这里加入的u‘的评价和v’的评价都是有效地

61
00:04:18,999 --> 00:04:20,520
这些有效评价在哪里？

62
00:04:20,520 --> 00:04:21,710
就是这些黑色的方格

63
00:04:21,710 --> 00:04:24,670
只需要记住这张图片

64
00:04:24,670 --> 00:04:30,620
好的 接下来就可以用给定的L矩阵和R矩阵 来观察预测值

65
00:04:30,620 --> 00:04:33,760
好的 接下来就可以用给定的L矩阵和R矩阵 来观察预测值

66
00:04:33,760 --> 00:04:41,040
我们将观察并评估 之前对这些黑色格子所做的的预测结果如何

67
00:04:42,330 --> 00:04:49,130
看看我们用来填充观察到的评价矩阵的L和U的效果如何

68
00:04:51,160 --> 00:04:55,320
那么 现在评估L和R 就像住宅价值预测问题中 评估回归系数的权值一样

69
00:04:55,320 --> 00:05:01,250
那么 现在评估L和R 就像住宅价值预测问题中 评估回归系数的权值一样

70
00:05:01,250 --> 00:05:06,930
在这个例子中 我们检索了所有的用户主题向量 所有的电影主题向量 以及

71
00:05:06,930 --> 00:05:13,010
在这个例子中 我们检索了所有的用户主题向量 所有的电影主题向量 以及

72
00:05:13,010 --> 00:05:17,570
然后找到一个最适合我们观察到的评价的巨大空间的参数组合

73
00:05:19,480 --> 00:05:25,200
这里所用的推理方式称之为矩阵因子分解模型 这个很关键

74
00:05:25,200 --> 00:05:30,610
之所以把它称之为矩阵因子分解模型 是因为

75
00:05:30,610 --> 00:05:34,150
这里将通过把这个矩阵因式分解的方式来逼近它自身

76
00:05:37,060 --> 00:05:43,365
关键是这个输出  一个包含被评估参数的集合

77
00:05:43,365 --> 00:05:49,180
遗憾的是 我才提到这个非常非常非常关键的点

78
00:05:49,180 --> 00:05:50,440
对此我表示非常抱歉

79
00:05:50,440 --> 00:05:52,720
让我们停一会儿 这样你可以记下来

80
00:05:53,950 --> 00:05:58,750
我接下来要讲的就是下面这个动画所表达的

81
00:05:58,750 --> 00:06:03,440
现在已经有很多用于因式分解问题的有效算法

82
00:06:03,440 --> 00:06:09,140
我们将在以后的推荐系统课程 或者矩阵因式分解课程中 详细探讨这个问题

83
00:06:09,140 --> 00:06:13,660
我们将在以后的推荐系统课程 或者矩阵因式分解课程中 详细探讨这个问题

84
00:06:15,000 --> 00:06:18,070
不过 现在 在已有了有效计算L和R估计值的算法基础上

85
00:06:18,070 --> 00:06:21,770
不过 现在 在已有了有效计算L和R估计值的算法基础上

86
00:06:21,770 --> 00:06:23,960
如何去生成预测值？

87
00:06:23,960 --> 00:06:30,340
如何去填充这些白色的格子 这是我们最初的目标

88
00:06:30,340 --> 00:06:35,370
好的  我们有评估得出的带帽Lu和带帽Rv

89
00:06:35,370 --> 00:06:37,520
好的  我们有评估得出的带帽Lu和带帽Rv

90
00:06:37,520 --> 00:06:41,500
我们可以得到预测值  方法就和之前所说的 

91
00:06:41,500 --> 00:06:43,670
假设知道这些向量确切值的基础上来预测的方法一样 

92
00:06:45,220 --> 00:06:49,350
好的 矩阵因式分解真的是一个非常强大的工具

93
00:06:49,350 --> 00:06:53,414
在许多不同的应用中 它都已经被证明是有用的

94
00:06:53,414 --> 00:06:58,246
但这边存在一个限制 是关于我们早前讨论过的冷启动问题 

95
00:06:58,246 --> 00:07:02,367
这个模型依然无法解决 当有一个新的用户或新的电影时 该怎么办 这个问题

96
00:07:02,367 --> 00:07:06,120
这个模型依然无法解决 当有一个新的用户或新的电影时 该怎么办 这个问题

97
00:07:06,120 --> 00:07:11,120
当出现对某个特定用户或特定电影无法作出评价的情况

98
00:07:11,120 --> 00:07:15,030
当出现对某个特定用户或特定电影无法作出评价的情况

99
00:07:15,030 --> 00:07:18,480
就可能表示 一部新的电影或者一个新的用户 出现。

100
00:07:18,480 --> 00:07:20,960
这真的是一个很重要的问题

101
00:07:20,960 --> 00:07:24,280
比如 Netflix(在线影片租赁提供商) 就会一直碰到

102
00:07:24,280 --> 00:07:28,050
我们如何能够对这些用户或电影作出预测？

103
00:07:28,050 --> 00:07:32,159
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community