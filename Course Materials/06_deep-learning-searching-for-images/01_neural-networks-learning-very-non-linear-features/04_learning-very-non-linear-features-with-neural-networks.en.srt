1
00:00:00,000 --> 00:00:04,692
[MUSIC]

2
00:00:04,692 --> 00:00:07,416
Great.
We talked about an application of finding

3
00:00:07,416 --> 00:00:10,850
cool shoes or
dresses just based on image features.

4
00:00:10,850 --> 00:00:14,080
The technique that we're gonna use
today is called deep learning.

5
00:00:14,080 --> 00:00:17,510
And in particular it's based on
something called neural networks.

6
00:00:17,510 --> 00:00:21,260
But before we get there,
let's talk about data representation.

7
00:00:21,260 --> 00:00:22,806
We discussed things like TFIDF and

8
00:00:22,806 --> 00:00:27,650
bag of word models, but how do you really
represent data when it comes to images?

9
00:00:29,090 --> 00:00:31,910
These are called features, and
is a key part of machine learning.

10
00:00:33,050 --> 00:00:37,150
So typically, when you talk about machine
learning, we're given some input.

11
00:00:37,150 --> 00:00:40,200
Let's say we're doing classification,
we talked about sentimental analysis.

12
00:00:40,200 --> 00:00:43,160
You're giving a sentence,
it goes through a classifier model and

13
00:00:43,160 --> 00:00:46,780
we decided that sentence has positive or
negative sentiment.

14
00:00:46,780 --> 00:00:49,300
In image classification,
the goal is to go from an image,

15
00:00:49,300 --> 00:00:52,990
this is my input, the pixels of
the images, to a classification.

16
00:00:52,990 --> 00:00:55,920
In this case, this is my dog and
I want to classify it as

17
00:00:55,920 --> 00:00:59,660
a Labrador retriever as opposed
to other kinds of dogs.

18
00:00:59,660 --> 00:01:03,070
So as we discussed, features
are the representation of our data

19
00:01:03,070 --> 00:01:05,000
that's use to feed into the classifier.

20
00:01:06,060 --> 00:01:08,060
There are many representations,
so for example text,

21
00:01:08,060 --> 00:01:11,050
we talked about bag of words and TFIDF.

22
00:01:11,050 --> 00:01:13,400
With images there's a lot
of other representations.

23
00:01:13,400 --> 00:01:15,270
And we'll discuss a few more
of those in this module.

24
00:01:16,330 --> 00:01:18,900
But today we're really gonna
focus on neural networks,

25
00:01:18,900 --> 00:01:21,820
which provide the non-linear
representation for the data.

26
00:01:23,195 --> 00:01:25,080
Now,let's go back to classification.

27
00:01:25,080 --> 00:01:26,850
Let's do a little review.

28
00:01:26,850 --> 00:01:30,710
We discuss linear classifiers
which create this line or

29
00:01:30,710 --> 00:01:35,630
linear decision boundary between say the
positive class and and the negative class.

30
00:01:35,630 --> 00:01:39,921
And the boundary is stated by the score,
w0 +

31
00:01:39,921 --> 00:01:45,730
w1 x the first feature,
x1 x2 x the second feature and so on.

32
00:01:45,730 --> 00:01:50,160
On one side, on the positive side,
the score is greater than zero and

33
00:01:50,160 --> 00:01:52,690
on the negative side
the score is less than zero.

34
00:01:52,690 --> 00:01:54,920
So, if I have this nice score function,

35
00:01:54,920 --> 00:01:58,180
I can separate the positives
from the negatives.

36
00:01:58,180 --> 00:02:02,410
In neural networks we're going to
represent such classifiers using graphs.

37
00:02:03,530 --> 00:02:08,070
And so, here we have a node for
each feature x1, x2, all the way

38
00:02:08,070 --> 00:02:13,730
to the dth feature, xd, and a node for the
output y, which we are trying to predict.

39
00:02:13,730 --> 00:02:16,950
So, the first feature x1 is
multiplied by the weight w1, so

40
00:02:16,950 --> 00:02:18,700
I'm putting that weight on the edge.

41
00:02:18,700 --> 00:02:21,870
X2 is multiplied the second weight,
w2 I'm gonna put it on that edge,

42
00:02:21,870 --> 00:02:26,850
all the way to xd, which is multiplied
the weight wd, to put in the last edge.

43
00:02:26,850 --> 00:02:30,970
And the last weight, w0 doesn't get
multiplied by any feature, but it gets

44
00:02:30,970 --> 00:02:34,900
multiplied by the number one, so we put
a little one at the top multiplying w0.

45
00:02:34,900 --> 00:02:40,340
And so, if you imagine multiplying
the weights, w0 through wd

46
00:02:40,340 --> 00:02:44,850
with the features x1 through xd and
the coefficient one, you get the score.

47
00:02:44,850 --> 00:02:48,400
And when the score is greater than zero,
we declared the output to be one and

48
00:02:48,400 --> 00:02:51,550
when it scores less than zero,
we declare the output to be zero.

49
00:02:51,550 --> 00:02:56,650
This is an example of a small,
one layer, neural network.

50
00:02:56,650 --> 00:03:01,470
So, we describe the small linear
classifiers as a neural network,

51
00:03:01,470 --> 00:03:02,940
as a one layer neural network.

52
00:03:02,940 --> 00:03:05,770
What can this one layer
neural network represent?

53
00:03:05,770 --> 00:03:10,620
Let's take the function x1 or x2.

54
00:03:10,620 --> 00:03:14,240
Can we represent that using
a small neural network like this?

55
00:03:14,240 --> 00:03:17,140
Well, let's define the function
a little bit more formally.

56
00:03:17,140 --> 00:03:22,891
So, what we have is variables x1,

57
00:03:22,891 --> 00:03:26,910
x2, and the output y.

58
00:03:26,910 --> 00:03:28,680
And there's some possibilities.

59
00:03:28,680 --> 00:03:32,998
X1 can be zero, x2 can be zero,
and since it's x1 or x2,

60
00:03:32,998 --> 00:03:36,340
the output y would be zero in this case.

61
00:03:36,340 --> 00:03:40,272
When x1 is one, and
x2 is zero, the output is one.

62
00:03:40,272 --> 00:03:43,360
When x is zero, when x2 is one,
the output is one,

63
00:03:43,360 --> 00:03:47,300
and similarly, when they're both one,
the output is 1.

64
00:03:47,300 --> 00:03:54,080
So, we want to define the score
function such that the value is greater

65
00:03:54,080 --> 00:03:59,890
than zero for the last three rows, but
it is less than zero for the first row.

66
00:03:59,890 --> 00:04:01,390
How do we do that?

67
00:04:01,390 --> 00:04:03,870
So for example, there are many
ways of doing it actually, but

68
00:04:03,870 --> 00:04:06,020
if I put a weight of one.

69
00:04:06,020 --> 00:04:11,620
When each one of the edges x1 and x2,
and we think about the score, the score

70
00:04:11,620 --> 00:04:16,830
of the first row is zero and the score
of the other rows are greater than zero.

71
00:04:16,830 --> 00:04:18,890
So, we wanna add a little
bit of separation, so

72
00:04:18,890 --> 00:04:21,190
we might put a negative
value on the first edge.

73
00:04:21,190 --> 00:04:26,200
Let's say -0.5 and so
let's see what happens to the score.

74
00:04:26,200 --> 00:04:33,366
When x1 is zero and x2 is zero,
then the score becomes -0.5 and

75
00:04:33,366 --> 00:04:38,460
yey I have something less than zero and
my score is correct, my output is correct.

76
00:04:38,460 --> 00:04:44,610
When x1 is one and x2 is zero,
I get the score of 0.5.

77
00:04:44,610 --> 00:04:48,220
Similarly, when x1 is zero and x2 is one.

78
00:04:48,220 --> 00:04:52,680
And finally when they're both one,
I get a score of 1.5.

79
00:04:52,680 --> 00:04:58,300
So, with this simple weights on the edges,
I represent the function x1 or x2.

80
00:04:59,470 --> 00:05:02,280
Now, can we represent the function x1 and
x2?

81
00:05:04,040 --> 00:05:09,560
Well, similarly we can put weights one and
one on the edges x1 and

82
00:05:09,560 --> 00:05:16,810
x2 but in this case, we only want to turn
it on when both x1 and x2 have value one.

83
00:05:16,810 --> 00:05:21,860
So, instead of putting -0.5 on
that top edge, we put -1.5.

84
00:05:21,860 --> 00:05:27,830
And, if you fill out the table just
like we did with the first example,

85
00:05:27,830 --> 00:05:31,320
you'll notice that we now
represented the function X1 and

86
00:05:31,320 --> 00:05:34,190
X2 using a simple neural network.

87
00:05:34,190 --> 00:05:38,180
So, a one layer neural network is
basically the same as the standard

88
00:05:38,180 --> 00:05:43,350
linear classifiers we've been
learning about in this course so far.

89
00:05:43,350 --> 00:05:46,280
So what can linear
classifiers not represent,

90
00:05:46,280 --> 00:05:49,200
we said it can represent x1 or x2.

91
00:05:49,200 --> 00:05:52,020
It can represent x1 and
x2 but what's a function,

92
00:05:52,020 --> 00:05:53,700
a very simple function
it cannot represent?

93
00:05:54,710 --> 00:05:56,350
Well, here's an example.

94
00:05:57,350 --> 00:06:00,760
There is no line that separates
the plusses and minus in this example.

95
00:06:00,760 --> 00:06:02,730
This function's called the XOR.

96
00:06:02,730 --> 00:06:05,830
I like to call it the counter
example to everything.

97
00:06:05,830 --> 00:06:07,560
So, whenever you're trying
to find the counter example,

98
00:06:07,560 --> 00:06:10,220
the first thing to try is XOR.

99
00:06:10,220 --> 00:06:14,940
Now, for this case, the linear features
we described are not enough and

100
00:06:14,940 --> 00:06:18,380
we need some kind of non-linear features,
and this is when

101
00:06:18,380 --> 00:06:22,110
you're networks come to play for
reals and we'll see an example of that.

102
00:06:22,110 --> 00:06:25,040
So let's review what XOR is.

103
00:06:25,040 --> 00:06:30,680
So, XOR has value one when
either X1 is true and

104
00:06:30,680 --> 00:06:35,300
x2 is false so not x2 or not x1, so

105
00:06:35,300 --> 00:06:39,000
x1 is false or zero and x2 has value 1.

106
00:06:39,000 --> 00:06:43,350
So, how can we represent
this with a neural network?

107
00:06:43,350 --> 00:06:47,650
Well, let's call this first term z1 and
the second term z2.

108
00:06:49,500 --> 00:06:52,310
What we're gonna do is build
a neural network to represent

109
00:06:52,310 --> 00:06:56,512
not directly the inputs x1 and
x2 to predict y, but

110
00:06:56,512 --> 00:07:01,660
don't predicts intermediate values z1 and
z2, and then those are going to predict y.

111
00:07:03,040 --> 00:07:03,920
So let's take z1.

112
00:07:03,920 --> 00:07:09,280
How do we represent a neural network,
only a neural network that can predict z1.

113
00:07:10,560 --> 00:07:14,100
We discussed that a little bit,
but here goes.

114
00:07:14,100 --> 00:07:17,820
It's a little bit different
than the end case from before.

115
00:07:17,820 --> 00:07:23,480
Since, we have to negate, we say not x2,
we put a minus 1 on that edge and

116
00:07:23,480 --> 00:07:26,690
we put a plus 1 on x1 and
a minus 0.5 in there.

117
00:07:26,690 --> 00:07:28,510
We now have our representation for z1.

118
00:07:30,050 --> 00:07:35,663
Similarly for z2,
we put a minus sign on the edge x1,

119
00:07:35,663 --> 00:07:42,617
this edge here, x1 to z2,
we put a plus one on the edge x2 to z2 and

120
00:07:42,617 --> 00:07:49,230
then -0.5 on the constant edge,
and now it represents z2.

121
00:07:50,430 --> 00:07:55,450
And the last step, if Z1 is it to exist,
all we have to do is or them.

122
00:07:55,450 --> 00:07:59,810
And we already know how to or
to bullien variables.

123
00:07:59,810 --> 00:08:03,360
It's just one, 1- 0.5.

124
00:08:03,360 --> 00:08:08,280
And this is an amazing time for us.

125
00:08:08,280 --> 00:08:13,160
We've now built out first
deep neural network,

126
00:08:13,160 --> 00:08:17,430
not super deep, has two layers but
is a little exciting.

127
00:08:19,370 --> 00:08:21,360
We just built our first neural network.

128
00:08:21,360 --> 00:08:24,880
It was a two layer neural network,
but in general neural networks about,

129
00:08:24,880 --> 00:08:28,050
there's layers and
layers of transformations of your data.

130
00:08:28,050 --> 00:08:30,820
And we use these transformations to
create these non linear features, and

131
00:08:30,820 --> 00:08:34,200
we'll see some examples of
that in computer vision.

132
00:08:34,200 --> 00:08:36,100
Now, neural networks have been around for

133
00:08:36,100 --> 00:08:40,200
about 50 years, about as long as
machine learning's been around.

134
00:08:40,200 --> 00:08:44,030
However, they fell is disfavor
around the 90's because

135
00:08:44,030 --> 00:08:46,890
folks are having a hard time getting
good accuracy in neural networks.

136
00:08:46,890 --> 00:08:49,740
But everything changed
about ten years ago,

137
00:08:49,740 --> 00:08:51,910
because of two things that came about.

138
00:08:51,910 --> 00:08:56,650
First, it was a lot more data and because
neural networks have so many, many,

139
00:08:56,650 --> 00:08:58,150
many more layers.

140
00:08:58,150 --> 00:09:01,830
So, many layers you need a lot of data
to be able to train all those layers.

141
00:09:01,830 --> 00:09:02,840
They have a lot of parameters.

142
00:09:04,030 --> 00:09:07,050
We'll see a really exciting neural
network of 60 million parameters.

143
00:09:07,050 --> 00:09:09,370
So, we need a lot of data to train them.

144
00:09:09,370 --> 00:09:12,310
So, we recently have come about lots and
lots and

145
00:09:12,310 --> 00:09:14,830
lots of data from a variety of sources,
especially the web.

146
00:09:15,860 --> 00:09:20,840
Now, the second really great change
that made deep neural networks possible

147
00:09:20,840 --> 00:09:22,940
is advances in computing resources.

148
00:09:22,940 --> 00:09:25,760
Because we have to deal with bigger
neural networks and more data,

149
00:09:25,760 --> 00:09:30,550
we need fast computers and
GPUs which were originally designed for

150
00:09:30,550 --> 00:09:33,070
accelerating graphics for computer games.

151
00:09:33,070 --> 00:09:37,150
Turned out to be exactly
the right tool to build and

152
00:09:37,150 --> 00:09:39,920
use neural networks with lots of data.

153
00:09:39,920 --> 00:09:41,570
So because of GPUs and

154
00:09:41,570 --> 00:09:44,800
because of these deep neural networks,
everything's changed.

155
00:09:44,800 --> 00:09:48,338
And now we've had a lot of
impact in the real world.

156
00:09:48,338 --> 00:09:52,359
[MUSIC]