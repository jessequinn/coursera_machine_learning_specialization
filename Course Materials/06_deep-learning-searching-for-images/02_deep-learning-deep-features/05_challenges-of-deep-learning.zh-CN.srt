1
00:00:00,000 --> 00:00:03,885
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community

2
00:00:03,885 --> 00:00:07,900
现在 神经网络提供了一些非常振奋人心的结果

3
00:00:07,900 --> 00:00:10,062
但是它们也确实面临着很多挑战

4
00:00:10,062 --> 00:00:14,670
好的一面是它们让我们可以真正的表征非线性的

5
00:00:14,670 --> 00:00:17,963
复杂的特征并且具有令人印象深刻的好的性能

6
00:00:17,963 --> 00:00:22,881
不仅仅是在计算机视觉领域
在其他领域比如语音识别也同样有效

7
00:00:22,881 --> 00:00:27,038
所以像Siri和其他一些系统

8
00:00:27,038 --> 00:00:32,270
都使用神经网络
另外一些文本分析任务也用了到神经网络

9
00:00:32,270 --> 00:00:35,240
神经网络有潜力在很广泛的领域造成重要影响

10
00:00:37,330 --> 00:00:39,020
但也面临一些挑战

11
00:00:39,020 --> 00:00:40,974
为了更好的理解这些挑战

12
00:00:40,974 --> 00:00:44,283
我们需要介绍训练神经网络的工作流

13
00:00:44,283 --> 00:00:47,627
在一开始我们需要大量的数据

14
00:00:47,627 --> 00:00:49,205
并且这些数据都得是标注数据

15
00:00:49,205 --> 00:00:54,310
每个图片必须指明是包含哪一种狗
是拉布拉多?

16
00:00:54,310 --> 00:00:59,495
是狮子狗? 是金毛还是吉娃娃?

17
00:00:59,495 --> 00:01:04,295
但是这需要大量的人工标注
可能非常困难

18
00:01:04,295 --> 00:01:06,165
然后我们开始
输入一些图片

19
00:01:06,165 --> 00:01:10,756
把它们划分为训练和测试数据集

20
00:01:10,756 --> 00:01:14,891
然后我们训练一个深度神经网络
可能得花一些时间

21
00:01:14,891 --> 00:01:19,612
但是验证过后 我们意识到复杂的八层结构

22
00:01:19,612 --> 00:01:23,302
包含6,000万参数的模型并不是我们想要的

23
00:01:23,302 --> 00:01:28,189
我们需要修改一下模型
或者调整一下参数 亦或者改变一下学习方法

24
00:01:28,189 --> 00:01:31,068
然后我们需要不停的迭代这个过程

25
00:01:31,068 --> 00:01:35,949
事实上 为了得到那个能取胜的神经网络

26
00:01:35,949 --> 00:01:41,859
他们必须联接很多代表不同特征的层

27
00:01:41,859 --> 00:01:46,498
和很多复杂的细节
所以说其实非常困难

28
00:01:46,498 --> 00:01:49,830
所以尽管神经网络有一些非常明显的优点

29
00:01:49,830 --> 00:01:51,900
它们也有缺点

30
00:01:51,900 --> 00:01:55,703
它们需要大量的数据来获得良好的性能
它们计算量大

31
00:01:55,703 --> 00:01:59,008
尽管可以使用GPU加速
但是计算量仍然偏大

32
00:01:59,008 --> 00:02:03,058
而且它们很难进行调优
你有太多的选择

33
00:02:03,058 --> 00:02:06,777
你用的层数越多 
关于层数和参数多少的选择就越复杂

34
00:02:06,777 --> 00:02:10,777
所以如果你仓促的把过多的选择和计算开销
搅在一起

35
00:02:10,777 --> 00:02:15,249
你会发现很难搞清楚

36
00:02:15,249 --> 00:02:18,005
到底哪个神经网络比较适用

37
00:02:18,005 --> 00:02:18,505
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community