1
00:00:00,185 --> 00:00:04,747
[음악]

2
00:00:04,747 --> 00:00:08,230
딥 신경망이 훌륭하다는 것도 알았고

3
00:00:08,230 --> 00:00:12,097
높은 정확도를 가졌다는 것도,
만들고 학습시키기 어렵다는 것도

4
00:00:12,097 --> 00:00:14,640
데이터가 많이 필요하다는 것도 알았습니다

5
00:00:14,640 --> 00:00:17,510
이제 아주 흥미로운 이야기를
하려고 합니다

6
00:00:17,510 --> 00:00:21,240
딥 특징이라는 건데 데이터가 많이 없어도

7
00:00:21,240 --> 00:00:23,690
신경망을 구축할 수 있게 도와줍니다

8
00:00:23,690 --> 00:00:29,340
데이터 이미지 분류 파이프라인으로
돌아가서

9
00:00:29,340 --> 00:00:35,730
이미지에서 시작해서 특징이나
다른 표현을 탐지하고

10
00:00:35,730 --> 00:00:42,220
선형 분류기 같은 데 집어넣었습니다

11
00:00:42,220 --> 00:00:45,280
그렇다면 신경망을 통해 배운 특징을

12
00:00:45,280 --> 00:00:48,660
다른 데서 사용할 수 없을까요?

13
00:00:48,660 --> 00:00:54,390
구석, 선, 얼굴 특징을 분류기에 넣는 거죠

14
00:00:55,410 --> 00:00:56,940
좀 다르게 해볼 수 있을까요?

15
00:00:58,830 --> 00:00:59,720
그러니까

16
00:00:59,720 --> 00:01:03,350
딥 특징이란 발상은 전이 학습이란 겁니다

17
00:01:03,350 --> 00:01:06,940
전이 학습은 꽤 오래된 개념인데

18
00:01:06,940 --> 00:01:12,100
최근 딥 신경망 분야에서
꽤 큰 영향을 끼쳤죠

19
00:01:12,100 --> 00:01:16,500
그러니까 아주 많은 데이터로

20
00:01:16,500 --> 00:01:17,510
신경망을 훈련시킵니다

21
00:01:17,510 --> 00:01:21,109
고양이와 개를 구분하는 작업이라고 할게요

22
00:01:22,110 --> 00:01:27,590
8층, 160만 파라미터가 있는
복잡한 신경망을 학습시킵니다

23
00:01:27,590 --> 00:01:31,500
고양이와 개 구분 작업에서는
정확도가 높겠죠

24
00:01:31,500 --> 00:01:35,910
이제 새로운 작업을 하려는데 데이터가

25
00:01:35,910 --> 00:01:41,080
조금밖에 없습니다

26
00:01:41,080 --> 00:01:45,150
의자, 코끼리, 자동차, 카메라를
수백 카테고리에서 골라낸다고 합시다

27
00:01:45,150 --> 00:01:51,420
고양이와 개를 구분할 때 썼던 특징을 재활용해서

28
00:01:51,420 --> 00:01:57,209
101개의 새로운 카테고리에 대해서도
높은 정확도를 낼 수 있을까요?

29
00:01:58,620 --> 00:02:00,660
이게 전이 학습의 개념입니다

30
00:02:00,660 --> 00:02:06,270
고양이와 개 구분에서 학습된 특징이

31
00:02:06,270 --> 00:02:10,590
코끼리와 카메라를 구분하는 새로운 작업에서의
정확도 향상에 도움을 주는 거죠

32
00:02:11,890 --> 00:02:14,730
전이 학습과 딥 신경망을 이해하기 위해

33
00:02:14,730 --> 00:02:19,120
딥 신경망이 뭘 학습하는지 정리해봅시다

34
00:02:19,120 --> 00:02:22,270
고양이와 개를 구분하는 딥 신경망입니다

35
00:02:22,270 --> 00:02:25,660
이 작업에는 아주 높은 정확도를

36
00:02:25,660 --> 00:02:27,410
보인다고 합시다

37
00:02:27,410 --> 00:02:31,970
마지막 층들은 고양이와 개에

38
00:02:31,970 --> 00:02:33,212
아주 특화돼 있겠죠

39
00:02:33,212 --> 00:02:36,295
전에 보여드린 예제에서는

40
00:02:36,295 --> 00:02:37,860
마지막 층에서 산호를 탐지했었죠

41
00:02:39,050 --> 00:02:41,510
중간층은 좀더 일반적입니다

42
00:02:41,510 --> 00:02:45,470
구석, 선, 원, 구불구불한 패턴 같은

43
00:02:45,470 --> 00:02:51,310
101개의 새 카테고리에도 적용될 수 있는

44
00:02:51,310 --> 00:02:57,500
일반적인 패턴을 표현합니다

45
00:02:57,500 --> 00:03:02,453
두 번째 작업인 101개의 카테고리를
어떻게 처리하나 볼까요

46
00:03:02,453 --> 00:03:07,944
고양이와 개를 구분하는 딥 신경망이
작업 2에도 적용될 수 있다는 걸 배웠죠

47
00:03:07,944 --> 00:03:09,672
생각해보면

48
00:03:09,672 --> 00:03:14,005
신경망의 마지막 부분은
고양이와 개에 특화돼서

49
00:03:14,005 --> 00:03:17,429
의자 탐지에는 별 쓸모가 없을 겁니다

50
00:03:17,429 --> 00:03:22,385
그러니 마지막 층 몇 개를 잘라내고

51
00:03:22,385 --> 00:03:27,287
앞층에 대해서는 가중치를 유지합니다

52
00:03:27,287 --> 00:03:30,026
일반적으로 적용되는 특징이기 때문이죠

53
00:03:30,026 --> 00:03:34,629
이제 마지막 층을 선형 분류기로 연결해서

54
00:03:34,629 --> 00:03:39,149
의자, 코끼리, 카메라에 대해

55
00:03:39,149 --> 00:03:41,507
조금 있는 데이터를 훈련시킵니다

56
00:03:44,201 --> 00:03:47,853
이전에 설명드린 세 층이 있을 때의

57
00:03:47,853 --> 00:03:49,910
예제로 돌아갑니다

58
00:03:49,910 --> 00:03:53,800
첫 번째 층에서 대각선을 탐지했죠

59
00:03:53,800 --> 00:03:57,510
두 번째에서 구불구불한 패턴과
구석을 탐지했습니다

60
00:03:57,510 --> 00:04:01,580
세 번째는 산호와 얼굴인데

61
00:04:01,580 --> 00:04:07,030
이 층들을 새 작업에 활용할 건데
조심해야 합니다

62
00:04:07,030 --> 00:04:11,773
3층은 너무 특화돼 있을지 몰라도
1층과 2층은 쓸만해 보입니다

63
00:04:11,773 --> 00:04:15,598
전이 학습이란 개념을 배웠는데

64
00:04:15,598 --> 00:04:20,481
딥러닝 파이프라인을 딥 특징과 함께
복습해보죠

65
00:04:20,481 --> 00:04:24,580
라벨이 붙은 데이터로 시작하죠
적은 데이터로도 충분합니다

66
00:04:24,580 --> 00:04:27,720
설명드린 딥 신경망으로

67
00:04:27,720 --> 00:04:30,110
특징을 추출합니다

68
00:04:30,110 --> 00:04:35,021
이 데이터 세트를 훈련과
검증 테스트 세트로 나눕니다

69
00:04:35,021 --> 00:04:38,488
선형 분류기, 서포트 벡터 머신와 같은

70
00:04:38,488 --> 00:04:41,330
간단한 분류기로 학습시킨 후

71
00:04:41,330 --> 00:04:45,670
검증하면 간단한 분류기라

72
00:04:45,670 --> 00:04:47,900
조정할 파라미터가 많이 없습니다

73
00:04:47,900 --> 00:04:49,100
간단하죠

74
00:04:49,100 --> 00:04:51,840
적은 데이터만 학습시켜도
성능이 뛰어납니다

75
00:04:52,920 --> 00:04:56,570
이 개념이 잘 적용되는 분야가 여럿 있고

76
00:04:56,570 --> 00:05:00,830
모듈 초반에 보여드린 드레스 고르는 데모가

77
00:05:00,830 --> 00:05:04,970
정확히 이 개념을 활용한 겁니다

78
00:05:04,970 --> 00:05:10,020
드레스에 대한 설명은 많이 없었지만

79
00:05:10,020 --> 00:05:16,940
이미지넷에서 훈련시킨 모델을 통해
드레스 쇼핑을 무사히 마쳤습니다

80
00:05:18,030 --> 00:05:22,550
그렇다면 딥 특징은 얼마나 일반적일까요?

81
00:05:22,550 --> 00:05:27,540
특이하고 아주 드문 작업에도
써먹을 수 있을까요?

82
00:05:28,780 --> 00:05:30,540
놀라실 겁니다

83
00:05:31,750 --> 00:05:33,345
쓰레기에 대해 말씀드리죠

84
00:05:33,345 --> 00:05:36,530
[웃음] 컴폴로지란 회사가 있습니다

85
00:05:36,530 --> 00:05:38,190
아주 재밌는 회사인데요

86
00:05:38,190 --> 00:05:43,540
쓰레기 수거 방식을 재창조하려고
하고 있습니다

87
00:05:43,540 --> 00:05:48,570
보통 쓰레기차가 집집마다
가게마다 돌아다니면서

88
00:05:48,570 --> 00:05:51,780
정기적으로 쓰레기를 매일
매주 수거합니다

89
00:05:52,830 --> 00:05:56,690
쓰레기차의 경로를 최적화해서

90
00:05:56,690 --> 00:06:00,930
수거에 드는 시간을 최소하하려고 합니다

91
00:06:00,930 --> 00:06:04,100
쓰레기통에 카메라를 설치해서

92
00:06:04,100 --> 00:06:06,540
얼마나 들어있나 알아냅니다

93
00:06:06,540 --> 00:06:14,480
꽉찬 쓰레기통 이미지 라벨은 없지만

94
00:06:14,480 --> 00:06:19,480
딥 특징과 사람에 의한 약간의 훈련 데이터로

95
00:06:19,480 --> 00:06:24,780
쓰레기통이 찬 정도를 알아내서
쓰레기 탐지기를 학습시켰고

96
00:06:24,780 --> 00:06:29,877
이를 통해 쓰레기차의 경로를
최적화했습니다

97
00:06:29,877 --> 00:06:36,650
쓰레기 수거에 드는 시간을 줄였죠

98
00:06:36,650 --> 00:06:38,823
딥 특징은 쓰레기에도 유용합니다

99
00:06:38,823 --> 00:06:44,399
[음악]