[MUSIC] Okay, so to handle this situation
of having very popular items, we can think about normalizing
the the co-occurrence matrix. And one way in which we can normalize this
matrix is with something called Jaccard similarity. So I wanna mention that this notion of normalizing this co-occurrence matrix
that we're talking about right now, is very similar to what we talked about
in that clustering and similarity module. When we were talking about tf/idf, the
Term Frequency Inverse Document Frequency. Where, there we were looking at
documents and we said that really, really common words just swamped out other
words that we might have cared about. And so we had this way of using tf/idf to renormalize our raw word counts that we
were using to represent our document. Well, in this case, this is a very similar notion of
accounting for a very popular item. And the way it works is pretty intuitive. We're just gonna count the number of
people who purchased some item i and some item j. So number, who purchased i and j, and that's what our matrix had before. So those are our raw counts. We're gonna normalize by the number of people who
purchased either of these items. So the number who purchased i or j. And so a simple Venn diagram
explains this very clearly. Here's the world of people
that purchased item i. And here's the world of people
who purchased item j, and Here, in this shaded area,
are the people who purchased i and j. So what we're gonna do,
is we're taking the counts we had before. This is our numerator,
this shaded area, purchased i and j. And we're just normalizing
by the total area. So that's, let me switch colors here
to make this a little bit more visible. So, circling the whole entire world of
unique users that purchased items i or j. And that's our denominator. Okay, so, that's one way in which we
can normalize our co-occurrence matrix, and there are other things we can think
about like cosine similarity, and we'll talk about these other
metrics more later in the course. But this method has its own limitations. Here, one issue is the fact that
only the current page matters, so it only matters that I just
bought Sophie the giraffe when we're looking at making
recommendations for me. We're not looking at the entire history
of things that I've purchased to inform these recommendations. So let's talk about a way in which we
can modify our approach to account for my history of purchases. Okay, so a really simple approach
is just to do a weighted average over the scores I would have
placed on the products. For each item in my purchase history, so let's go through a concrete
example of this. Let's imagine that the only items I ever
purchased on Amazon were diapers and milk. So now I wanna make recommendations for
me, the user that only purchases diapers and milk, and what I'm gonna do is,
I'm gonna go through every item that I might think about recommending, and
I'm gonna compute the score as follows. So let's say I'm looking at
whether I wanna recommend the item baby wipes to myself. And in this case, what I'm gonna do is I'm gonna compute a
weighted average over how much I would've recommended baby wipes just based
on having purchased diapers before. So that's using exactly
the techniques we just talked about. So looking at the row diapers, and looking at how many times
people also bought baby wipes. But then I'm also gonna look
at the row for milk, and then look at how many times people
who bought milk bought baby wipes. And I'm gonna average these two results
to say how much, or how likely it is that I would purchase baby wipes,
given this purchase history that I have. Okay, and of course we could do other
variance instead of it just the simple weighted average I could weight more
heavily my recent purchase history to account for context and so on. Okay, so then when I want to make
the recommendation I just sort this weighted average score and recommend
the products that have the most weight. So very similar to the purchase
we talked about before, but now we're combining weights
based on my purchase history. Okay, but
this method still has some limitations. So for example, it doesn't use contacts,
like time of day, at least not directly. It doesn't use features of
the user like my age or gender or anything like that, cuz it's grouping all users together when it's thinking about
looking at this co-occurrence matrix. And likewise,
it doesn't use features of the products. Okay, so everything's just pooled
together without any kind of notion of different properties of these products
or users to drive these recommendations. And another big problem that we face
here is something called the Cold start problem. And this is a really important problem
that we face in a lot of different domains. But let's talk about it in this context. So, the Cold start problem is the fact
that, let's say we get a new user or a new product. How do we form recommendations? Right, I have no observations ever for
that product, so I have no notion of how often it's been
purchased along with something else cuz it's never been purchased. And likewise for a user, I've no
information about my past purchases. [MUSIC]