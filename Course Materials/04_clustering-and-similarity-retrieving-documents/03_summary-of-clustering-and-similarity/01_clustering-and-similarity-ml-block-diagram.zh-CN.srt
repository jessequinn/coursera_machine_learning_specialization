1
00:00:00,820 --> 00:00:06,140
这个模块中我们已经讨论了

2
00:00:06,140 --> 00:00:10,920
一个文件检索的任务 还讨论了一个聚类的概念

3
00:00:10,920 --> 00:00:15,780
用来揭示数据的潜在结构 我们

4
00:00:15,780 --> 00:00:19,890
还谈到了一些聚类这个概念有用武之地的领域

5
00:00:19,890 --> 00:00:22,960
现在让我们来从头讨论一下一个聚类算法的工作流程

6
00:00:24,560 --> 00:00:27,290
如果你觉得你觉得你已经对此了然于胸了 因为你已经

7
00:00:27,290 --> 00:00:29,710
在另外两个模块中见过这个流程图了

8
00:00:29,710 --> 00:00:30,380
那么 赶快醒一醒

9
00:00:30,380 --> 00:00:31,850
因为这次这个可是有点不太一样的

10
00:00:33,330 --> 00:00:35,780
好的 让我们来看看我们的训练数据

11
00:00:35,780 --> 00:00:37,420
在这里 我们的训练数据是

12
00:00:37,420 --> 00:00:43,870
用来进行文件聚类的文件代号和

13
00:00:43,870 --> 00:00:50,550
文本表

14
00:00:50,550 --> 00:00:52,820
所以我们有一堆文件

15
00:00:52,820 --> 00:00:55,690
以及与它们对应的文本

16
00:00:55,690 --> 00:00:57,610
然后我们将会提取一些特征量

17
00:00:57,610 --> 00:01:01,428
我们之前谈到过一些描绘一个文件的不同的方法

18
00:01:01,428 --> 00:01:06,863
这里我要用到其中一个叫做"tf-idf"的方法

19
00:01:06,863 --> 00:01:10,560
全称叫做词频-逆文档频率法

20
00:01:12,520 --> 00:01:14,620
我们下一步要做的是试着

21
00:01:14,620 --> 00:01:17,770
用这个表示法来对我们的文件进行聚类

22
00:01:17,770 --> 00:01:21,850
所以我们将把这些特征量放入一些机器学习的模型中

23
00:01:21,850 --> 00:01:24,694
在这里我们用的是一个聚类模型

24
00:01:29,500 --> 00:01:33,720
对每个文件我们会输出一个聚类标签

25
00:01:33,720 --> 00:01:38,490
那么这里的输出量y帽就是我们的聚类标签

26
00:01:40,140 --> 00:01:43,110
好的 有意思的是

27
00:01:43,110 --> 00:01:46,520
因为我们打算评估我们的聚类结果的准确度

28
00:01:46,520 --> 00:01:50,070
当然 这里我们并没有真正的集群标签 因此

29
00:01:50,070 --> 00:01:57,370
我还是把它称为我们的预测或者预估标签

30
00:01:58,890 --> 00:02:02,370
而我们并没有一个可以用来比对的真实标签

31
00:02:02,370 --> 00:02:07,130
因此这里的y 它并不存在

32
00:02:08,280 --> 00:02:13,840
这是因为 就像我们提到过的那样 我们的设定是无监督式学习

33
00:02:15,620 --> 00:02:16,520
无监督式的

34
00:02:18,500 --> 00:02:19,460
好

35
00:02:19,460 --> 00:02:20,570
所以我们没有真正的标签 但是

36
00:02:20,570 --> 00:02:24,740
我们需要用某种方法来评估我们的聚类准确度

37
00:02:24,740 --> 00:02:30,680
所以让我们在这里画一个小的图 也就是我们的沃罗诺伊图 还有

38
00:02:30,680 --> 00:02:36,633
我们的k均值算法以及集群中心的集合 我们还有数据

39
00:02:38,551 --> 00:02:40,947
我不知道 假设我们的数据长这样

40
00:02:40,947 --> 00:02:42,750
我在这里画一下点

41
00:02:44,590 --> 00:02:49,290
我们的准确度的度量方法

42
00:02:49,290 --> 00:02:54,700
也就是我们用来质量评估的是聚类的一致性

43
00:02:54,700 --> 00:02:58,275
我们会测量每个观测点到

44
00:02:58,275 --> 00:03:00,209
它所在的集群中心的距离

45
00:03:02,682 --> 00:03:07,990
一个好的聚类算法中这些距离会很小

46
00:03:09,050 --> 00:03:14,050
所以我们的目标是最小化这些距离 而我们需要测量的是

47
00:03:14,050 --> 00:03:19,020
准确度 要测量这些距离 我们需要原始数据

48
00:03:19,020 --> 00:03:21,690
我们需要我们的词频-逆文档频率

49
00:03:22,760 --> 00:03:29,330
所以那些值会从这里输入 然后我们还需要集群中心

50
00:03:29,330 --> 00:03:33,690
所以W帽 也就是我们的当前估计值

51
00:03:33,690 --> 00:03:36,780
是我们的模型参量 而k均值算法

52
00:03:36,780 --> 00:03:39,930
这是我们的集群 啊

53
00:03:42,190 --> 00:03:46,190
让我看看我们有没有评对这个词 集群中心

54
00:03:46,190 --> 00:03:47,933
那是W帽的指代意义

55
00:03:47,933 --> 00:03:52,636
当然我们也需要用W帽来测算距离

56
00:03:52,636 --> 00:03:57,845
因此与其用真实的集群标签来评估准确度

57
00:03:57,845 --> 00:04:03,280
我们不如用文件标识和集群中心

58
00:04:03,280 --> 00:04:06,797
把原始数据输入到质量度量中

59
00:04:06,797 --> 00:04:11,852
用于计算到集群中心的距离

60
00:04:16,599 --> 00:04:20,780
那是我们计算误差的度量 虽然它其实不是误差

61
00:04:20,780 --> 00:04:22,210
只是能估量

62
00:04:22,210 --> 00:04:22,710
啊哦

63
00:04:25,460 --> 00:04:26,770
只是能估量质量

64
00:04:29,930 --> 00:04:32,090
所以我就不把那个词写在那了

65
00:04:32,090 --> 00:04:35,180
好了 我想这是有点乱

66
00:04:35,180 --> 00:04:38,240
但是就让我们标成到集群中心的距离

67
00:04:38,240 --> 00:04:39,920
那我们的算法是什么呢？

68
00:04:39,920 --> 00:04:43,350
我们之前谈到过k均值是一个聚类的方法

69
00:04:43,350 --> 00:04:46,158
当然还有其他的方法 但是这里我们集中讨论k均值 那么

70
00:04:46,158 --> 00:04:47,270
k均值是怎么算的呢？

71
00:04:47,270 --> 00:04:54,300
让我们重新画一下这个图 好吧 干脆让我换一种颜色好了

72
00:04:54,300 --> 00:04:56,320
那样我们可以节约点时间

73
00:04:57,320 --> 00:05:02,980
好的 k均值是在试着最小化这个距离 或者说这些距离的总和

74
00:05:02,980 --> 00:05:07,540
最小化的方式是通过迭代循环

75
00:05:07,540 --> 00:05:11,870
所以这个是我们之前的W帽 我们把它更新成

76
00:05:13,680 --> 00:05:20,290
这个新的W帽 用来表示这些点的质量中心

77
00:05:20,290 --> 00:05:24,040
因此这些点会发生偏移

78
00:05:24,040 --> 00:05:27,520
而这个点会直接放在这个观测量上

79
00:05:29,600 --> 00:05:34,900
这就是这个聚类算法的工作流程

80
00:05:34,900 --> 00:05:37,130
让我们再从一个高水平上来总结一下

81
00:05:37,130 --> 00:05:41,575
我们拿到原始数据 用某种方法来表示它们 可以是单词统计量

82
00:05:41,575 --> 00:05:44,440
可以是词频-逆文档频率 或者这类数据的标准化值

83
00:05:44,440 --> 00:05:48,000
我们可以用像二元或者三元词组来

84
00:05:48,000 --> 00:05:49,400
表示我们的文件

85
00:05:50,500 --> 00:05:55,950
然后我们的聚类算法 比如k均值算法 可以输出集群标签

86
00:05:55,950 --> 00:06:01,400
并且我们可以通过迭代来一次次地更新

87
00:06:01,400 --> 00:06:07,410
集群中心 也就是这个聚类模型的参量

88
00:06:07,410 --> 00:06:15,620
迭代更新的依据是观测量到集群中心的距离

89
00:06:15,620 --> 00:06:20,020
不同于其他模块 在这个模块中 我们为大家

90
00:06:20,020 --> 00:06:24,030
详细地讲解了一些算法的背后的细节

91
00:06:24,030 --> 00:06:28,190
尤其是对聚类 我们讨论了k均值算法 以及

92
00:06:28,190 --> 00:06:31,690
文件检索问题 我们还谈到了相邻社区检索问题

93
00:06:31,690 --> 00:06:34,570
并提供了解决问题的算法的细节

94
00:06:34,570 --> 00:06:37,520
你还特别研究了用ipython Notebook

95
00:06:37,520 --> 00:06:39,900
如何进行维基百科条目检索

96
00:06:39,900 --> 00:06:42,190
因此 到这里 你应该可以走出课堂

97
00:06:42,190 --> 00:06:46,630
搭建一个炫酷的新闻稿件的检索系统

98
00:06:46,630 --> 00:06:51,770
或者一个炫酷到我都想不到的其他检索系统

99
00:06:51,770 --> 00:06:53,760
当然还有好多好多有趣的例子

100
00:06:53,760 --> 00:06:57,611
所以 同学们 请务必走向大千世界 多多探索新的好主意

101
00:06:57,611 --> 00:07:01,299
[背景音乐]
翻译: RyukaSuu |审阅: 19waa
Coursera Global Translator Community