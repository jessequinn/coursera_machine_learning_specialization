1
00:00:00,820 --> 00:00:06,140
En este módulo hablaremos de 

2
00:00:06,140 --> 00:00:10,920
una tarea de recuperación de documentos y también hablaremos sobre una noción de agrupamiento donde

3
00:00:10,920 --> 00:00:15,780
tratamos de develar parte de la estructura subyacente en los datos y hablamos

4
00:00:15,780 --> 00:00:19,890
de muchas áreas diferentes en las que la noción de agrupamiento puede ser realmente útil.

5
00:00:19,890 --> 00:00:22,960
Vayamos a través del proceso del algoritmo de agrupamiento

6
00:00:24,560 --> 00:00:27,290
Y si sienten que ya saben ésto porque ya han visto este proceso

7
00:00:27,290 --> 00:00:29,710
en los otros dos módulos, 

8
00:00:29,710 --> 00:00:30,380
bueno, ¡Despierten!

9
00:00:30,380 --> 00:00:31,850
Porque éste va a ser un poquito diferente

10
00:00:33,330 --> 00:00:35,780
Bien, hablemos de nuestros datos de aprendizaje

11
00:00:35,780 --> 00:00:37,420
Acá, nuestros datos de aprendizaje para

12
00:00:37,420 --> 00:00:43,870
una tarea de agrupamiento de documentos será una tabla con 

13
00:00:43,870 --> 00:00:50,550
la identificación, ID, del documento y el texto del documento

14
00:00:50,550 --> 00:00:52,820
Tenemos un gran grupo de documentos

15
00:00:52,820 --> 00:00:55,690
y tenemos todos los textos asociados con ellos

16
00:00:55,690 --> 00:00:57,610
y luego vamos a extraer algún conjunto de características.

17
00:00:57,610 --> 00:01:01,428
Hemos hablado de muchas formas diferentes de representar un documento

18
00:01:01,428 --> 00:01:06,863
Pero la que usaré aqui, como ejemplo es nuestra tf-idf

19
00:01:06,863 --> 00:01:10,560
es decir la representación: frecuencia de términos - frecuencia inversa en documentos

20
00:01:12,520 --> 00:01:14,620
Y lo que vamos a hacer es: vamos a tratar de 

21
00:01:14,620 --> 00:01:17,770
agrupar nuestros documentos basándonos en esta representación

22
00:01:17,770 --> 00:01:21,850
Vamos a pasar estas caracteristicas por un modelo de aprendizaje automático

23
00:01:21,850 --> 00:01:24,694
que en este caso es el modelo de agrupamiento

24
00:01:29,500 --> 00:01:33,720
vamos a entregar para cada documento un rótulo de grupo

25
00:01:33,720 --> 00:01:38,490
De esta forma, la salida y-sombrero es nuestro rótulo de grupo

26
00:01:40,140 --> 00:01:43,110
Y acá es donde se vuelve interesante

27
00:01:43,110 --> 00:01:46,520
porque vamos a evaluar la exactitud de nuestros rótulos de clase

28
00:01:46,520 --> 00:01:50,070
Aunque en este caso no tenemos los verdaderos rótulos de clase, de modo que

29
00:01:50,070 --> 00:01:57,370
debería decir que ésta es nuestra predicción o estimación de rótulo de clase

30
00:01:58,890 --> 00:02:02,370
pero no tenemo el verdadero rótulo de clase para compararlos

31
00:02:02,370 --> 00:02:07,130
Esta y aquí, en realidad no existe

32
00:02:08,280 --> 00:02:13,840
y eso es porque, como ya dijimos, estamos en una configuración de aprendizaje sin supervisión

33
00:02:15,620 --> 00:02:16,520
No supervisado.

34
00:02:18,500 --> 00:02:19,460
Está bien. Regresa al principio 
de ésta.

35
00:02:19,460 --> 00:02:20,570
De modo que no lo tenemos pero 

36
00:02:20,570 --> 00:02:24,740
de alguna forma queremos tener alguna medida de la precision de nuestro agrupamiento

37
00:02:24,740 --> 00:02:30,680
Voy a hacer un pequeño dibujo acá que sera nuestra teselación de Voroni

38
00:02:30,680 --> 00:02:36,633
nuestro algoritmo k-medias donde tenemos algún conjunto de centros de agrupado y tenemos datos

39
00:02:38,551 --> 00:02:40,947
Podría decir que nuestros datos se ven así. No sé.

40
00:02:40,947 --> 00:02:42,750
Solo voy a dibujar algunos puntos acá.

41
00:02:44,590 --> 00:02:49,290
Y la medición de precisión que vamos a usar,

42
00:02:49,290 --> 00:02:54,700
la forma en que vamos a medir la calidad es observar qué tan coherente es nuestro agrupamiento

43
00:02:54,700 --> 00:02:58,275
Vamos a mirar la distancia de cada observación 

44
00:02:58,275 --> 00:03:00,209
al centro del grupo al que está asignado

45
00:03:02,682 --> 00:03:07,990
y un buen algoritmo de agrupamiento tendrá esas distancias muy pequeñas

46
00:03:09,050 --> 00:03:14,050
La meta es minimizar estas distancias, de modo que lo que buscamos es 

47
00:03:14,050 --> 00:03:19,020
medir esta precisión, medir estas distancias. Lo que necesitamos son nuestros datos

48
00:03:19,020 --> 00:03:21,690
necesitamos nuestros vetores tf-idf

49
00:03:22,760 --> 00:03:29,330
que van a entrar acá y también necesitamos los centros de los grupos

50
00:03:29,330 --> 00:03:33,690
Entonces w-sombrero tiene nuestra estimación actual 

51
00:03:33,690 --> 00:03:36,780
ese es nuestro parámetro del modelo aquí y el algoritmo k-medias

52
00:03:36,780 --> 00:03:39,930
Son nuestros centros... ups!

53
00:03:42,190 --> 00:03:46,190
Veamos si lo puedo escribir correctamente, centros de agrupamiento

54
00:03:46,190 --> 00:03:47,933
Eso es lo que w-sombrero representa

55
00:03:47,933 --> 00:03:52,636
Por supuesto, para medir esas distancias también necesitamos w-sombrero

56
00:03:52,636 --> 00:03:57,845
Entonces, en lugar de tener rótulos de clase reales para evaluar la precisión 

57
00:03:57,845 --> 00:04:03,280
vamos a tomar nuestra representación de los documentos y nuestros centros de grupos

58
00:04:03,280 --> 00:04:06,797
vamos a insertarlos en esta medición de calidad

59
00:04:06,797 --> 00:04:11,852
que mira las distancias a los centros de grupos

60
00:04:16,599 --> 00:04:20,780
Esa es nuestra medida de error, si bien no es realmente un error

61
00:04:20,780 --> 00:04:22,210
es solo una medida de

62
00:04:22,210 --> 00:04:22,710
Un problema.

63
00:04:25,460 --> 00:04:26,770
Es una medida de calidad

64
00:04:29,930 --> 00:04:32,090
No voy aponer esa palabra acá

65
00:04:32,090 --> 00:04:35,180
Está bien, creo que está un poco confuso

66
00:04:35,180 --> 00:04:38,240
pero escribamos distancias a los centros de agrupamiento

67
00:04:38,240 --> 00:04:39,920
¿Entonces qué hace nuestro algoritmo?

68
00:04:39,920 --> 00:04:43,350
Hablamos de k-medias como el método para hacer agrupamiento

69
00:04:43,350 --> 00:04:46,158
Por supuesto que hay otros, pero concentrémosnos en k-medias. 

70
00:04:46,158 --> 00:04:47,270
¿qué hace k-medias?

71
00:04:47,270 --> 00:04:54,300
Redibujemos este diagrama, realmente, puedo cambiar a otro color.

72
00:04:54,300 --> 00:04:56,320
eso nos ahorrará algún tiempo

73
00:04:57,320 --> 00:05:02,980
K-medias está tratando de minimizar esta distancia, o la suma de estas distancias.

74
00:05:02,980 --> 00:05:07,540
y la forma en que lo hace es actualizando iterativamente 

75
00:05:07,540 --> 00:05:11,870
entonces esta es nuestra w-sombrero que teníamos antees y la estamos cambiando

76
00:05:13,680 --> 00:05:20,290
a la nueva w-sombrero que representa el centro de masas de estos puntos

77
00:05:20,290 --> 00:05:24,040
Estos puntos están siendo desplazados

78
00:05:24,040 --> 00:05:27,520
y este punto va a ir directamente encima de esa observación

79
00:05:29,600 --> 00:05:34,900
este es el proceso de agrupamiento.

80
00:05:34,900 --> 00:05:37,130
Repitámoslo a alto nivel una vez más.

81
00:05:37,130 --> 00:05:41,575
Tomamos nuestros documentos, los representamos en alguna forma, usando o una cruda cuenta de palabras, tf-idf, o normalizaciones de estas cosas.

82
00:05:41,575 --> 00:05:44,440
tdf-idf, normalizaciones de estas cosas

83
00:05:44,440 --> 00:05:48,000
muchos tipos de bigramas, trigramas, cosas que podemos buscar 

84
00:05:48,000 --> 00:05:49,400
para representar nuestros documentos

85
00:05:50,500 --> 00:05:55,950
Luego nuestro algoritmo de agrupamiento, como k-medias produce los rótulos de grupos y

86
00:05:55,950 --> 00:06:01,400
iterativamente, volvemos hasta acá una y otra vez actualizando

87
00:06:01,400 --> 00:06:07,410
nuestros centros de grupos, esos son los parámetros de este modelo de agrupamiento

88
00:06:07,410 --> 00:06:15,620
mirando que tan lejos están nuestras observaciones asignadas están de esos centros de grupo

89
00:06:15,620 --> 00:06:20,020
En este módulo, en contraste con alguno de los otros módulos que les presentamos, les mostramos realmente 

90
00:06:20,020 --> 00:06:24,030
algunos de los detalles del algoritmo detrás del método que estamos observando

91
00:06:24,030 --> 00:06:28,190
Específicamente para agrupamiento, hablamos  sobre el algoritmo k-medias, y luego

92
00:06:28,190 --> 00:06:31,690
para nuestra tarea de recuperación de documentos, hemos hablado de hacer la búsqueda del vecino más cercano,

93
00:06:31,690 --> 00:06:34,570
y mostramos algunos detalles del algoritmo para hacerlo, y

94
00:06:34,570 --> 00:06:37,520
has explorado en particular que en una notebook ipython

95
00:06:37,520 --> 00:06:39,900
 haciendo entrada o búsqueda dentro de Wikipedia

96
00:06:39,900 --> 00:06:42,190
En este momento, deberías ser capaz de ir y

97
00:06:42,190 --> 00:06:46,630
construir un buen sistema de búsqueda sobre artículos de noticias

98
00:06:46,630 --> 00:06:51,770
O cualquier otra búsqueda realmente genial que no puedo imaginar ahora.

99
00:06:51,770 --> 00:06:53,760
Por supuesto hay montones de ejemplos interesantes

100
00:06:53,760 --> 00:06:57,611
Vayan y piensen sobre esas ideas de las que no puedo imaginarme ahora

101
00:06:57,611 --> 00:07:01,299
[MÚSICA]