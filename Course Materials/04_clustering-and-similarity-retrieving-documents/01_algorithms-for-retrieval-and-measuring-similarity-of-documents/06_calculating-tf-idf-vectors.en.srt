1
00:00:07,061 --> 00:00:11,591
Okay, so one way to represent this
trade-off between something that's common

2
00:00:11,591 --> 00:00:15,254
locally but rare globally is
something that's called TF-IDF or

3
00:00:15,254 --> 00:00:18,000
Term frequency- inverse
document frequency.

4
00:00:19,270 --> 00:00:24,180
Okay, so first let's describe what term
frequency is, and here what we're gonna do

5
00:00:24,180 --> 00:00:28,700
is look locally, we're looking just at
the document that the person is currently

6
00:00:28,700 --> 00:00:32,170
reading and we simply count
the number of words like we did.

7
00:00:32,170 --> 00:00:34,270
So this is just our word count factor.

8
00:00:35,490 --> 00:00:40,130
But then what we're gonna do is,
we're gonna

9
00:00:40,130 --> 00:00:45,260
downweight this factor base on something
called the Inverse Document Frequency.

10
00:00:46,370 --> 00:00:50,127
So for this, we're gonna look at
all documents in our corpus and

11
00:00:50,127 --> 00:00:54,190
were gonna compute the following thing,
which is log of

12
00:00:55,250 --> 00:00:59,880
the number of documents in
the corpus divided by 1

13
00:00:59,880 --> 00:01:04,410
plus the number of documents that
contain the word that we're looking at.

14
00:01:05,470 --> 00:01:09,140
Okay, so let's think a little
bit about why we have this form.

15
00:01:09,140 --> 00:01:14,430
So first,
let's think about a very common word.

16
00:01:14,430 --> 00:01:16,111
So, a word appearing in many documents.

17
00:01:22,933 --> 00:01:24,310
Well what happens?

18
00:01:24,310 --> 00:01:29,243
We end up with the log of some large #

19
00:01:29,243 --> 00:01:34,013
/ 1 + another large number, and

20
00:01:34,013 --> 00:01:40,099
we'll say this is approximately log of 1,

21
00:01:40,099 --> 00:01:43,074
which is equal to 0.

22
00:01:44,100 --> 00:01:49,140
So what we see here, is that we're gonna
be very, very strongly downweighting all

23
00:01:49,140 --> 00:01:54,340
the way to zero, the counts of any word
that appears extremely frequently.

24
00:01:55,550 --> 00:01:58,930
Where that appears in
all of our documents.

25
00:01:58,930 --> 00:02:04,131
Okay, but in contrast,
if we have a rare word,

26
00:02:04,131 --> 00:02:09,985
were gonna have log of,
I would say, a large number,

27
00:02:09,985 --> 00:02:15,319
assuming we have a large
number of documents that

28
00:02:15,319 --> 00:02:21,303
we were searching over divided
by 1 + a small number,

29
00:02:21,303 --> 00:02:27,026
and this is gonna be, or
to say some largish number,

30
00:02:27,026 --> 00:02:31,380
or a not a zero or a small number.

31
00:02:31,380 --> 00:02:36,790
And the reason that we have this one here,
is the fact that we can't assume that

32
00:02:36,790 --> 00:02:42,269
every word appears in any
document in the corpus.

33
00:02:43,310 --> 00:02:46,010
So there might be some word in
the vocabulary that doesn't appear

34
00:02:46,010 --> 00:02:47,610
anywhere in our corpus.

35
00:02:47,610 --> 00:02:50,200
And so we wanna avoid dividing by 0.

36
00:02:50,200 --> 00:02:55,300
Okay, so let's look at an example
where there's the index for

37
00:02:55,300 --> 00:03:01,200
the word the, and
let's say that the appears.

38
00:03:01,200 --> 00:03:03,570
I don't know.
Something like a thousand times in

39
00:03:03,570 --> 00:03:07,814
the document that I'm looking at,
and then there's the word messi.

40
00:03:07,814 --> 00:03:12,640
Messi appears five times.

41
00:03:12,640 --> 00:03:19,650
Okay, now I'm gonna look at computing the
inverse document frequency for that word.

42
00:03:19,650 --> 00:03:21,190
So the word the,

43
00:03:21,190 --> 00:03:25,290
let's assume that the word the appears
in every document in the corpus.

44
00:03:25,290 --> 00:03:30,470
Well, really every document except one,
so I can do some easy math here.

45
00:03:30,470 --> 00:03:34,580
So, when I'm looking at this entry,

46
00:03:34,580 --> 00:03:39,230
I'm gonna compute log of the number
of documents in the corpus and

47
00:03:39,230 --> 00:03:43,730
let's assume that we have 64
documents in this corpus.

48
00:03:43,730 --> 00:03:48,160
And then we have 1+, and I assume
that the word the didn't appear in

49
00:03:48,160 --> 00:03:52,860
one of these 64 documents,
must have been a pretty short document.

50
00:03:52,860 --> 00:03:58,680
And so, what this gives us is the 0
that we talking about before.

51
00:04:00,240 --> 00:04:03,790
So the the gets downweighted
completely by zero.

52
00:04:05,390 --> 00:04:11,740
In contrast, when we look at Messi,
let's assume that

53
00:04:13,390 --> 00:04:17,950
again we have some 64 total documents and

54
00:04:17,950 --> 00:04:23,860
lets assume that the word Messi appears
only in three of these documents,

55
00:04:23,860 --> 00:04:27,940
so we get log of 16,

56
00:04:27,940 --> 00:04:33,965
which if we use log
base two gives us four.

57
00:04:33,965 --> 00:04:36,870
Okay, so this is our term frequency, and

58
00:04:36,870 --> 00:04:40,179
our inverse document frequency for
these two words.

59
00:04:42,370 --> 00:04:47,085
And when we code to compute the term
frequency, inverse document frequency, for

60
00:04:47,085 --> 00:04:51,800
the specific document which is gonna be
our new representation of this document,

61
00:04:51,800 --> 00:04:54,630
we simply multiply these
two factors together.

62
00:04:54,630 --> 00:05:00,260
So theres some numbers here,
where the word the,

63
00:05:00,260 --> 00:05:06,030
turns into a 0 and then these are some
other numbers and then the word

64
00:05:07,460 --> 00:05:12,200
Messi is gonna be upweighted,
so a weight of 20.

65
00:05:12,200 --> 00:05:13,130
And again,

66
00:05:13,130 --> 00:05:17,090
there's some other computation we're doing
for all the other words in our vocabulary.

67
00:05:17,090 --> 00:05:20,921
But the point that we wanna make here is
the fact that these very common words like

68
00:05:20,921 --> 00:05:22,806
the, get downweighted and the rare and

69
00:05:22,806 --> 00:05:26,251
potentially very important words
like Messi are getting upweighted.