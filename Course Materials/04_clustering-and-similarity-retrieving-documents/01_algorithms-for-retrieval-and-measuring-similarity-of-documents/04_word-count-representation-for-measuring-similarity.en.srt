1
00:00:00,000 --> 00:00:03,754
[MUSIC]

2
00:00:03,754 --> 00:00:07,691
So the first thing we need to describe is
how are we gonna represent the documents

3
00:00:07,691 --> 00:00:08,937
that we're looking at.

4
00:00:08,937 --> 00:00:12,771
Okay, so perhaps the most popular model to
represent a document is something called

5
00:00:12,771 --> 00:00:13,851
the bag of words model,

6
00:00:13,851 --> 00:00:17,670
where we simply ignore the order of
words that are present in the document.

7
00:00:17,670 --> 00:00:20,870
And the reason it's called a bag of
words model is we think of taking a bag,

8
00:00:20,870 --> 00:00:24,940
throwing all the words from that document
into the bag, shaking it up, and

9
00:00:24,940 --> 00:00:28,350
the new document we've created
with the words all jumbled up

10
00:00:28,350 --> 00:00:31,540
has exactly the same
representation as I'll describe

11
00:00:31,540 --> 00:00:34,630
as the original document
where the words were ordered.

12
00:00:36,800 --> 00:00:40,540
And what we're gonna do,
instead of considering the structure,

13
00:00:40,540 --> 00:00:41,380
the order of the words,

14
00:00:41,380 --> 00:00:46,400
is we're simply gonna count the number of
instances of every word in the document.

15
00:00:46,400 --> 00:00:49,060
So let's look at a specific
example of this.

16
00:00:49,060 --> 00:00:53,290
So in this document we're gonna imagine
that there's just one sentence, and that

17
00:00:53,290 --> 00:00:58,820
sentence says that Carlos calls the sport
futbol, Emily calls the sport soccer.

18
00:01:00,650 --> 00:01:06,050
Actually I guess that's really two
sentences, but that's the entire document.

19
00:01:06,050 --> 00:01:09,010
And what we're gonna do to
count the number of instances

20
00:01:09,010 --> 00:01:13,170
of words in this very short document
is we're just gonna look at a vector.

21
00:01:13,170 --> 00:01:17,160
And this vector is defined
over the vocabulary

22
00:01:17,160 --> 00:01:19,310
in whatever language we're looking at.

23
00:01:19,310 --> 00:01:24,280
So maybe one word in our
vocabulary is the name, Carlos.

24
00:01:25,320 --> 00:01:30,540
Another place in this vector is
the index for the word sport.

25
00:01:32,600 --> 00:01:38,170
And then, somewhere else we have the word
futbol luckily in our English vocabulary

26
00:01:38,170 --> 00:01:44,710
that I'm writing here and then let's say,
Emily is this last entry.

27
00:01:44,710 --> 00:01:45,740
What words are we missing?

28
00:01:45,740 --> 00:01:50,450
We're missing the word calls,
and of course the word the.

29
00:01:51,840 --> 00:01:53,856
Okay, so how many instances of Carlos?

30
00:01:53,856 --> 00:01:55,480
Well, there's only one.

31
00:01:55,480 --> 00:01:57,403
How many instances of the?

32
00:01:57,403 --> 00:02:00,060
We have two of the.

33
00:02:00,060 --> 00:02:03,671
Two of calls, two of sports,

34
00:02:03,671 --> 00:02:09,320
one of futbol and
I forgot the word soccer.

35
00:02:09,320 --> 00:02:13,740
One word of Emily and
let just throw soccer in here,

36
00:02:13,740 --> 00:02:18,740
imagining this was the index and
this would be our word count factor with.

37
00:02:20,200 --> 00:02:23,440
For this document,
every other entry would be zero.

38
00:02:24,440 --> 00:02:28,370
And all these other entries represent all
the other words that are out there in

39
00:02:28,370 --> 00:02:32,401
the vocabulary, like the word cat, and

40
00:02:32,401 --> 00:02:37,280
dog, and tree, and
every other word you can think of.

41
00:02:38,400 --> 00:02:39,850
So it's a very, very long and

42
00:02:39,850 --> 00:02:45,220
sparse vector that counts the number
of words that we see in this document.

43
00:02:45,220 --> 00:02:48,976
Okay, so we talked about this
representation of our documents in terms

44
00:02:48,976 --> 00:02:50,613
of just these raw word counts.

45
00:02:50,613 --> 00:02:51,965
This bag of words model.

46
00:02:51,965 --> 00:02:55,659
And now we want to talk about how we're
gonna measure the similarity between

47
00:02:55,659 --> 00:02:58,892
different documents because we're
gonna use that in order to find

48
00:02:58,892 --> 00:03:02,890
documents that are related to one another
and so on, like we talked about before.

49
00:03:02,890 --> 00:03:07,430
Carlos is reading an article, so whatâ€™s
another article he might be interested in?

50
00:03:07,430 --> 00:03:12,250
Okay, so imagine that this is
the count factor that we have for

51
00:03:12,250 --> 00:03:18,290
this article on soccer, with this
famous Argentinian player, Messi.

52
00:03:18,290 --> 00:03:22,410
And then there's another article
here that I'm showing in blue and

53
00:03:22,410 --> 00:03:24,227
the associated word counts.

54
00:03:24,227 --> 00:03:29,826
And this article is about another
famous soccer player, Pele.

55
00:03:29,826 --> 00:03:32,954
Is that right?

56
00:03:32,954 --> 00:03:33,895
>> Pele.
>> Pele.

57
00:03:33,895 --> 00:03:38,921
[LAUGH] So
when we think about measuring similarity,

58
00:03:38,921 --> 00:03:46,130
what we can do is simply look at
an element-wise product over this vector.

59
00:03:46,130 --> 00:03:49,260
So for every element in the vector,
we're gonna

60
00:03:49,260 --> 00:03:53,900
multiply the two elements appearing
in these two different count vectors.

61
00:03:53,900 --> 00:03:57,490
And add up over all the different
elements in this vector.

62
00:03:57,490 --> 00:04:01,527
So here I've done this math
where we have 1 times 3,

63
00:04:01,527 --> 00:04:04,667
all the other elements multiplied to 0,

64
00:04:04,667 --> 00:04:10,410
except at some point that fifth entry
in the vector we have 5 times 2.

65
00:04:10,410 --> 00:04:12,910
And if we do this multiplication
over the whole vector,

66
00:04:12,910 --> 00:04:15,360
the sum of these terms is 13.

67
00:04:15,360 --> 00:04:19,470
So that measures the similarity
between these two articles on soccer.

68
00:04:20,690 --> 00:04:22,880
But now let's compare to another article,

69
00:04:22,880 --> 00:04:27,540
which happens to be something
about a conflict in Africa.

70
00:04:27,540 --> 00:04:32,620
And so I'm providing the examples of
word counts that appear in this article.

71
00:04:32,620 --> 00:04:36,360
And what we see, is when we go to measure
the similarities between these articles,

72
00:04:36,360 --> 00:04:39,780
using the method that I described
of element-wise product, and

73
00:04:39,780 --> 00:04:43,880
then adding, that the similarity
here in this case is 0.

74
00:04:43,880 --> 00:04:47,170
Okay, so let's talk about
an issue that arises when we

75
00:04:47,170 --> 00:04:50,709
use these raw word counts to measure
the similarity between documents.

76
00:04:51,810 --> 00:04:55,700
So to do this, let's look at these green
and blue articles that we had before.

77
00:04:55,700 --> 00:04:58,700
And so I'm repeating the word
count vectors that we had, and

78
00:04:58,700 --> 00:05:02,350
what we calculated before was the fact
that the similarity between these two

79
00:05:02,350 --> 00:05:05,820
articles that are both about soccer is 13.

80
00:05:05,820 --> 00:05:06,750
Okay, but

81
00:05:06,750 --> 00:05:10,570
now let's look at what happens if we
simply double the length of the documents.

82
00:05:10,570 --> 00:05:15,360
So now every word that appeared in that
original document appears twice in this

83
00:05:15,360 --> 00:05:17,760
twice as long document.

84
00:05:17,760 --> 00:05:23,380
So, the word count vector is simply two
times the word count vector we had before.

85
00:05:23,380 --> 00:05:26,826
So, when we go to calculate
the similarity here,

86
00:05:26,826 --> 00:05:30,858
what we see is now the similarity
is calculated to be 52.

87
00:05:30,858 --> 00:05:31,580
So, let's think about this.

88
00:05:31,580 --> 00:05:35,258
What we're saying is
that two documents that

89
00:05:35,258 --> 00:05:39,435
are related to each other
in the same way as before.

90
00:05:39,435 --> 00:05:44,117
They're both talking about
the same two sports, but

91
00:05:44,117 --> 00:05:48,601
one just is replicated twice
is a lot more similar.

92
00:05:48,601 --> 00:05:53,880
We would say, yes, Carlos is a lot more
interested in this longer document.

93
00:05:53,880 --> 00:05:59,120
Then what happened when Carlos was
reading the shorter documents.

94
00:05:59,120 --> 00:06:02,870
So this doesn't make a lot of sense when
we're trying to do document retrieval.

95
00:06:02,870 --> 00:06:07,110
And it biases very strongly
towards long documents.

96
00:06:07,110 --> 00:06:09,920
So let's think about how
we can cope with this.

97
00:06:09,920 --> 00:06:10,950
So one solution

98
00:06:12,370 --> 00:06:15,540
is very straight forward where we're
simply gonna normalize this vector.

99
00:06:15,540 --> 00:06:17,740
So we take this word count vector and

100
00:06:17,740 --> 00:06:19,910
we're gonna compute
the norm of the vector.

101
00:06:19,910 --> 00:06:23,560
And so if you guys remember
computing the norm of a vector,

102
00:06:23,560 --> 00:06:29,670
we simply add the square of every entry in
the vector, and then take the square root.

103
00:06:29,670 --> 00:06:30,624
So, in this case,

104
00:06:30,624 --> 00:06:35,420
we have the square root of 1 squared plus
5 squared plus 3 squared plus 1 squared.

105
00:06:35,420 --> 00:06:38,730
And that happens to be the number and so,

106
00:06:38,730 --> 00:06:44,180
the resulting normalize word count vector
is shown on the bottom of the slide here.

107
00:06:44,180 --> 00:06:48,210
And what this does is, it allows us to
place all of our articles that we're

108
00:06:48,210 --> 00:06:51,733
considering, regardless of their length,
on equal footing.

109
00:06:51,733 --> 00:06:55,280
And then use this normalized
vector when we go to do retrieval.

110
00:06:55,280 --> 00:06:57,189
[MUSIC]